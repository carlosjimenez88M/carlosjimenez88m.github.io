<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Llms on The Probability Engine</title><link>https://carlosdanieljimenez.com/tags/llms/</link><description>Recent content in Llms on The Probability Engine</description><generator>Hugo -- 0.147.3</generator><language>en-us</language><lastBuildDate>Tue, 10 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://carlosdanieljimenez.com/tags/llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention Windows: A Novel Framework for Measuring Narrative Cognitive Load in Beatles vs Pink Floyd</title><link>https://carlosdanieljimenez.com/post/2026-02-10-attention-windows-beatles-floyd/</link><pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/2026-02-10-attention-windows-beatles-floyd/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This research introduces &lt;strong>Attention Windows&lt;/strong>, a novel framework for measuring the cognitive span required by listeners to follow lyrical narratives. How long can a theme persist before the lyrics shift to something new? Building on previous semantic embedding analyses of the Beatles and Pink Floyd, we develop a multi-method approach to quantify this narrative architecture across two iconic albums: &lt;em>The Dark Side of the Moon&lt;/em> and &lt;em>Abbey Road&lt;/em>.&lt;/p>
&lt;p>&lt;strong>Core Finding (UNEXPECTED):&lt;/strong> The analysis reveals a surprising inversion of our initial hypothesis. The Beatles exhibit significantly longer attention windows (μ = 0.57 lines, SD = 1.48) than Pink Floyd (μ = 0.25 lines, SD = 0.97) when measured with OpenAI&amp;rsquo;s text-embedding-ada-002 at its calibrated threshold (θ = 0.85). This counterintuitive result (p &amp;lt; 0.001) illuminates something fundamental about musical structure: the Beatles&amp;rsquo; verse-chorus repetition creates strong measurable coherence between consecutive lines, while Pink Floyd&amp;rsquo;s through-composed, non-repetitive approach—precisely what makes them feel &amp;ldquo;thematically sustained&amp;rdquo;—actually produces lower line-to-line similarity. The metric, it turns out, captures structural repetition rather than abstract thematic continuity, offering unexpected insights into how pop and progressive rock architectures differ at the semantic level.&lt;/p></description></item><item><title>Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach</title><link>https://carlosdanieljimenez.com/post/2026-01-07-literary_mapping/</link><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/2026-01-07-literary_mapping/</guid><description>&lt;h2 id="post-objective">Post Objective&lt;/h2>
&lt;ul>
&lt;li>Data cleaning and preliminary analysis process&lt;/li>
&lt;li>Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs&lt;/li>
&lt;li>Understanding the connections and most representative ideas within the document set&lt;/li>
&lt;/ul>
&lt;h2 id="intention">Intention&lt;/h2>
&lt;p>Understanding a story&amp;rsquo;s behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the &lt;strong>vector narrative arc&lt;/strong> combined with a &lt;strong>literary map&lt;/strong> constitutes an interesting route to address increasingly common problems.&lt;/p></description></item><item><title>MLflow for Generative AI Systems</title><link>https://carlosdanieljimenez.com/post/mlflow_genai/</link><pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/mlflow_genai/</guid><description>&lt;h1 id="mlflow-for-generative-ai-systems">MLflow for Generative AI Systems&lt;/h1>
&lt;p>I&amp;rsquo;ll start this post by recalling what Hayen said in her book &lt;strong>Designing Machine Learning Systems (2022): &amp;lsquo;Systems are meant to learn&amp;rsquo;.&lt;/strong> This statement reflects a simple fact: today, LLMs and to a lesser extent vision language models are winning in the Data Science world. But how do we measure this learning? RLHF work is always a good indicator that perplexity will improve, but let&amp;rsquo;s return to a key point: LLMs must work as a system, therefore debugging is important, and that&amp;rsquo;s where the necessary tool for every Data Scientist, AI Engineer, ML Engineer, and MLOps Engineer comes in: MLflow.&lt;/p></description></item><item><title>Raspberry Pi 16GB, Servers, and MLOps</title><link>https://carlosdanieljimenez.com/post/mlops-servers-raspberry/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/mlops-servers-raspberry/</guid><description>Raspberry Pi 5 (16 Gbs) like a Server</description></item></channel></rss>