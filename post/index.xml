<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on The Probability Engine</title><link>https://carlosdanieljimenez.com/post/</link><description>Recent content in Posts on The Probability Engine</description><generator>Hugo -- 0.147.3</generator><language>en-us</language><lastBuildDate>Tue, 10 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://carlosdanieljimenez.com/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention Windows: A Novel Framework for Measuring Narrative Cognitive Load in Beatles vs Pink Floyd</title><link>https://carlosdanieljimenez.com/post/2026-02-10-attention-windows-beatles-floyd/</link><pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/2026-02-10-attention-windows-beatles-floyd/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This research introduces &lt;strong>Attention Windows&lt;/strong> (ventanas atencionales), a novel theoretical framework for measuring the cognitive span required by listeners to comprehend lyrical narrative units. Building upon previous semantic embedding analysis of Beatles and Pink Floyd, we develop a multi-method approach to quantify narrative complexity across complete albums (&lt;em>The Dark Side of the Moon&lt;/em> and &lt;em>Abbey Road&lt;/em>).&lt;/p>
&lt;p>&lt;strong>Core Finding (UNEXPECTED):&lt;/strong> Contrario a la hipótesis inicial, The Beatles exhiben attention windows significativamente más largos (μ = 0.41 lines, SD = 1.30) que Pink Floyd (μ = 0.05 lines, SD = 0.24). Este resultado inverso (t = -3.94, p &amp;lt; 0.001, Cohen&amp;rsquo;s d = -0.34) revela que las estructuras pop repetitivas de Beatles generan mayor coherencia local medible con embeddings, mientras que el lenguaje abstracto y poético cambiante de Floyd reduce la similitud directa. Los resultados sugieren que el método mide &amp;ldquo;repetitividad&amp;rdquo; más que &amp;ldquo;coherencia temática abstracta&amp;rdquo;, revelando limitaciones importantes del threshold estricto (0.70) para análisis lírico.&lt;/p></description></item><item><title>Anatomy of an MLOps Pipeline - Part 1: Pipeline and Orchestration</title><link>https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-1-en/</link><pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-1-en/</guid><description>Part 1: Philosophy, project architecture and orchestration with Hydra + MLflow. Steps for preprocessing, feature engineering, hyperparameter tuning and model registry.</description></item><item><title>Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure</title><link>https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-2-en/</link><pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-2-en/</guid><description>Part 2: CI/CD with GitHub Actions, W&amp;amp;B vs MLflow comparison, complete containerization with Docker, and production-ready API architecture with FastAPI.</description></item><item><title>Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices</title><link>https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/</link><pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/</guid><description>Part 3: Model selection strategies, advanced testing, production patterns, data drift, model monitoring, and production readiness checklist.</description></item><item><title>Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach</title><link>https://carlosdanieljimenez.com/post/2026-01-07-literary_mapping/</link><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/2026-01-07-literary_mapping/</guid><description>&lt;h2 id="post-objective">Post Objective&lt;/h2>
&lt;ul>
&lt;li>Data cleaning and preliminary analysis process&lt;/li>
&lt;li>Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs&lt;/li>
&lt;li>Understanding the connections and most representative ideas within the document set&lt;/li>
&lt;/ul>
&lt;h2 id="intention">Intention&lt;/h2>
&lt;p>Understanding a story&amp;rsquo;s behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the &lt;strong>vector narrative arc&lt;/strong> combined with a &lt;strong>literary map&lt;/strong> constitutes an interesting route to address increasingly common problems.&lt;/p></description></item><item><title>MLflow for Generative AI Systems</title><link>https://carlosdanieljimenez.com/post/mlflow_genai/</link><pubDate>Wed, 08 Oct 2025 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/mlflow_genai/</guid><description>&lt;h1 id="mlflow-for-generative-ai-systems">MLflow for Generative AI Systems&lt;/h1>
&lt;p>I&amp;rsquo;ll start this post by recalling what Hayen said in her book &lt;strong>Designing Machine Learning Systems (2022): &amp;lsquo;Systems are meant to learn&amp;rsquo;.&lt;/strong> This statement reflects a simple fact: today, LLMs and to a lesser extent vision language models are winning in the Data Science world. But how do we measure this learning? RLHF work is always a good indicator that perplexity will improve, but let&amp;rsquo;s return to a key point: LLMs must work as a system, therefore debugging is important, and that&amp;rsquo;s where the necessary tool for every Data Scientist, AI Engineer, ML Engineer, and MLOps Engineer comes in: MLflow.&lt;/p></description></item><item><title>The Decline of a Framework</title><link>https://carlosdanieljimenez.com/post/tensorflow/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/tensorflow/</guid><description>Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists</description></item><item><title>Raspberry Pi 16GB, Servers, and MLOps</title><link>https://carlosdanieljimenez.com/post/mlops-servers-raspberry/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/mlops-servers-raspberry/</guid><description>Raspberry Pi 5 (16 Gbs) like a Server</description></item><item><title>MLops into Raspberry Pi 5</title><link>https://carlosdanieljimenez.com/post/mlops_raspberrypi5/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/mlops_raspberrypi5/</guid><description>A robust implementation of facilities for MLOps development</description></item><item><title>Artifact Design and Pipeline in MLOps Part I</title><link>https://carlosdanieljimenez.com/post/artifacts-designs/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/artifacts-designs/</guid><description>Introduction to Artifacts designs</description></item><item><title>Edge Computing and Edge Machine Learning</title><link>https://carlosdanieljimenez.com/post/edge-computing/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/edge-computing/</guid><description>A Brief Introduction to AI/Edge Computing</description></item><item><title>MLOps Guides: A Comprehensive Overview</title><link>https://carlosdanieljimenez.com/post/mlops-guide/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://carlosdanieljimenez.com/post/mlops-guide/</guid><description>Machine Learning Operations, deployment strategies, and best practices for production ML systems</description></item></channel></rss>