<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Decline of a Framework | The Probability Engine</title>
<meta name=keywords content="tensorflow,pytorch,mlflow,edge-ai,deep-learning"><meta name=description content="Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists"><meta name=author content="Carlos Daniel Jim√©nez"><link rel=canonical href=https://carlosdanieljimenez.com/post/tensorflow/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://carlosdanieljimenez.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://carlosdanieljimenez.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://carlosdanieljimenez.com/favicon-32x32.png><link rel=apple-touch-icon href=https://carlosdanieljimenez.com/apple-touch-icon.png><link rel=mask-icon href=https://carlosdanieljimenez.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://carlosdanieljimenez.com/post/tensorflow/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://carlosdanieljimenez.com/post/tensorflow/"><meta property="og:site_name" content="The Probability Engine"><meta property="og:title" content="The Decline of a Framework"><meta property="og:description" content="Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-05-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-12T00:00:00+00:00"><meta property="article:tag" content="Tensorflow"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Mlflow"><meta property="article:tag" content="Edge-Ai"><meta property="article:tag" content="Deep-Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Decline of a Framework"><meta name=twitter:description content="Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://carlosdanieljimenez.com/post/"},{"@type":"ListItem","position":2,"name":"The Decline of a Framework","item":"https://carlosdanieljimenez.com/post/tensorflow/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Decline of a Framework","name":"The Decline of a Framework","description":"Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists","keywords":["tensorflow","pytorch","mlflow","edge-ai","deep-learning"],"articleBody":"Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists Throughout my journey in the world of data, I‚Äôve witnessed many changes ‚Äî some tools fading out of popularity while others take the spotlight. R, for example, has become more niche, used mostly by statisticians and academics. Flask, once a common choice for lightweight APIs, gradually gave way to FastAPI thanks to its modularity and support for asynchronous features, redefining how APIs are designed and deployed.\nWhile these shifts are worth noting, today‚Äôs post focuses on TensorFlow, not just as a framework, but as a case study of how the need for rapid experimentation and testing environments is reshaping the AI landscape. Ultimately, it‚Äôs a reflection on innovation and the importance of tools that are close to deployment, yet don‚Äôt require massive compute power to start building.\nThe Book That Started It All One book that significantly influenced my career as a data scientist is Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur√©lien G√©ron. In fact, I own all three editions, and I frequently revisit them both for my work as an MLOps engineer‚Äîwhen optimizing models‚Äîand in my role as an educator.\nThanks to this book, I was able to internalize critical concepts such as:\nHow do you know when a model has reached its performance ceiling?\nHow do you update a model when the data distribution shifts?\nThis foundational knowledge led me into the world of Deep Learning, which continues to dominate the market today, especially with the rise of LLMs.\nTensorFlow: A Defining Path That book also introduced me to TensorFlow, a robust and well-documented framework backed by Google. It proved invaluable in my early Edge Machine Learning projects‚Äîan area I remain passionate about.\nBut not everything was smooth. TensorFlow 1.x had several major drawbacks: static graphs made debugging difficult, the syntax lacked the clarity of idiomatic Python, and the learning curve was steep. While TensorFlow 2.x addressed many of these issues, it didn‚Äôt offer seamless migration for TF1 projects, creating additional friction within the community.\nIts technical complexity and tightly coupled architecture ultimately hindered broader adoption, especially in a Python-dominated developer ecosystem.\nThe Rise of PyTorch A turning point came in May 2021, when PyTorch overtook TensorFlow in search volume. Several factors contributed to this shift:\nNLP and LLMs: Most models on Hugging Face were built with PyTorch.\nAcademic Research: Over 95% of models on Papers with Code used PyTorch, compared to only 5% with TensorFlow.\nStartups and Innovation: PyTorch‚Äôs dynamic graph execution, fully Pythonic design, and ease of debugging made it the go-to choice for rapid prototyping. It became the standard in agile environments, giving rise to the famous quote: ‚ÄúWe went from experimenting in weeks to days.‚Äù\n2022: The Tipping Point By 2022, PyTorch had clearly become the dominant framework. According to Hugging Face:\n92% of models were trained using PyTorch.\nOnly 14% supported TensorFlow.\nCompanies like OpenAI had already migrated completely to PyTorch by 2020, with many others following suit. Even in fields like Computer Vision, PyTorch established itself as the framework of choice (a topic for another post).\nGoogle‚Äôs Shift Toward JAX In parallel, Google began focusing its efforts on JAX/Flax, a framework geared more toward research. While JAX has not yet achieved mainstream adoption or stood out in model competitions, its emergence marked a strategic pivot‚ÄîGoogle began moving away from TensorFlow as its default deep learning platform.\nA telling sign: the official TensorFlow YouTube channel significantly reduced its publishing frequency, and newer Google research releases were already being built with JAX.\nKeras‚Äô New Role: Beyond TensorFlow One final point worth noting is the evolution of Keras. With version 3, it‚Äôs no longer exclusive to TensorFlow. It is now multi-backend, supporting PyTorch, JAX, and of course, TensorFlow. This change allows tools like Keras Tuner to be used across different frameworks, promoting more flexible, framework-agnostic development workflows.\nWhy TensorFlow Lost Ground (My Perspective) Taken together, these shifts led to TensorFlow‚Äôs gradual decline in both research and production. Despite Google‚Äôs powerful TPU infrastructure‚Äîwhich works well with TensorFlow‚Äîthe broader market and research community moved in a different direction.\nTo me, the breaking point wasn‚Äôt just the lack of backward compatibility or internal complexity‚Äîit was the lack of Pythonic elegance. In a field dominated by Python developers, this became a critical flaw.\nThe battle may have been lost‚Äînot because TensorFlow lacked potential, but because the needs of the ecosystem evolved faster than the framework itself.\nA fun fact (interesting tidbit) There‚Äôs something that caught my attention I‚Äôm going to present two graphs. The first one shows the trend according to Google Trends, where the blue line is TensorFlow and the red one is PyTorch. I had talked about the decline in usage of the framework, and you can see how it‚Äôs falling, but what particularly catches my attention is that there‚Äôs a downward trend for PyTorch in 2025. I don‚Äôt know if it has something to do with Rust, or if researchers and developers have fewer publications, considering we‚Äôre in an era where model releases or versions are an everyday occurrence. In the second graph, also obtained from Google Trends, something else caught my attention even more: In Latin America, Africa, and Eastern Europe, TensorFlow still predominates (there‚Äôs something to infer from that), although in India‚Äôs case, usage is 59% TF and the rest PyTorch, which means that in industry it continues to be applied aggressively. It would be worth analyzing which industries these are. I don‚Äôt know if it has something to do with Vision Language Models, but well, these are the data points and they should be shared. Final Thoughts Deep Learning reached a stage where graph flexibility and computational efficiency became decisive factors in framework adoption.\nAs in all things, technological Darwinism prevailed‚Äîdriven by LLMs, agentic systems, and RAG-based architectures, experimentation speed and model development agility became more valuable than raw power.\nStartups and the industry as a whole paved the path toward innovation, and while TensorFlow fought many worthy battles, its future might lie in niche areas like domotics and Edge AI.\nThe key takeaway is this: in machine learning, development speed and team-wide standardization outweigh personal preferences. I may personally enjoy working with TensorFlow, but in production environments, that choice could come at a cost if it‚Äôs not aligned with the team‚Äôs common stack.\nThat‚Äôs all for this entry‚Äî I hope you enjoyed these reflections!\n","wordCount":"1073","inLanguage":"en","datePublished":"2025-05-12T00:00:00Z","dateModified":"2025-05-12T00:00:00Z","author":{"@type":"Person","name":"Carlos Daniel Jim√©nez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://carlosdanieljimenez.com/post/tensorflow/"},"publisher":{"@type":"Organization","name":"The Probability Engine","logo":{"@type":"ImageObject","url":"https://carlosdanieljimenez.com/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://carlosdanieljimenez.com/ accesskey=h title="The Probability Engine (Alt + H)">The Probability Engine</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://carlosdanieljimenez.com/mlops/ title=MLOps><span>MLOps</span></a></li><li><a href=https://carlosdanieljimenez.com/agentic-ai/ title="Agentic AI"><span>Agentic AI</span></a></li><li><a href=https://carlosdanieljimenez.com/tidytuesday/ title=TidyTuesday><span>TidyTuesday</span></a></li><li><a href=https://carlosdanieljimenez.com/post/ title=Posts><span>Posts</span></a></li><li><a href=https://carlosdanieljimenez.com/edge-computing/ title="Edge Computing"><span>Edge Computing</span></a></li><li><a href=https://carlosdanieljimenez.com/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://carlosdanieljimenez.com/about/ title=About><span>About</span></a></li><li><a href=https://carlosdanieljimenez.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">The Decline of a Framework</h1><div class=post-description>Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists</div><div class=post-meta><span title='2025-05-12 00:00:00 +0000 UTC'>May 12, 2025</span>&nbsp;¬∑&nbsp;<span>Carlos Daniel Jim√©nez</span></div></header><div class=post-content><h3 id=reflections-on-tensorflow-in-the-context-of-the-modern-ai-engine-and-the-evolving-role-of-data-scientists>Reflections on TensorFlow in the context of the modern AI engine and the evolving role of Data Scientists<a hidden class=anchor aria-hidden=true href=#reflections-on-tensorflow-in-the-context-of-the-modern-ai-engine-and-the-evolving-role-of-data-scientists>#</a></h3><p>Throughout my journey in the world of data, I‚Äôve witnessed many changes ‚Äî some tools fading out of popularity while others take the spotlight. R, for example, has become more niche, used mostly by statisticians and academics. Flask, once a common choice for lightweight APIs, gradually gave way to FastAPI thanks to its modularity and support for asynchronous features, redefining how APIs are designed and deployed.</p><p>While these shifts are worth noting, today‚Äôs post focuses on TensorFlow, not just as a framework, but as a case study of how the need for rapid experimentation and testing environments is reshaping the AI landscape. Ultimately, it‚Äôs a reflection on innovation and the importance of tools that are close to deployment, yet don‚Äôt require massive compute power to start building.</p><h3 id=the-book-that-started-it-all>The Book That Started It All<a hidden class=anchor aria-hidden=true href=#the-book-that-started-it-all>#</a></h3><p>One book that significantly influenced my career as a data scientist is Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur√©lien G√©ron. In fact, I own all three editions, and I frequently revisit them both for my work as an MLOps engineer‚Äîwhen optimizing models‚Äîand in my role as an educator.</p><p>Thanks to this book, I was able to internalize critical concepts such as:</p><ul><li><p>How do you know when a model has reached its performance ceiling?</p></li><li><p>How do you update a model when the data distribution shifts?</p></li></ul><p>This foundational knowledge led me into the world of Deep Learning, which continues to dominate the market today, especially with the rise of LLMs.</p><h3 id=tensorflow-a-defining-path>TensorFlow: A Defining Path<a hidden class=anchor aria-hidden=true href=#tensorflow-a-defining-path>#</a></h3><p>That book also introduced me to TensorFlow, a robust and well-documented framework backed by Google. It proved invaluable in my early Edge Machine Learning projects‚Äîan area I remain passionate about.</p><p>But not everything was smooth. TensorFlow 1.x had several major drawbacks: static graphs made debugging difficult, the syntax lacked the clarity of idiomatic Python, and the learning curve was steep. While TensorFlow 2.x addressed many of these issues, it didn‚Äôt offer seamless migration for TF1 projects, creating additional friction within the community.</p><p>Its technical complexity and tightly coupled architecture ultimately hindered broader adoption, especially in a Python-dominated developer ecosystem.</p><h3 id=the-rise-of-pytorch>The Rise of PyTorch<a hidden class=anchor aria-hidden=true href=#the-rise-of-pytorch>#</a></h3><p>A turning point came in May 2021, when PyTorch overtook TensorFlow in search volume. Several factors contributed to this shift:</p><ol><li><p>NLP and LLMs: Most models on Hugging Face were built with PyTorch.</p></li><li><p>Academic Research: Over 95% of models on Papers with Code used PyTorch, compared to only 5% with TensorFlow.</p></li><li><p>Startups and Innovation: PyTorch‚Äôs dynamic graph execution, fully Pythonic design, and ease of debugging made it the go-to choice for rapid prototyping. It became the standard in agile environments, giving rise to the famous quote: &ldquo;We went from experimenting in weeks to days.&rdquo;</p></li></ol><h3 id=2022-the-tipping-point>2022: The Tipping Point<a hidden class=anchor aria-hidden=true href=#2022-the-tipping-point>#</a></h3><p>By 2022, PyTorch had clearly become the dominant framework. According to Hugging Face:</p><ul><li><p>92% of models were trained using PyTorch.</p></li><li><p>Only 14% supported TensorFlow.</p></li></ul><p>Companies like OpenAI had already migrated completely to PyTorch by 2020, with many others following suit. Even in fields like Computer Vision, PyTorch established itself as the framework of choice (a topic for another post).</p><h3 id=googles-shift-toward-jax>Google‚Äôs Shift Toward JAX<a hidden class=anchor aria-hidden=true href=#googles-shift-toward-jax>#</a></h3><p>In parallel, Google began focusing its efforts on JAX/Flax, a framework geared more toward research. While JAX has not yet achieved mainstream adoption or stood out in model competitions, its emergence marked a strategic pivot‚ÄîGoogle began moving away from TensorFlow as its default deep learning platform.</p><p>A telling sign: the official TensorFlow YouTube channel significantly reduced its publishing frequency, and newer Google research releases were already being built with JAX.</p><h3 id=keras-new-role-beyond-tensorflow>Keras‚Äô New Role: Beyond TensorFlow<a hidden class=anchor aria-hidden=true href=#keras-new-role-beyond-tensorflow>#</a></h3><p>One final point worth noting is the evolution of Keras. With version 3, it‚Äôs no longer exclusive to TensorFlow. It is now multi-backend, supporting PyTorch, JAX, and of course, TensorFlow. This change allows tools like Keras Tuner to be used across different frameworks, promoting more flexible, framework-agnostic development workflows.</p><h3 id=why-tensorflow-lost-ground-my-perspective>Why TensorFlow Lost Ground (My Perspective)<a hidden class=anchor aria-hidden=true href=#why-tensorflow-lost-ground-my-perspective>#</a></h3><p>Taken together, these shifts led to TensorFlow‚Äôs gradual decline in both research and production. Despite Google‚Äôs powerful TPU infrastructure‚Äîwhich works well with TensorFlow‚Äîthe broader market and research community moved in a different direction.</p><p>To me, the breaking point wasn‚Äôt just the lack of backward compatibility or internal complexity‚Äîit was the lack of Pythonic elegance. In a field dominated by Python developers, this became a critical flaw.</p><p>The battle may have been lost‚Äînot because TensorFlow lacked potential, but because the needs of the ecosystem evolved faster than the framework itself.</p><h3 id=a-fun-fact-interesting-tidbit>A fun fact (interesting tidbit)<a hidden class=anchor aria-hidden=true href=#a-fun-fact-interesting-tidbit>#</a></h3><p>There&rsquo;s something that caught my attention I&rsquo;m going to present two graphs. The first one shows the trend according to Google Trends, where the blue line is TensorFlow and the red one is PyTorch. I had talked about the decline in usage of the framework, and you can see how it&rsquo;s falling, but what particularly catches my attention is that there&rsquo;s a downward trend for PyTorch in 2025. I don&rsquo;t know if it has something to do with Rust, or if researchers and developers have fewer publications, considering we&rsquo;re in an era where model releases or versions are an everyday occurrence.
<img loading=lazy src="https://github.com/carlosjimenez88M/carlosjimenez88m.github.io/blob/master/img/comparative.png?raw=true"></p><p>In the second graph, also obtained from Google Trends, something else caught my attention even more: In Latin America, Africa, and Eastern Europe, TensorFlow still predominates (there&rsquo;s something to infer from that), although in India&rsquo;s case, usage is 59% TF and the rest PyTorch, which means that in industry it continues to be applied aggressively. It would be worth analyzing which industries these are. I don&rsquo;t know if it has something to do with Vision Language Models, but well, these are the data points and they should be shared.
<img loading=lazy src="https://github.com/carlosjimenez88M/carlosjimenez88m.github.io/blob/master/img/tf2.png?raw=true"></p><h3 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h3><ul><li><p>Deep Learning reached a stage where graph flexibility and computational efficiency became decisive factors in framework adoption.</p></li><li><p>As in all things, technological Darwinism prevailed‚Äîdriven by LLMs, agentic systems, and RAG-based architectures, experimentation speed and model development agility became more valuable than raw power.</p></li><li><p>Startups and the industry as a whole paved the path toward innovation, and while TensorFlow fought many worthy battles, its future might lie in niche areas like domotics and Edge AI.</p></li><li><p>The key takeaway is this: in machine learning, development speed and team-wide standardization outweigh personal preferences. I may personally enjoy working with TensorFlow, but in production environments, that choice could come at a cost if it‚Äôs not aligned with the team&rsquo;s common stack.</p></li></ul><p>That‚Äôs all for this entry‚Äî
I hope you enjoyed these reflections!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://carlosdanieljimenez.com/tags/tensorflow/>Tensorflow</a></li><li><a href=https://carlosdanieljimenez.com/tags/pytorch/>Pytorch</a></li><li><a href=https://carlosdanieljimenez.com/tags/mlflow/>Mlflow</a></li><li><a href=https://carlosdanieljimenez.com/tags/edge-ai/>Edge-Ai</a></li><li><a href=https://carlosdanieljimenez.com/tags/deep-learning/>Deep-Learning</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://carlosdanieljimenez.com/>The Probability Engine</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div class=newsletter-cta><div class=newsletter-content><h3>üì¨ ¬øTe sirvi√≥ esto?</h3><p>Escribo sobre MLOps, Edge AI y c√≥mo hacer que los modelos funcionen fuera del laboratorio.
Un email al mes, m√°ximo. Sin spam, sin pitch de cursos, solo contenido t√©cnico.</p><form action=https://buttondown.com/api/emails/embed-subscribe/carlosjimenez88m method=post class=newsletter-form target=popupwindow onsubmit='window.open("https://buttondown.com/carlosjimenez88m","popupwindow")'><div class=form-group><input type=email name=email id=bd-email placeholder=tu@email.com required aria-label="Email address">
<input type=submit value=Suscribirse></div></form><p class=newsletter-stats>Ya somos ingenieros construyendo ML y sistemas basados en Edge Computing.</p></div></div><style>.newsletter-cta{margin:3rem auto 2rem;padding:2rem;max-width:650px;background:var(--entry);border:1px solid var(--border);border-radius:8px;text-align:center}.newsletter-content h3{margin:0 0 1rem;font-size:1.5rem;color:var(--primary)}.newsletter-content p{margin:0 0 1.5rem;color:var(--secondary);line-height:1.6}.newsletter-form{margin:1.5rem 0}.form-group{display:flex;gap:.5rem;max-width:500px;margin:0 auto;flex-wrap:wrap;justify-content:center}.newsletter-form input[type=email]{flex:1;min-width:250px;padding:.75rem 1rem;font-size:1rem;border:1px solid var(--border);border-radius:6px;background:var(--theme);color:var(--content);transition:border-color .2s ease}.newsletter-form input[type=email]:focus{outline:none;border-color:var(--primary);box-shadow:0 0 0 3px rgba(var(--primary-rgb),.1)}.newsletter-form input[type=submit]{padding:.75rem 2rem;font-size:1rem;font-weight:600;color:#fff;background:var(--primary);border:none;border-radius:6px;cursor:pointer;transition:opacity .2s ease}.newsletter-form input[type=submit]:hover{opacity:.9}.newsletter-stats{font-size:.875rem;color:var(--secondary);margin-top:1rem;font-style:italic}@media screen and (max-width:600px){.newsletter-cta{padding:1.5rem 1rem;margin:2rem .5rem 1rem}.newsletter-content h3{font-size:1.25rem}.form-group{flex-direction:column;width:100%}.newsletter-form input[type=email],.newsletter-form input[type=submit]{width:100%;min-width:100%}}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>