<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach | The Probability Engine</title>
<meta name=keywords content="LLMs,NLP"><meta name=description content="Post Objective

Data cleaning and preliminary analysis process
Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs
Understanding the connections and most representative ideas within the document set

Intention
Understanding a story&rsquo;s behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the vector narrative arc combined with a literary map constitutes an interesting route to address increasingly common problems."><meta name=author content="Carlos Daniel Jim√©nez"><link rel=canonical href=https://carlosdanieljimenez.com/tidytuesday/2026-01-07-analisis/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=16x16 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=32x32 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=apple-touch-icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=mask-icon href=https://carlosdanieljimenez.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://carlosdanieljimenez.com/tidytuesday/2026-01-07-analisis/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=icon type=image/png href=/img/icon.jpeg><link rel=apple-touch-icon href=/img/icon.jpeg><link rel="shortcut icon" href=/img/icon.jpeg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://carlosdanieljimenez.com/tidytuesday/2026-01-07-analisis/"><meta property="og:site_name" content="The Probability Engine"><meta property="og:title" content="Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach"><meta property="og:description" content="Post Objective Data cleaning and preliminary analysis process Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs Understanding the connections and most representative ideas within the document set Intention Understanding a story‚Äôs behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the vector narrative arc combined with a literary map constitutes an interesting route to address increasingly common problems."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="tidytuesday"><meta property="article:published_time" content="2026-01-07T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-07T00:00:00+00:00"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach"><meta name=twitter:description content="Post Objective

Data cleaning and preliminary analysis process
Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs
Understanding the connections and most representative ideas within the document set

Intention
Understanding a story&rsquo;s behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the vector narrative arc combined with a literary map constitutes an interesting route to address increasingly common problems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"TidyTuesday","item":"https://carlosdanieljimenez.com/tidytuesday/"},{"@type":"ListItem","position":2,"name":"Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach","item":"https://carlosdanieljimenez.com/tidytuesday/2026-01-07-analisis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach","name":"Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach","description":"Post Objective Data cleaning and preliminary analysis process Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs Understanding the connections and most representative ideas within the document set Intention Understanding a story\u0026rsquo;s behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the vector narrative arc combined with a literary map constitutes an interesting route to address increasingly common problems.\n","keywords":["LLMs","NLP"],"articleBody":"Post Objective Data cleaning and preliminary analysis process Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs Understanding the connections and most representative ideas within the document set Intention Understanding a story‚Äôs behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the vector narrative arc combined with a literary map constitutes an interesting route to address increasingly common problems.\nThe problems this duo solves range from defining more precise categories in user preferences (for example, bohemian rock or slow psychological horror), understanding their emotional digital footprint, to detecting plagiarism by inspiration‚Äîas in the Coldplay vs. Joe Satriani case with the song ‚ÄúViva la Vida‚Äù‚Äîwhere graphs can validate semantic similarity or structural plagiarism.\nGenerally, this technique is not widely discussed or popularized given its hybrid approach, since it entails statistical rigor in verifying the purity of the clusters with which TSEs are constructed in the context of embeddings (this is one of the reasons for the model selection itself; I will detail this more precisely later).\nThe Geometry of Meaning Meaning can be represented as a physical location in multidimensional space by transforming text fragments into Embeddings.\nThe Mathematics If $w$ is our text, the embedding function $f$ transforms it into a vector $v$:\n$$v = f(w) \\in \\mathbb{R}^d$$\nWhere $d$ is the model‚Äôs dimension (in the case of text-embedding-004, $d=768$).\nEmbeddings have algebraic properties. If the model is good, the mathematical operation:\n$$\\text{Vector}(\\text{King}) - \\text{Vector}(\\text{Man}) + \\text{Vector}(\\text{Woman}) \\approx \\text{Vector}(\\text{Queen})$$\nThis proves the model captured semantics (gender and hierarchy) as directions in space.\nThese meanings have interesting properties such that when using a similarity measure:\nThe Formula Given two text vectors $A$ and $B$, their similarity is:\n$$\\text{similarity}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\nWe obtain:\nIf the result is 1: They are semantic twins If it‚Äôs 0: They are orthogonal (unrelated) If it‚Äôs -1: They are semantic opposites Up to this point, we‚Äôve mentioned some classic concepts from NLP and LLM theory. From here comes the interesting part: analyzing grammatical influence in a network. Semantic graphs work to transform a series of documentations or texts into fragments that describe edges that can evaluate semantic similarity, resulting in the influence of one text over another or, in other words, Nearest Neighbor Search:\n$$E_{ij} = \\begin{cases} \\text{similarity}(V_i, V_j) \u0026 \\text{if similarity}(V_i, V_j) \u003e \\text{threshold} \\\\ 0 \u0026 \\text{if similarity}(V_i, V_j) \\leq \\text{threshold} \\end{cases}$$\nVonnegut‚Äôs Theory Kurt Vonnegut in 1981 proposed a central idea about literary works: although each story is unique, emotional patterns are recognizable (on a Y-axis, which we‚Äôll call semantic position) and repetitive (on an X-axis, which we‚Äôll call Narrative Time). Thanks to this, we can identify the emotional charge of a text. Some of the patterns he found based on this theory are:\n‚ÄúMan in a Hole‚Äù: The protagonist starts well, falls into problems, and then gets out of them ‚ÄúBoy Meets Girl‚Äù: Starts normal, improves with the romantic encounter, drops with separation, and rises again with reunion ‚ÄúFrom Bad to Worse‚Äù: Starts bad and ends worse‚Äîas in Kafka‚Äôs works ‚ÄúCinderella‚Äù: Rises, falls, and then rises even higher‚Äîthe archetypal story Now, what catches our attention is how this complements the Narrative Arc where events are mapped by emotion.\nFor the above to make sense, let‚Äôs develop the following exercise based on a tidytuesday proposed by the R community.\nImplementation Loading Libraries Let‚Äôs start by loading the libraries we‚Äôll use for the exercise and predefining some data visualization standards:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import re from sklearn.decomposition import PCA from sklearn.manifold import TSNE from sklearn.cluster import KMeans from sklearn.metrics.pairwise import cosine_similarity from scipy.cluster.hierarchy import dendrogram, linkage import google.generativeai as genai import warnings warnings.filterwarnings('ignore') plt.style.use('seaborn-v0_8-whitegrid') sns.set_palette(\"husl\") plt.rcParams['figure.dpi'] = 100 plt.rcParams['font.size'] = 10 Data Loading and Feature Engineering Next, we load the databases, apply feature engineering treatment, minimal data cleaning, and finally concatenation:\nchristmas_novel_text['text'] = christmas_novel_text['text'].replace(r'^\\s*$', np.nan, regex=True) full_dataset = christmas_novels\\ .merge(christmas_novel_text, left_on='gutenberg_id', right_on='gutenberg_id')\\ .dropna(subset=['text'])\\ .reset_index(drop=True)\\ .merge(christmas_novel_authors, left_on='gutenberg_author_id', right_on='gutenberg_author_id', how='left').reset_index(drop=True) full_dataset['birthdate'] = full_dataset['birthdate'].astype('Int64') full_dataset['deathdate'] = full_dataset['deathdate'].astype('Int64') full_dataset['author_age_at_death'] = full_dataset['deathdate'] - full_dataset['birthdate'] The intention of calculating authors‚Äô age at death was to validate some correlation between their narrative length and age. The result was null, but for experimental purposes, it functioned as an important theoretical arc‚Äîknowing whether longevity was reflected in their texts (remember, there‚Äôs a study saying Nobel Prize winners live several years longer than the societal average).\ngrp_novel_text[['author_age_at_death','word_count']].corr() # author_age_at_death word_count # author_age_at_death 1.000000 -0.070052 # word_count -0.070052 1.000000 Text Cleaning Now for the text cleaning part, we applied a function built with regular expressions as follows:\ndef clean_gutenberg_keep_punctuation(text): text = re.sub(r'\\[illustration:.*?\\]', '', text, flags=re.IGNORECASE) text = re.sub(r'\\[transcriber\\'?s? note:.*?\\]', '', text, flags=re.IGNORECASE) text = re.sub(r'\\[.*?\\]', '', text) text = re.sub(r'-{3,}', ' ', text) text = re.sub(r'\\s+', ' ', text) text = re.sub(r'_', ' ', text) text = re.sub(r'\u003c.*?\u003e', ' ', text) text = re.sub(r'\"', ' ', text) text = re.sub(r'\\* \\* \\* \\* \\*', ' ', text) return text.strip() Note that this is very specific to the texts we‚Äôre dealing with, so when concatenating, this will help prevent noise from being included in the texts, allowing us to work with purer embeddings.\ngrp_novel_text = ( full_dataset .assign(text=full_dataset['text'].fillna('').astype(str)) .groupby(['gutenberg_id','title','author','author_age_at_death'])['text'] .apply(lambda x: ' '.join(s.strip() for s in x if s.strip())) .reset_index() ) grp_novel_text['text'] = grp_novel_text['text'].apply(clean_gutenberg_keep_punctuation) grp_novel_text['text'] = grp_novel_text['text'].str.replace(r'\\s+', ' ', regex=True).str.lower().str.strip() grp_novel_text['word_count'] = grp_novel_text['text'].apply(lambda x: len(x.split())) grp_novel_text.head() Creating Embeddings Now we have a structured database, to which I intend to add one more step: aggregate or transform the text into embeddings. In this case, I‚Äôll work with models/text-embedding-004. Understanding the attention window that models have, I‚Äôll split while attempting to mathematize a sentiment scale by concept block that allows me to discuss or discover behavioral patterns within Christmas novels.\nWithout falling into romances, I selected this embedding model for the following reasons:\ni) Dimensional Elasticity: the most important dimensions are stored at the beginning of the vector, so the strong or explanatory variance is the first we‚Äôll encounter when working on this type of project (also known as ‚ÄúMatryoshka‚Äù Embeddings).\nii) Since graphs will be used to search for the idea or text segment that best describes the document cluster, this type of embedding stores or has better semantic resolution or MTEB scores.\ndef get_embedding_safe(text): try: result = genai.embed_content( model=\"models/text-embedding-004\", content=text ) return result['embedding'] except: return None def split_text(text, chunk_size=3000, overlap=100): chunks = [] start = 0 while start \u003c len(text): end = start + chunk_size chunk = text[start:end] chunks.append(chunk) start += (chunk_size - overlap) return chunks all_chunks = [] print(\"Processing books...\") for index, row in grp_novel_text.iterrows(): book_chunks = split_text(row['text']) for i, chunk in enumerate(book_chunks): all_chunks.append({ 'book_title': row['title'], 'author': row['author'], 'chunk_id': i, 'text_chunk': chunk }) df_chunks = pd.DataFrame(all_chunks) df_chunks['embedding'] = df_chunks['text_chunk'].apply(get_embedding_safe) df_chunks.head() Finding Optimal Clusters Now comes an important part: after having embeddings, comes the search for their meaning. The general idea is to discover what exists in this taxonomy of texts and meanings. Remember that embeddings are concept maps where movement means something, and this is where their geometry makes sense.\nLet‚Äôs have a hypothesis about the Christmas Novel: it spans genres from horror to the religious representation of unity. By fragmenting the text and preserving certain elastic memory within chunks, we can see that there are moments, and each moment obeys a subgenre‚Äîand here comes the first finding:\nmatrix = np.vstack(df_chunks['embedding'].values) inertia = [] silhouette_scores = [] K_range = range(2, 13) for k in K_range: kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) kmeans.fit(matrix) inertia.append(kmeans.inertia_) score = silhouette_score(matrix, kmeans.labels_) silhouette_scores.append(score) fig, ax1 = plt.subplots(figsize=(10, 5)) color = 'tab:red' ax1.set_xlabel('Number of Clusters (k)') ax1.set_ylabel('Inertia (Lower is better)', color=color) ax1.plot(K_range, inertia, marker='o', color=color) ax1.tick_params(axis='y', labelcolor=color) ax2 = ax1.twinx() color = 'tab:blue' ax2.set_ylabel('Silhouette Score (Higher is better)', color=color) ax2.plot(K_range, silhouette_scores, marker='x', color=color) ax2.tick_params(axis='y', labelcolor=color) plt.title('Analysis to find optimal K: Elbow vs Silhouette') plt.grid(True) plt.show() Mathematically, it shows there are two genres in the clusters‚Äîprobably the author with the greatest presence and the rest of the authors. But following the genre thesis, we‚Äôll set k=4 to see what happens with the map design from a t-SNE, where we seek to understand semantic proximity:\nLiterary Cartography with t-SNE kmeans_final = KMeans(n_clusters=K_OPTIMO, random_state=42, n_init=10) df_chunks['cluster'] = kmeans_final.fit_predict(matrix) # 4. 2D VISUALIZATION (t-SNE) # t-SNE reduces the 768 dimensions to 2 (X and Y) while respecting semantic proximity print(\"Running t-SNE for dimensionality reduction...\") tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='random') proyeccion_2d = tsne.fit_transform(matrix) df_chunks['x'] = proyeccion_2d[:, 0] df_chunks['y'] = proyeccion_2d[:, 1] plt.figure(figsize=(12, 8)) scatter = plt.scatter(df_chunks['x'], df_chunks['y'], c=df_chunks['cluster'], cmap='viridis', alpha=0.6, s=10) plt.colorbar(scatter, label='Cluster ID') for i in range(0, len(df_chunks), 100): # Label 1 out of every 100 points to avoid saturation plt.text(df_chunks['x'].iloc[i], df_chunks['y'].iloc[i], df_chunks['book_title'].iloc[i][:15], fontsize=8, alpha=0.7) plt.title(f'Semantic Map of Books (t-SNE with {K_OPTIMO} clusters)') plt.xlabel('t-SNE Dimension 1') plt.ylabel('t-SNE Dimension 2') plt.show() This result is what‚Äôs known as literary cartography, where the findings are:\nThe Continent of the Supernatural (Cluster 3): Where Dickens‚Äô ghosts dwell‚Äîan isolated and dense group The Valley of Tears (Cluster 0): Where texts about silent tragedy, lies, and social conflict reside The Hill of Duty (Cluster 2): Texts centered on morality, homeland, and honor The Garden of Romance (Cluster 1): Feelings, flowers, and happy endings With this, we achieve semantic archaeology on the Christmas novel reduced to 4 themes. But the question that follows is: how do we measure the emotion of each story and, finally, which ones might have been inspired by others?\nThe Behavior of a Story from sklearn.decomposition import PCA matrix = np.vstack(df_chunks['embedding'].values) pca = PCA(n_components=1, random_state=42) narrative_axis = pca.fit_transform(matrix) df_chunks['vonnegut_axis'] = narrative_axis book1 = \"A Christmas Carol in Prose; Being a Ghost Story of Christmas\" # The classic book2 = \"Mr. Blake's Walking-Stick: A Christmas Story for Boys and Girls\" # The flat y1 = df_chunks[df_chunks['book_title'] == book1]['vonnegut_axis'].values y2 = df_chunks[df_chunks['book_title'] == book2]['vonnegut_axis'].values def smooth(y, box_pts): box = np.ones(box_pts)/box_pts y_smooth = np.convolve(y, box, mode='same') return y_smooth plt.figure(figsize=(14, 6)) plt.plot(smooth(y1, 5), label='A Christmas Carol (Dynamic Arc)', color='#E63946', linewidth=3) plt.plot(smooth(y2, 5), label=\"Mr. Blake's Walking-Stick (Flat Arc)\", color='#457B9D', linestyle='--', linewidth=2) plt.title('The Shape of Stories (Vonnegut Vectorial Analysis)', fontsize=16) plt.xlabel('Narrative Time (Book Progress)', fontsize=12) plt.ylabel('Emotional State (Principal Component 1)', fontsize=12) plt.legend() plt.grid(True, alpha=0.3) plt.show() The above image is what‚Äôs called Narrative Dynamism, where mathematically different emotions are described (imagine sadness, fear, and then a moment of joy due to some situation in the story) as represented by the red line, while the blue represents an emotional temperature that doesn‚Äôt change, like a linear story.\nTherefore, this method helps determine what‚Äôs worth reading and what has boring behavior, so to speak. There‚Äôs something important about this method‚Äôs applications that hasn‚Äôt been mentioned: with this technique, you can evaluate when a story is losing coherence (aggressive fluctuations).\nImportant note: pca=1 to give weight to the work‚Äôs main theme being studied.\nGraph-Based Inspiration Given the previous steps, we have: sub-themes within stories, context thanks to embeddings, narrative evolution thanks to PCA and seeing peaks and valleys. Now we need to understand the bridges of similarity.\nWith this, I seek to understand which embedding vectors connect at which point given the stories or text fragments, but not viewing it from how many words they have in common, but rather which ideas are similar. Therefore, what we‚Äôll work on next is:\nUnderstanding which authors can be compared through a directed filter Comparing paragraphs per author pair to understand existing iterations Finding fragments that have intentional or revealed semantic similarity With this, we can find where styles that authors use converge with others to say that their texts have influences from others, or in other words, reveal the influences that exist between Christmas literature.\nimport networkx as nx import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.metrics.pairwise import cosine_similarity top_authors = df_chunks['author'].value_counts().head(5).index.tolist() df_top = df_chunks[df_chunks['author'].isin(top_authors)].copy() matrix_top = matrix[df_top.index] print(f\"Analyzing connections between: {top_authors}\") sim_matrix_top = cosine_similarity(matrix_top) G = nx.Graph() connections_found = [] for i in range(len(top_authors)): for j in range(i + 1, len(top_authors)): auth_A = top_authors[i] auth_B = top_authors[j] idxs_A = np.where(df_top['author'] == auth_A)[0] idxs_B = np.where(df_top['author'] == auth_B)[0] sub_sim = sim_matrix_top[np.ix_(idxs_A, idxs_B)] max_idx_flat = np.argmax(sub_sim) r_local, c_local = np.unravel_index(max_idx_flat, sub_sim.shape) max_score = sub_sim[r_local, c_local] global_idx_A = idxs_A[r_local] global_idx_B = idxs_B[c_local] node_id_A = df_top.index[global_idx_A] G.add_node(node_id_A, label=auth_A, text=df_top.iloc[global_idx_A]['text_chunk'], type='fragment') node_id_B = df_top.index[global_idx_B] G.add_node(node_id_B, label=auth_B, text=df_top.iloc[global_idx_B]['text_chunk'], type='fragment') G.add_edge(node_id_A, node_id_B, weight=max_score) connections_found.append({ 'Auth1': auth_A, 'Auth2': auth_B, 'Score': max_score, 'Text1': df_top.iloc[global_idx_A]['text_chunk'][:100], 'Text2': df_top.iloc[global_idx_B]['text_chunk'][:100] }) plt.figure(figsize=(12, 8)) pos = nx.circular_layout(G) Evidence of Similarity (Real Texts) üîó McIntosh, Maria J. ‚Üî Thackeray, William Makepeace Similarity: 0.7623\nüìú McIntosh: ‚Äúd have done had her fingers trembled less. can you sing? elevated above all apprehension by the indi‚Ä¶‚Äù üìú Thackeray: ‚Äú, struck me with a terror which i cannot describe, and impressed me with the fact of the vast progre‚Ä¶‚Äù üîó McIntosh, Maria J. ‚Üî Finley, Martha Similarity: 0.7740\nüìú McIntosh: ‚Äúe interrupted, for all were busy in preparing for this important day. miss donaldson was superintend‚Ä¶‚Äù üìú Finley: ‚Äútiful. i‚Äôm sure everybody thinks so. don‚Äôt they, papa? as far as my knowledge goes, he answered, smi‚Ä¶‚Äù üîó McIntosh, Maria J. ‚Üî Allen, James Lane Similarity: 0.7815\nüìú McIntosh: ‚Äúcould have been so rigid in his observance of a soldier‚Äôs duty, yet so inexpressibly tender as a man‚Ä¶‚Äù üìú Allen: ‚Äú, it slipped from his hand and there was a loud clangor. she stepped quickly out upon the stone befo‚Ä¶‚Äù üîó McIntosh, Maria J. ‚Üî Hale, Edward Everett Similarity: 0.8113 (The highest!)\nüìú McIntosh: ‚Äúth, $3; cloth, gilt leaves, $4; morocco extra, $6. cheaper edition, with portrait and 4 plates. im. ‚Ä¶‚Äù üìú Hale: ‚Äúby mrs. harriet beecher stowe, mrs. a. d. t. whitney, miss lucretia hale, rev. e. e. hale, f. b. per‚Ä¶‚Äù üîó Thackeray, William Makepeace ‚Üî Finley, Martha Similarity: 0.7520\nüìú Thackeray: ‚Äúdavison! who is it? cried out miss raby, starting and turning as white as a sheet. i told her it wa‚Ä¶‚Äù üìú Finley: ‚Äúry nice old fellow, returned the little girl with an arch look and smile. so i‚Äôll hang mine up. and ‚Ä¶‚Äù üîó Thackeray, William Makepeace ‚Üî Allen, James Lane Similarity: 0.7663\nüìú Thackeray: ‚Äúme, and pledge a hand to all young friends, as fits the merry christmas time. on life‚Äôs wide scene y‚Ä¶‚Äù üîó Allen, James Lane ‚Üî Hale, Edward Everett Similarity: 0.7855\nüìú Allen: ‚Äúre most rapidly dying out in this civilization‚Äìthe shadow of that romance which for ages was the ea‚Ä¶‚Äù üìú Hale: ‚Äún those virgins arose and trimmed their lamps. and i will light them, said she aloud. that will save‚Ä¶‚Äù Interpreting the Findings 1. The ‚ÄúStructural‚Äù Finding: When AI Reads Format, Not Story The Case: McIntosh vs. Hale (similarity: 0.8113‚Äîthe highest!)\nMcIntosh text: ‚Äú‚Ä¶th, $3; cloth, gilt leaves, $4; morocco extra, $6. cheaper edition‚Ä¶‚Äù\nHale text: ‚Äú‚Ä¶by mrs. harriet beecher stowe‚Ä¶ rev. e. e. hale, f. b. per‚Ä¶‚Äù\nWhat happened here? A human would say they‚Äôre not similar. One talks about money and binding (‚Äúcloth‚Äù, ‚Äúgilt‚Äù); the other is a list of author names (‚ÄúStowe‚Äù, ‚ÄúHale‚Äù).\nThe embedding (004) says: ‚ÄúBoth are lists of editorial metadata.‚Äù\nThe brilliant interpretation: The model detected structure. Both are non-narrative fragments: catalogs, short lists, proper names, or figures. Mathematically, both ‚Äúbreak‚Äù the novel‚Äôs fluid narrative.\nConclusion: Your tool automatically separates ‚Äúcontent‚Äù (the story) from ‚Äúpackaging‚Äù (editorial advertising).\n2. The ‚ÄúTonal‚Äù Finding: When AI Detects Solemnity The Case: Allen vs. Hale (similarity: 0.7855)\nAllen text: ‚Äú‚Ä¶shadow of that romance which for ages was the ea‚Ä¶‚Äù\nHale text: ‚Äú‚Ä¶virgins arose and trimmed their lamps. and i will light them‚Ä¶‚Äù\nWhat happened here? Allen talks about ‚Äúancient romance.‚Äù Hale cites a biblical parable (the virgins and the lamps).\nThe embedding says: ‚ÄúBoth have an elevated and mystical tone.‚Äù\nThe brilliant interpretation: The model didn‚Äôt search for repeated words. It searched for the feeling of ‚Äúantiquity‚Äù, ‚Äúsolemnity‚Äù, and ‚Äúspirituality‚Äù. It connected romantic nostalgia with religion because both occupy the same place in the ‚ÄúVictorian sentiments space‚Äù.\nConclusion: The Invisible Architecture of Stories This exercise reveals a fundamental truth: literary narratives are living geometries. What Vonnegut perceived as artistic intuition in 1981‚Äîthat every story traces an emotional shape in space‚Äîtoday materializes as pure mathematics. We haven‚Äôt reduced art to numbers; we‚Äôve discovered that art was always geometry disguised as prose.\nThe Three Pillars of a New Reading 1. Literary geometry exists (and always did) Every word in ‚ÄúA Christmas Carol‚Äù occupies a precise position in a 768-dimensional space. It‚Äôs not metaphor: it‚Äôs topology. When Dickens wrote ‚Äúredemption,‚Äù he didn‚Äôt just choose a concept‚Äîhe chose a vector coordinate close to ‚Äúhope‚Äù and distant from ‚Äúdespair.‚Äù Literature is spatial navigation. Great authors are cartographers of emotional territories we can only now map.\n2. Machines see the bones beneath the skin of words While humans read surface‚Äîwords, phrases, metaphors‚Äîembeddings read deep structure. When the model connected an editorial catalog with a list of Victorian authors, it didn‚Äôt make a mistake: it discovered both shared the same syntactic architecture. It sees patterns we feel but don‚Äôt name. It‚Äôs like discovering two distant buildings were designed by the same invisible architect.\n3. Inspiration is a measurable ghost Semantic graphs don‚Äôt just trace obvious literary influences. They reveal phantom conversations between minds separated by centuries, dialoguing through archetypes that transcend languages, cultures, and vocabularies. Thackeray responding to a narrative gesture by McIntosh that she never consciously formulated. The literary tradition is a collective neural network, and we‚Äôre just learning to read it.\nApplication Horizons: Beyond Dickens This methodology transcends the Christmas corpus. Imagine it deployed in:\nTruth Engineering: Comparing the vector signature of verified narratives vs. structured disinformation. Lies have recognizable topologies.\nScreenplay Science: Predicting emotional resonance in audiences not by what a film says, but by how it breathes‚Äîits vector narrative arc.\nRecommendation by Emotional Architecture: ‚ÄúIf you were captivated by Breaking Bad‚Äôs dynamic arc, here are books with the same narrative curvature‚Äù‚Äîbeyond genre, we recommend by shape.\nCoherence Audits: Detecting the exact moment where a text loses its thread. Chaotic fluctuations in PCA are symptoms of narrative disorientation.\nComputational Cultural Archaeology: Tracking how themes‚ÄîChristmas, revolutionary, apocalyptic‚Äîevolve through centuries, mutating but preserving their vector core.\nThe Horizon: Attentional Engineering We call this attentional engineering: the rigorous study of how texts capture, sustain, and release human consciousness. Each peak in a narrative arc marks a moment where the author decided to tension the string of attention. Each cluster in the semantic map reveals a conceptual attractor basin‚Äîregions of idea space where certain thoughts naturally gravitate.\nThe civilizational promise: In an era of information overabundance, these techniques don‚Äôt replace human judgment‚Äîthey amplify it. Not to read for us, but to point us toward what deserves to be read. To find the needles of meaning in infinite haystacks of noise.\nAs Vonnegut observed four decades ago: every story has shape. Today, for the first time in human history, we can draw that shape with the same precision we measure galaxies.\nLiterature ceases to be a purely subjective art. It becomes a science of human experience‚Äîrigorous, measurable, but no less beautiful for it.\nThe map finally matches the territory.\nResources Complete code and datasets: [Link to GitHub]\nModels used: text-embedding-004 (Google), scikit-learn, networkx\nTheoretical inspiration: Kurt Vonnegut (1981), ‚ÄúThe Shape of Stories‚Äù\nIf you found this analysis interesting, consider applying these techniques to your own text corpus. The code is adaptable to any literary genre, from sci-fi to romance, from poetry to screenplays. The geometry of meaning awaits.\n","wordCount":"3221","inLanguage":"en","datePublished":"2026-01-07T00:00:00Z","dateModified":"2026-01-07T00:00:00Z","author":{"@type":"Person","name":"Carlos Daniel Jim√©nez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://carlosdanieljimenez.com/tidytuesday/2026-01-07-analisis/"},"publisher":{"@type":"Organization","name":"The Probability Engine","logo":{"@type":"ImageObject","url":"https://carlosdanieljimenez.com/img/icon.jpeg"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://carlosdanieljimenez.com/ accesskey=h title="The Probability Engine (Alt + H)">The Probability Engine</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://carlosdanieljimenez.com/mlops/ title=MLOps><span>MLOps</span></a></li><li><a href=https://carlosdanieljimenez.com/agentic-ai/ title="Agentic AI"><span>Agentic AI</span></a></li><li><a href=https://carlosdanieljimenez.com/tidytuesday/ title=TidyTuesday><span>TidyTuesday</span></a></li><li><a href=https://carlosdanieljimenez.com/post/ title=Posts><span>Posts</span></a></li><li><a href=https://carlosdanieljimenez.com/edge-computing/ title="Edge Computing"><span>Edge Computing</span></a></li><li><a href=https://carlosdanieljimenez.com/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://carlosdanieljimenez.com/about/ title=About><span>About</span></a></li><li><a href=https://carlosdanieljimenez.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Literary Mapping of Christmas Novels: A Vector Narrative Arc Approach</h1><div class=post-meta><span title='2026-01-07 00:00:00 +0000 UTC'>January 7, 2026</span>&nbsp;¬∑&nbsp;<span>Carlos Daniel Jim√©nez</span></div></header><div class=post-content><h2 id=post-objective>Post Objective<a hidden class=anchor aria-hidden=true href=#post-objective>#</a></h2><ul><li>Data cleaning and preliminary analysis process</li><li>Understanding the emotional charge or plot development of texts through semantic archaeology based on PCAs</li><li>Understanding the connections and most representative ideas within the document set</li></ul><h2 id=intention>Intention<a hidden class=anchor aria-hidden=true href=#intention>#</a></h2><p>Understanding a story&rsquo;s behavior at the level of its variance is a challenge addressed by attentional engineering. Therefore, using lesser-known methods such as the <strong>vector narrative arc</strong> combined with a <strong>literary map</strong> constitutes an interesting route to address increasingly common problems.</p><p>The problems this duo solves range from defining more precise categories in user preferences (for example, <em>bohemian rock</em> or <em>slow psychological horror</em>), understanding their emotional digital footprint, to detecting plagiarism by inspiration‚Äîas in the Coldplay vs. Joe Satriani case with the song &ldquo;Viva la Vida&rdquo;‚Äîwhere graphs can validate semantic similarity or structural plagiarism.</p><p>Generally, this technique is not widely discussed or popularized given its hybrid approach, since it entails statistical rigor in verifying the purity of the clusters with which TSEs are constructed in the context of embeddings (this is one of the reasons for the model selection itself; I will detail this more precisely later).</p><h2 id=the-geometry-of-meaning>The Geometry of Meaning<a hidden class=anchor aria-hidden=true href=#the-geometry-of-meaning>#</a></h2><p>Meaning can be represented as a physical location in multidimensional space by transforming text fragments into <strong>Embeddings</strong>.</p><h3 id=the-mathematics>The Mathematics<a hidden class=anchor aria-hidden=true href=#the-mathematics>#</a></h3><p>If <code>$w$</code> is our text, the embedding function <code>$f$</code> transforms it into a vector <code>$v$</code>:</p><p><code>$$v = f(w) \in \mathbb{R}^d$$</code></p><p>Where <code>$d$</code> is the model&rsquo;s dimension (in the case of <code>text-embedding-004</code>, <code>$d=768$</code>).</p><p>Embeddings have algebraic properties. If the model is good, the mathematical operation:</p><p><code>$$\text{Vector}(\text{King}) - \text{Vector}(\text{Man}) + \text{Vector}(\text{Woman}) \approx \text{Vector}(\text{Queen})$$</code></p><p>This proves the model captured <strong>semantics</strong> (gender and hierarchy) as directions in space.</p><p>These meanings have interesting properties such that when using a similarity measure:</p><h3 id=the-formula>The Formula<a hidden class=anchor aria-hidden=true href=#the-formula>#</a></h3><p>Given two text vectors <code>$A$</code> and <code>$B$</code>, their similarity is:</p><p><code>$$\text{similarity}(A, B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}$$</code></p><p>We obtain:</p><ul><li>If the result is <strong>1</strong>: They are semantic twins</li><li>If it&rsquo;s <strong>0</strong>: They are orthogonal (unrelated)</li><li>If it&rsquo;s <strong>-1</strong>: They are semantic opposites</li></ul><p>Up to this point, we&rsquo;ve mentioned some classic concepts from NLP and LLM theory. From here comes the interesting part: analyzing grammatical influence in a network. Semantic graphs work to transform a series of documentations or texts into fragments that describe edges that can evaluate semantic similarity, resulting in the influence of one text over another or, in other words, <strong>Nearest Neighbor Search</strong>:</p><p><code>$$E_{ij} = \begin{cases} \text{similarity}(V_i, V_j) & \text{if similarity}(V_i, V_j) > \text{threshold} \\ 0 & \text{if similarity}(V_i, V_j) \leq \text{threshold} \end{cases}$$</code></p><h2 id=vonneguts-theory>Vonnegut&rsquo;s Theory<a hidden class=anchor aria-hidden=true href=#vonneguts-theory>#</a></h2><p>Kurt Vonnegut in 1981 proposed a central idea about literary works: although each story is unique, emotional patterns are recognizable (on a Y-axis, which we&rsquo;ll call <strong>semantic position</strong>) and repetitive (on an X-axis, which we&rsquo;ll call <strong>Narrative Time</strong>). Thanks to this, we can identify the emotional charge of a text. Some of the patterns he found based on this theory are:</p><ul><li><strong>&ldquo;Man in a Hole&rdquo;</strong>: The protagonist starts well, falls into problems, and then gets out of them</li><li><strong>&ldquo;Boy Meets Girl&rdquo;</strong>: Starts normal, improves with the romantic encounter, drops with separation, and rises again with reunion</li><li><strong>&ldquo;From Bad to Worse&rdquo;</strong>: Starts bad and ends worse‚Äîas in Kafka&rsquo;s works</li><li><strong>&ldquo;Cinderella&rdquo;</strong>: Rises, falls, and then rises even higher‚Äîthe archetypal story</li></ul><p>Now, what catches our attention is how this complements the <strong>Narrative Arc</strong> where events are mapped by emotion.</p><p>For the above to make sense, let&rsquo;s develop the following exercise based on a <strong>tidytuesday</strong> proposed by the R community.</p><h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><h3 id=loading-libraries>Loading Libraries<a hidden class=anchor aria-hidden=true href=#loading-libraries>#</a></h3><p>Let&rsquo;s start by loading the libraries we&rsquo;ll use for the exercise and predefining some data visualization standards:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.manifold</span> <span class=kn>import</span> <span class=n>TSNE</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.cluster</span> <span class=kn>import</span> <span class=n>KMeans</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics.pairwise</span> <span class=kn>import</span> <span class=n>cosine_similarity</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.cluster.hierarchy</span> <span class=kn>import</span> <span class=n>dendrogram</span><span class=p>,</span> <span class=n>linkage</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>google.generativeai</span> <span class=k>as</span> <span class=nn>genai</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>warnings</span>
</span></span><span class=line><span class=cl><span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>style</span><span class=o>.</span><span class=n>use</span><span class=p>(</span><span class=s1>&#39;seaborn-v0_8-whitegrid&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>set_palette</span><span class=p>(</span><span class=s2>&#34;husl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;figure.dpi&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>rcParams</span><span class=p>[</span><span class=s1>&#39;font.size&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>10</span>
</span></span></code></pre></div><h3 id=data-loading-and-feature-engineering>Data Loading and Feature Engineering<a hidden class=anchor aria-hidden=true href=#data-loading-and-feature-engineering>#</a></h3><p>Next, we load the databases, apply feature engineering treatment, minimal data cleaning, and finally concatenation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>christmas_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>christmas_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;^\s*$&#39;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>nan</span><span class=p>,</span> <span class=n>regex</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>full_dataset</span> <span class=o>=</span> <span class=n>christmas_novels</span>\
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=n>christmas_novel_text</span><span class=p>,</span> <span class=n>left_on</span><span class=o>=</span><span class=s1>&#39;gutenberg_id&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>           <span class=n>right_on</span><span class=o>=</span><span class=s1>&#39;gutenberg_id&#39;</span><span class=p>)</span>\
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>])</span>\
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>reset_index</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>\
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=n>christmas_novel_authors</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>left_on</span><span class=o>=</span><span class=s1>&#39;gutenberg_author_id&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>           <span class=n>right_on</span><span class=o>=</span><span class=s1>&#39;gutenberg_author_id&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>           <span class=n>how</span><span class=o>=</span><span class=s1>&#39;left&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>reset_index</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;birthdate&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;birthdate&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;Int64&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;deathdate&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;deathdate&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;Int64&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;author_age_at_death&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;deathdate&#39;</span><span class=p>]</span> <span class=o>-</span> <span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;birthdate&#39;</span><span class=p>]</span>
</span></span></code></pre></div><p>The intention of calculating authors&rsquo; age at death was to validate some correlation between their narrative length and age. The result was null, but for experimental purposes, it functioned as an important theoretical arc‚Äîknowing whether longevity was reflected in their texts (remember, there&rsquo;s a study saying Nobel Prize winners live several years longer than the societal average).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>grp_novel_text</span><span class=p>[[</span><span class=s1>&#39;author_age_at_death&#39;</span><span class=p>,</span><span class=s1>&#39;word_count&#39;</span><span class=p>]]</span><span class=o>.</span><span class=n>corr</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#                    author_age_at_death  word_count</span>
</span></span><span class=line><span class=cl><span class=c1># author_age_at_death         1.000000    -0.070052</span>
</span></span><span class=line><span class=cl><span class=c1># word_count                 -0.070052     1.000000</span>
</span></span></code></pre></div><h3 id=text-cleaning>Text Cleaning<a hidden class=anchor aria-hidden=true href=#text-cleaning>#</a></h3><p>Now for the text cleaning part, we applied a function built with regular expressions as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>clean_gutenberg_keep_punctuation</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\[illustration:.*?\]&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>,</span> <span class=n>flags</span><span class=o>=</span><span class=n>re</span><span class=o>.</span><span class=n>IGNORECASE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\[transcriber</span><span class=se>\&#39;</span><span class=s1>?s? note:.*?\]&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>,</span> <span class=n>flags</span><span class=o>=</span><span class=n>re</span><span class=o>.</span><span class=n>IGNORECASE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\[.*?\]&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;-{3,}&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\s+&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;_&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;&lt;.*?&gt;&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;&#34;&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\* \* \* \* \*&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>text</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span></code></pre></div><p>Note that this is very specific to the texts we&rsquo;re dealing with, so when concatenating, this will help prevent noise from being included in the texts, allowing us to work with purer embeddings.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>grp_novel_text</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>full_dataset</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>text</span><span class=o>=</span><span class=n>full_dataset</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=s1>&#39;&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>str</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>groupby</span><span class=p>([</span><span class=s1>&#39;gutenberg_id&#39;</span><span class=p>,</span><span class=s1>&#39;title&#39;</span><span class=p>,</span><span class=s1>&#39;author&#39;</span><span class=p>,</span><span class=s1>&#39;author_age_at_death&#39;</span><span class=p>])[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>s</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span> <span class=k>for</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>x</span> <span class=k>if</span> <span class=n>s</span><span class=o>.</span><span class=n>strip</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>reset_index</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>grp_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>grp_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>clean_gutenberg_keep_punctuation</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>grp_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>grp_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\s+&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=n>regex</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>grp_novel_text</span><span class=p>[</span><span class=s1>&#39;word_count&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>grp_novel_text</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>split</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=n>grp_novel_text</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=creating-embeddings>Creating Embeddings<a hidden class=anchor aria-hidden=true href=#creating-embeddings>#</a></h2><p>Now we have a structured database, to which I intend to add one more step: aggregate or transform the text into embeddings. In this case, I&rsquo;ll work with <code>models/text-embedding-004</code>. Understanding the attention window that models have, I&rsquo;ll split while attempting to mathematize a sentiment scale by concept block that allows me to discuss or discover behavioral patterns within Christmas novels.</p><p>Without falling into romances, I selected this embedding model for the following reasons:</p><p><strong>i) Dimensional Elasticity</strong>: the most important dimensions are stored at the beginning of the vector, so the strong or explanatory variance is the first we&rsquo;ll encounter when working on this type of project (also known as <strong>&ldquo;Matryoshka&rdquo; Embeddings</strong>).</p><p><strong>ii)</strong> Since graphs will be used to search for the idea or text segment that best describes the document cluster, this type of embedding stores or has better semantic resolution or <strong>MTEB scores</strong>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_embedding_safe</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=n>genai</span><span class=o>.</span><span class=n>embed_content</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>=</span><span class=s2>&#34;models/text-embedding-004&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>content</span><span class=o>=</span><span class=n>text</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;embedding&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>split_text</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>chunk_size</span><span class=o>=</span><span class=mi>3000</span><span class=p>,</span> <span class=n>overlap</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>start</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>end</span> <span class=o>=</span> <span class=n>start</span> <span class=o>+</span> <span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>        <span class=n>chunk</span> <span class=o>=</span> <span class=n>text</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>chunk</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span> <span class=o>+=</span> <span class=p>(</span><span class=n>chunk_size</span> <span class=o>-</span> <span class=n>overlap</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>all_chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Processing books...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>index</span><span class=p>,</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>grp_novel_text</span><span class=o>.</span><span class=n>iterrows</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>book_chunks</span> <span class=o>=</span> <span class=n>split_text</span><span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>chunk</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>book_chunks</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>all_chunks</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;book_title&#39;</span><span class=p>:</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;author&#39;</span><span class=p>:</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;author&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;chunk_id&#39;</span><span class=p>:</span> <span class=n>i</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;text_chunk&#39;</span><span class=p>:</span> <span class=n>chunk</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>all_chunks</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;embedding&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;text_chunk&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>get_embedding_safe</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=finding-optimal-clusters>Finding Optimal Clusters<a hidden class=anchor aria-hidden=true href=#finding-optimal-clusters>#</a></h2><p>Now comes an important part: after having embeddings, comes the search for their meaning. The general idea is to discover what exists in this taxonomy of texts and meanings. Remember that embeddings are concept maps where movement means something, and this is where their geometry makes sense.</p><p>Let&rsquo;s have a hypothesis about the Christmas Novel: it spans genres from horror to the religious representation of unity. By fragmenting the text and preserving certain elastic memory within chunks, we can see that there are moments, and each moment obeys a subgenre‚Äîand here comes the first finding:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>matrix</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>(</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;embedding&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>values</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inertia</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>silhouette_scores</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>K_range</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>13</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>K_range</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                    <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>matrix</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>inertia</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>kmeans</span><span class=o>.</span><span class=n>inertia_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>silhouette_score</span><span class=p>(</span><span class=n>matrix</span><span class=p>,</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>labels_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>silhouette_scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax1</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;tab:red&#39;</span>
</span></span><span class=line><span class=cl><span class=n>ax1</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Number of Clusters (k)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax1</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Inertia (Lower is better)&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax1</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>inertia</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax1</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>labelcolor</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ax2</span> <span class=o>=</span> <span class=n>ax1</span><span class=o>.</span><span class=n>twinx</span><span class=p>()</span> 
</span></span><span class=line><span class=cl><span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;tab:blue&#39;</span>
</span></span><span class=line><span class=cl><span class=n>ax2</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Silhouette Score (Higher is better)&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax2</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>silhouette_scores</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax2</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>labelcolor</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Analysis to find optimal K: Elbow vs Silhouette&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>Mathematically, it shows there are two genres in the clusters‚Äîprobably the author with the greatest presence and the rest of the authors. But following the genre thesis, we&rsquo;ll set <code>k=4</code> to see what happens with the map design from a t-SNE, where we seek to understand semantic proximity:</p><h2 id=literary-cartography-with-t-sne>Literary Cartography with t-SNE<a hidden class=anchor aria-hidden=true href=#literary-cartography-with-t-sne>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>kmeans_final</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>K_OPTIMO</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;cluster&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>kmeans_final</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>matrix</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 2D VISUALIZATION (t-SNE)</span>
</span></span><span class=line><span class=cl><span class=c1># t-SNE reduces the 768 dimensions to 2 (X and Y) while respecting semantic proximity</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Running t-SNE for dimensionality reduction...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tsne</span> <span class=o>=</span> <span class=n>TSNE</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>perplexity</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>init</span><span class=o>=</span><span class=s1>&#39;random&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>proyeccion_2d</span> <span class=o>=</span> <span class=n>tsne</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>matrix</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>proyeccion_2d</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;y&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>proyeccion_2d</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>],</span> <span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;y&#39;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>                     <span class=n>c</span><span class=o>=</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;cluster&#39;</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Cluster ID&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>df_chunks</span><span class=p>),</span> <span class=mi>100</span><span class=p>):</span>  <span class=c1># Label 1 out of every 100 points to avoid saturation</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;y&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>             <span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;book_title&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>i</span><span class=p>][:</span><span class=mi>15</span><span class=p>],</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Semantic Map of Books (t-SNE with </span><span class=si>{</span><span class=n>K_OPTIMO</span><span class=si>}</span><span class=s1> clusters)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;t-SNE Dimension 1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;t-SNE Dimension 2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>This result is what&rsquo;s known as <strong>literary cartography</strong>, where the findings are:</p><ul><li><strong>The Continent of the Supernatural (Cluster 3)</strong>: Where Dickens&rsquo; ghosts dwell‚Äîan isolated and dense group</li><li><strong>The Valley of Tears (Cluster 0)</strong>: Where texts about silent tragedy, lies, and social conflict reside</li><li><strong>The Hill of Duty (Cluster 2)</strong>: Texts centered on morality, homeland, and honor</li><li><strong>The Garden of Romance (Cluster 1)</strong>: Feelings, flowers, and happy endings</li></ul><p>With this, we achieve semantic archaeology on the Christmas novel reduced to 4 themes. But the question that follows is: how do we measure the emotion of each story and, finally, which ones might have been inspired by others?</p><h2 id=the-behavior-of-a-story>The Behavior of a Story<a hidden class=anchor aria-hidden=true href=#the-behavior-of-a-story>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>matrix</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>(</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;embedding&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>values</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>narrative_axis</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>matrix</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;vonnegut_axis&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>narrative_axis</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>book1</span> <span class=o>=</span> <span class=s2>&#34;A Christmas Carol in Prose; Being a Ghost Story of Christmas&#34;</span>  <span class=c1># The classic</span>
</span></span><span class=line><span class=cl><span class=n>book2</span> <span class=o>=</span> <span class=s2>&#34;Mr. Blake&#39;s Walking-Stick: A Christmas Story for Boys and Girls&#34;</span>  <span class=c1># The flat</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y1</span> <span class=o>=</span> <span class=n>df_chunks</span><span class=p>[</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;book_title&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=n>book1</span><span class=p>][</span><span class=s1>&#39;vonnegut_axis&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl><span class=n>y2</span> <span class=o>=</span> <span class=n>df_chunks</span><span class=p>[</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;book_title&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=n>book2</span><span class=p>][</span><span class=s1>&#39;vonnegut_axis&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>smooth</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>box_pts</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>box</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>box_pts</span><span class=p>)</span><span class=o>/</span><span class=n>box_pts</span>
</span></span><span class=line><span class=cl>    <span class=n>y_smooth</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>convolve</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>box</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y_smooth</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>smooth</span><span class=p>(</span><span class=n>y1</span><span class=p>,</span> <span class=mi>5</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;A Christmas Carol (Dynamic Arc)&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;#E63946&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>smooth</span><span class=p>(</span><span class=n>y2</span><span class=p>,</span> <span class=mi>5</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Mr. Blake&#39;s Walking-Stick (Flat Arc)&#34;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;#457B9D&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;The Shape of Stories (Vonnegut Vectorial Analysis)&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Narrative Time (Book Progress)&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Emotional State (Principal Component 1)&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>The above image is what&rsquo;s called <strong>Narrative Dynamism</strong>, where mathematically different emotions are described (imagine sadness, fear, and then a moment of joy due to some situation in the story) as represented by the red line, while the blue represents an emotional temperature that doesn&rsquo;t change, like a linear story.</p><p>Therefore, this method helps determine what&rsquo;s worth reading and what has boring behavior, so to speak. There&rsquo;s something important about this method&rsquo;s applications that hasn&rsquo;t been mentioned: with this technique, you can evaluate when a story is losing coherence (aggressive fluctuations).</p><blockquote><p><strong>Important note</strong>: <code>pca=1</code> to give weight to the work&rsquo;s main theme being studied.</p></blockquote><h2 id=graph-based-inspiration>Graph-Based Inspiration<a hidden class=anchor aria-hidden=true href=#graph-based-inspiration>#</a></h2><p>Given the previous steps, we have: sub-themes within stories, context thanks to embeddings, narrative evolution thanks to PCA and seeing peaks and valleys. Now we need to understand the bridges of similarity.</p><p>With this, I seek to understand which embedding vectors connect at which point given the stories or text fragments, but not viewing it from how many words they have in common, but rather which ideas are similar. Therefore, what we&rsquo;ll work on next is:</p><ol><li>Understanding which authors can be compared through a directed filter</li><li>Comparing paragraphs per author pair to understand existing iterations</li><li>Finding fragments that have intentional or revealed semantic similarity</li></ol><p>With this, we can find where styles that authors use converge with others to say that their texts have influences from others, or in other words, reveal the influences that exist between Christmas literature.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>networkx</span> <span class=k>as</span> <span class=nn>nx</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics.pairwise</span> <span class=kn>import</span> <span class=n>cosine_similarity</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>top_authors</span> <span class=o>=</span> <span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;author&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>value_counts</span><span class=p>()</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>index</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>df_top</span> <span class=o>=</span> <span class=n>df_chunks</span><span class=p>[</span><span class=n>df_chunks</span><span class=p>[</span><span class=s1>&#39;author&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>isin</span><span class=p>(</span><span class=n>top_authors</span><span class=p>)]</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>matrix_top</span> <span class=o>=</span> <span class=n>matrix</span><span class=p>[</span><span class=n>df_top</span><span class=o>.</span><span class=n>index</span><span class=p>]</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Analyzing connections between: </span><span class=si>{</span><span class=n>top_authors</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sim_matrix_top</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>matrix_top</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>G</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>Graph</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>connections_found</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>top_authors</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>top_authors</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>auth_A</span> <span class=o>=</span> <span class=n>top_authors</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>auth_B</span> <span class=o>=</span> <span class=n>top_authors</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>idxs_A</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>df_top</span><span class=p>[</span><span class=s1>&#39;author&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=n>auth_A</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>idxs_B</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>df_top</span><span class=p>[</span><span class=s1>&#39;author&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=n>auth_B</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>sub_sim</span> <span class=o>=</span> <span class=n>sim_matrix_top</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>ix_</span><span class=p>(</span><span class=n>idxs_A</span><span class=p>,</span> <span class=n>idxs_B</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>max_idx_flat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>sub_sim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>r_local</span><span class=p>,</span> <span class=n>c_local</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>unravel_index</span><span class=p>(</span><span class=n>max_idx_flat</span><span class=p>,</span> <span class=n>sub_sim</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>max_score</span> <span class=o>=</span> <span class=n>sub_sim</span><span class=p>[</span><span class=n>r_local</span><span class=p>,</span> <span class=n>c_local</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>global_idx_A</span> <span class=o>=</span> <span class=n>idxs_A</span><span class=p>[</span><span class=n>r_local</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>global_idx_B</span> <span class=o>=</span> <span class=n>idxs_B</span><span class=p>[</span><span class=n>c_local</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>node_id_A</span> <span class=o>=</span> <span class=n>df_top</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=n>global_idx_A</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>G</span><span class=o>.</span><span class=n>add_node</span><span class=p>(</span><span class=n>node_id_A</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>auth_A</span><span class=p>,</span> <span class=n>text</span><span class=o>=</span><span class=n>df_top</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>global_idx_A</span><span class=p>][</span><span class=s1>&#39;text_chunk&#39;</span><span class=p>],</span> <span class=nb>type</span><span class=o>=</span><span class=s1>&#39;fragment&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>node_id_B</span> <span class=o>=</span> <span class=n>df_top</span><span class=o>.</span><span class=n>index</span><span class=p>[</span><span class=n>global_idx_B</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>G</span><span class=o>.</span><span class=n>add_node</span><span class=p>(</span><span class=n>node_id_B</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>auth_B</span><span class=p>,</span> <span class=n>text</span><span class=o>=</span><span class=n>df_top</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>global_idx_B</span><span class=p>][</span><span class=s1>&#39;text_chunk&#39;</span><span class=p>],</span> <span class=nb>type</span><span class=o>=</span><span class=s1>&#39;fragment&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>G</span><span class=o>.</span><span class=n>add_edge</span><span class=p>(</span><span class=n>node_id_A</span><span class=p>,</span> <span class=n>node_id_B</span><span class=p>,</span> <span class=n>weight</span><span class=o>=</span><span class=n>max_score</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>connections_found</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;Auth1&#39;</span><span class=p>:</span> <span class=n>auth_A</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;Auth2&#39;</span><span class=p>:</span> <span class=n>auth_B</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;Score&#39;</span><span class=p>:</span> <span class=n>max_score</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;Text1&#39;</span><span class=p>:</span> <span class=n>df_top</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>global_idx_A</span><span class=p>][</span><span class=s1>&#39;text_chunk&#39;</span><span class=p>][:</span><span class=mi>100</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;Text2&#39;</span><span class=p>:</span> <span class=n>df_top</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>global_idx_B</span><span class=p>][</span><span class=s1>&#39;text_chunk&#39;</span><span class=p>][:</span><span class=mi>100</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>pos</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>circular_layout</span><span class=p>(</span><span class=n>G</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=evidence-of-similarity-real-texts>Evidence of Similarity (Real Texts)<a hidden class=anchor aria-hidden=true href=#evidence-of-similarity-real-texts>#</a></h2><h3 id=-mcintosh-maria-j--thackeray-william-makepeace>üîó McIntosh, Maria J. ‚Üî Thackeray, William Makepeace<a hidden class=anchor aria-hidden=true href=#-mcintosh-maria-j--thackeray-william-makepeace>#</a></h3><p><strong>Similarity: 0.7623</strong></p><ul><li>üìú <strong>McIntosh</strong>: &ldquo;d have done had her fingers trembled less. can you sing? elevated above all apprehension by the indi&mldr;&rdquo;</li><li>üìú <strong>Thackeray</strong>: &ldquo;, struck me with a terror which i cannot describe, and impressed me with the fact of the vast progre&mldr;&rdquo;</li></ul><h3 id=-mcintosh-maria-j--finley-martha>üîó McIntosh, Maria J. ‚Üî Finley, Martha<a hidden class=anchor aria-hidden=true href=#-mcintosh-maria-j--finley-martha>#</a></h3><p><strong>Similarity: 0.7740</strong></p><ul><li>üìú <strong>McIntosh</strong>: &ldquo;e interrupted, for all were busy in preparing for this important day. miss donaldson was superintend&mldr;&rdquo;</li><li>üìú <strong>Finley</strong>: &ldquo;tiful. i&rsquo;m sure everybody thinks so. don&rsquo;t they, papa? as far as my knowledge goes, he answered, smi&mldr;&rdquo;</li></ul><h3 id=-mcintosh-maria-j--allen-james-lane>üîó McIntosh, Maria J. ‚Üî Allen, James Lane<a hidden class=anchor aria-hidden=true href=#-mcintosh-maria-j--allen-james-lane>#</a></h3><p><strong>Similarity: 0.7815</strong></p><ul><li>üìú <strong>McIntosh</strong>: &ldquo;could have been so rigid in his observance of a soldier&rsquo;s duty, yet so inexpressibly tender as a man&mldr;&rdquo;</li><li>üìú <strong>Allen</strong>: &ldquo;, it slipped from his hand and there was a loud clangor. she stepped quickly out upon the stone befo&mldr;&rdquo;</li></ul><h3 id=-mcintosh-maria-j--hale-edward-everett>üîó McIntosh, Maria J. ‚Üî Hale, Edward Everett<a hidden class=anchor aria-hidden=true href=#-mcintosh-maria-j--hale-edward-everett>#</a></h3><p><strong>Similarity: 0.8113</strong> <em>(The highest!)</em></p><ul><li>üìú <strong>McIntosh</strong>: &ldquo;th, $3; cloth, gilt leaves, $4; morocco extra, $6. cheaper edition, with portrait and 4 plates. im. &mldr;&rdquo;</li><li>üìú <strong>Hale</strong>: &ldquo;by mrs. harriet beecher stowe, mrs. a. d. t. whitney, miss lucretia hale, rev. e. e. hale, f. b. per&mldr;&rdquo;</li></ul><h3 id=-thackeray-william-makepeace--finley-martha>üîó Thackeray, William Makepeace ‚Üî Finley, Martha<a hidden class=anchor aria-hidden=true href=#-thackeray-william-makepeace--finley-martha>#</a></h3><p><strong>Similarity: 0.7520</strong></p><ul><li>üìú <strong>Thackeray</strong>: &ldquo;davison! who is it? cried out miss raby, starting and turning as white as a sheet. i told her it wa&mldr;&rdquo;</li><li>üìú <strong>Finley</strong>: &ldquo;ry nice old fellow, returned the little girl with an arch look and smile. so i&rsquo;ll hang mine up. and &mldr;&rdquo;</li></ul><h3 id=-thackeray-william-makepeace--allen-james-lane>üîó Thackeray, William Makepeace ‚Üî Allen, James Lane<a hidden class=anchor aria-hidden=true href=#-thackeray-william-makepeace--allen-james-lane>#</a></h3><p><strong>Similarity: 0.7663</strong></p><ul><li>üìú <strong>Thackeray</strong>: &ldquo;me, and pledge a hand to all young friends, as fits the merry christmas time. on life&rsquo;s wide scene y&mldr;&rdquo;</li></ul><h3 id=-allen-james-lane--hale-edward-everett>üîó Allen, James Lane ‚Üî Hale, Edward Everett<a hidden class=anchor aria-hidden=true href=#-allen-james-lane--hale-edward-everett>#</a></h3><p><strong>Similarity: 0.7855</strong></p><ul><li>üìú <strong>Allen</strong>: &ldquo;re most rapidly dying out in this civilization&ndash;the shadow of that romance which for ages was the ea&mldr;&rdquo;</li><li>üìú <strong>Hale</strong>: &ldquo;n those virgins arose and trimmed their lamps. and i will light them, said she aloud. that will save&mldr;&rdquo;</li></ul><h2 id=interpreting-the-findings>Interpreting the Findings<a hidden class=anchor aria-hidden=true href=#interpreting-the-findings>#</a></h2><h3 id=1-the-structural-finding-when-ai-reads-format-not-story>1. The &ldquo;Structural&rdquo; Finding: When AI Reads Format, Not Story<a hidden class=anchor aria-hidden=true href=#1-the-structural-finding-when-ai-reads-format-not-story>#</a></h3><p><strong>The Case:</strong> McIntosh vs. Hale (similarity: 0.8113‚Äîthe highest!)</p><p><strong>McIntosh text:</strong> &ldquo;&mldr;th, $3; cloth, gilt leaves, $4; morocco extra, $6. cheaper edition&mldr;&rdquo;<br><strong>Hale text:</strong> &ldquo;&mldr;by mrs. harriet beecher stowe&mldr; rev. e. e. hale, f. b. per&mldr;&rdquo;</p><h4 id=what-happened-here>What happened here?<a hidden class=anchor aria-hidden=true href=#what-happened-here>#</a></h4><p>A human would say they&rsquo;re not similar. One talks about money and binding (&ldquo;cloth&rdquo;, &ldquo;gilt&rdquo;); the other is a list of author names (&ldquo;Stowe&rdquo;, &ldquo;Hale&rdquo;).</p><p>The embedding (004) says: <strong>&ldquo;Both are lists of editorial metadata.&rdquo;</strong></p><h4 id=the-brilliant-interpretation>The brilliant interpretation:<a hidden class=anchor aria-hidden=true href=#the-brilliant-interpretation>#</a></h4><p>The model detected <strong>structure</strong>. Both are non-narrative fragments: catalogs, short lists, proper names, or figures. Mathematically, both &ldquo;break&rdquo; the novel&rsquo;s fluid narrative.</p><p><strong>Conclusion:</strong> Your tool automatically separates &ldquo;content&rdquo; (the story) from &ldquo;packaging&rdquo; (editorial advertising).</p><h3 id=2-the-tonal-finding-when-ai-detects-solemnity>2. The &ldquo;Tonal&rdquo; Finding: When AI Detects Solemnity<a hidden class=anchor aria-hidden=true href=#2-the-tonal-finding-when-ai-detects-solemnity>#</a></h3><p><strong>The Case:</strong> Allen vs. Hale (similarity: 0.7855)</p><p><strong>Allen text:</strong> &ldquo;&mldr;shadow of that romance which for ages was the ea&mldr;&rdquo;<br><strong>Hale text:</strong> &ldquo;&mldr;virgins arose and trimmed their lamps. and i will light them&mldr;&rdquo;</p><h4 id=what-happened-here-1>What happened here?<a hidden class=anchor aria-hidden=true href=#what-happened-here-1>#</a></h4><p>Allen talks about &ldquo;ancient romance.&rdquo; Hale cites a biblical parable (the virgins and the lamps).</p><p>The embedding says: <strong>&ldquo;Both have an elevated and mystical tone.&rdquo;</strong></p><h4 id=the-brilliant-interpretation-1>The brilliant interpretation:<a hidden class=anchor aria-hidden=true href=#the-brilliant-interpretation-1>#</a></h4><p>The model didn&rsquo;t search for repeated words. It searched for the feeling of &ldquo;antiquity&rdquo;, &ldquo;solemnity&rdquo;, and &ldquo;spirituality&rdquo;. It connected romantic nostalgia with religion because both occupy the same place in the &ldquo;Victorian sentiments space&rdquo;.</p><h2 id=conclusion-the-invisible-architecture-of-stories>Conclusion: The Invisible Architecture of Stories<a hidden class=anchor aria-hidden=true href=#conclusion-the-invisible-architecture-of-stories>#</a></h2><p>This exercise reveals a fundamental truth: <strong>literary narratives are living geometries</strong>. What Vonnegut perceived as artistic intuition in 1981‚Äîthat every story traces an emotional shape in space‚Äîtoday materializes as pure mathematics. We haven&rsquo;t reduced art to numbers; we&rsquo;ve discovered that art was always geometry disguised as prose.</p><h3 id=the-three-pillars-of-a-new-reading>The Three Pillars of a New Reading<a hidden class=anchor aria-hidden=true href=#the-three-pillars-of-a-new-reading>#</a></h3><h4 id=1-literary-geometry-exists-and-always-did>1. Literary geometry exists (and always did)<a hidden class=anchor aria-hidden=true href=#1-literary-geometry-exists-and-always-did>#</a></h4><p>Every word in &ldquo;A Christmas Carol&rdquo; occupies a precise position in a 768-dimensional space. It&rsquo;s not metaphor: it&rsquo;s <strong>topology</strong>. When Dickens wrote &ldquo;redemption,&rdquo; he didn&rsquo;t just choose a concept‚Äîhe chose a vector coordinate close to &ldquo;hope&rdquo; and distant from &ldquo;despair.&rdquo; Literature is spatial navigation. Great authors are cartographers of emotional territories we can only now map.</p><h4 id=2-machines-see-the-bones-beneath-the-skin-of-words>2. Machines see the bones beneath the skin of words<a hidden class=anchor aria-hidden=true href=#2-machines-see-the-bones-beneath-the-skin-of-words>#</a></h4><p>While humans read surface‚Äîwords, phrases, metaphors‚Äîembeddings read <strong>deep structure</strong>. When the model connected an editorial catalog with a list of Victorian authors, it didn&rsquo;t make a mistake: it discovered both shared the same syntactic architecture. It sees patterns we feel but don&rsquo;t name. It&rsquo;s like discovering two distant buildings were designed by the same invisible architect.</p><h4 id=3-inspiration-is-a-measurable-ghost>3. Inspiration is a measurable ghost<a hidden class=anchor aria-hidden=true href=#3-inspiration-is-a-measurable-ghost>#</a></h4><p>Semantic graphs don&rsquo;t just trace obvious literary influences. They reveal <strong>phantom conversations</strong> between minds separated by centuries, dialoguing through archetypes that transcend languages, cultures, and vocabularies. Thackeray responding to a narrative gesture by McIntosh that she never consciously formulated. The literary tradition is a collective neural network, and we&rsquo;re just learning to read it.</p><h3 id=application-horizons-beyond-dickens>Application Horizons: Beyond Dickens<a hidden class=anchor aria-hidden=true href=#application-horizons-beyond-dickens>#</a></h3><p>This methodology transcends the Christmas corpus. Imagine it deployed in:</p><ul><li><p><strong>Truth Engineering</strong>: Comparing the vector signature of verified narratives vs. structured disinformation. Lies have recognizable topologies.</p></li><li><p><strong>Screenplay Science</strong>: Predicting emotional resonance in audiences not by what a film says, but by how it <strong>breathes</strong>‚Äîits vector narrative arc.</p></li><li><p><strong>Recommendation by Emotional Architecture</strong>: &ldquo;If you were captivated by <em>Breaking Bad&rsquo;s</em> dynamic arc, here are books with the same narrative curvature&rdquo;‚Äîbeyond genre, we recommend by <strong>shape</strong>.</p></li><li><p><strong>Coherence Audits</strong>: Detecting the exact moment where a text loses its thread. Chaotic fluctuations in PCA are symptoms of narrative disorientation.</p></li><li><p><strong>Computational Cultural Archaeology</strong>: Tracking how themes‚ÄîChristmas, revolutionary, apocalyptic‚Äîevolve through centuries, mutating but preserving their vector core.</p></li></ul><h3 id=the-horizon-attentional-engineering>The Horizon: Attentional Engineering<a hidden class=anchor aria-hidden=true href=#the-horizon-attentional-engineering>#</a></h3><p>We call this <strong>attentional engineering</strong>: the rigorous study of how texts capture, sustain, and release human consciousness. Each peak in a narrative arc marks a moment where the author decided to tension the string of attention. Each cluster in the semantic map reveals a <strong>conceptual attractor basin</strong>‚Äîregions of idea space where certain thoughts naturally gravitate.</p><p><strong>The civilizational promise</strong>: In an era of information overabundance, these techniques don&rsquo;t replace human judgment‚Äîthey <strong>amplify</strong> it. Not to read for us, but to <strong>point us toward what deserves to be read</strong>. To find the needles of meaning in infinite haystacks of noise.</p><p>As Vonnegut observed four decades ago: every story has shape. Today, for the first time in human history, <strong>we can draw that shape with the same precision we measure galaxies</strong>.</p><p>Literature ceases to be a purely subjective art. It becomes a science of human experience‚Äîrigorous, measurable, but no less beautiful for it.</p><p><strong>The map finally matches the territory.</strong></p><h2 id=resources>Resources<a hidden class=anchor aria-hidden=true href=#resources>#</a></h2><p><strong>Complete code and datasets</strong>: [Link to GitHub]<br><strong>Models used</strong>: <code>text-embedding-004</code> (Google), <code>scikit-learn</code>, <code>networkx</code><br><strong>Theoretical inspiration</strong>: Kurt Vonnegut (1981), &ldquo;The Shape of Stories&rdquo;</p><p><em>If you found this analysis interesting, consider applying these techniques to your own text corpus. The code is adaptable to any literary genre, from sci-fi to romance, from poetry to screenplays. The geometry of meaning awaits.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://carlosdanieljimenez.com/tags/llms/>LLMs</a></li><li><a href=https://carlosdanieljimenez.com/tags/nlp/>NLP</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://carlosdanieljimenez.com/>The Probability Engine</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div class=newsletter-cta><div class=newsletter-content><h3>üì¨ Did this help?</h3><p>I write about MLOps, Edge AI, and making models work outside the lab.
One email per month, max. No spam, no course pitches, just technical content.</p><form action=https://buttondown.com/api/emails/embed-subscribe/carlosjimenez88m method=post class=newsletter-form target=popupwindow onsubmit='window.open("https://buttondown.com/carlosjimenez88m","popupwindow")'><div class=form-group><input type=email name=email id=bd-email placeholder=your@email.com required aria-label="Email address">
<input type=submit value=Subscribe></div></form><p class=newsletter-stats>Join engineers building ML systems and Edge Computing infrastructure.</p></div></div><style>.newsletter-cta{margin:3rem auto 2rem;padding:2rem;max-width:650px;background:var(--entry);border:1px solid var(--border);border-radius:8px;text-align:center}.newsletter-content h3{margin:0 0 1rem;font-size:1.5rem;color:var(--primary)}.newsletter-content p{margin:0 0 1.5rem;color:var(--secondary);line-height:1.6}.newsletter-form{margin:1.5rem 0}.form-group{display:flex;gap:.5rem;max-width:500px;margin:0 auto;flex-wrap:wrap;justify-content:center}.newsletter-form input[type=email]{flex:1;min-width:250px;padding:.75rem 1rem;font-size:1rem;border:1px solid var(--border);border-radius:6px;background:var(--theme);color:var(--content);transition:border-color .2s ease}.newsletter-form input[type=email]:focus{outline:none;border-color:var(--primary);box-shadow:0 0 0 3px rgba(var(--primary-rgb),.1)}.newsletter-form input[type=submit]{padding:.75rem 2rem;font-size:1rem;font-weight:600;color:#fff;background:var(--primary);border:none;border-radius:6px;cursor:pointer;transition:opacity .2s ease}.newsletter-form input[type=submit]:hover{opacity:.9}.newsletter-stats{font-size:.875rem;color:var(--secondary);margin-top:1rem;font-style:italic}@media screen and (max-width:600px){.newsletter-cta{padding:1.5rem 1rem;margin:2rem .5rem 1rem}.newsletter-content h3{font-size:1.25rem}.form-group{flex-direction:column;width:100%}.newsletter-form input[type=email],.newsletter-form input[type=submit]{width:100%;min-width:100%}}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>