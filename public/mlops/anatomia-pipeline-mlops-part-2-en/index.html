<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure | The Probability Engine</title>
<meta name=keywords content="mlops,ci-cd,docker,fastapi,github-actions,wandb,mlflow"><meta name=description content="Part 2: CI/CD with GitHub Actions, W&amp;B vs MLflow comparison, complete containerization with Docker, and production-ready API architecture with FastAPI."><meta name=author content="Carlos Daniel Jiménez"><link rel=canonical href=https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-part-2-en/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=16x16 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=32x32 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=apple-touch-icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=mask-icon href=https://carlosdanieljimenez.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-part-2-en/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=icon type=image/png href=/img/icon.jpeg><link rel=apple-touch-icon href=/img/icon.jpeg><link rel="shortcut icon" href=/img/icon.jpeg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-part-2-en/"><meta property="og:site_name" content="The Probability Engine"><meta property="og:title" content="Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure"><meta property="og:description" content="Part 2: CI/CD with GitHub Actions, W&amp;B vs MLflow comparison, complete containerization with Docker, and production-ready API architecture with FastAPI."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="mlops"><meta property="article:published_time" content="2026-01-13T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-13T00:00:00+00:00"><meta property="article:tag" content="Mlops"><meta property="article:tag" content="Ci-Cd"><meta property="article:tag" content="Docker"><meta property="article:tag" content="Fastapi"><meta property="article:tag" content="Github-Actions"><meta property="article:tag" content="Wandb"><meta name=twitter:card content="summary"><meta name=twitter:title content="Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure"><meta name=twitter:description content="Part 2: CI/CD with GitHub Actions, W&amp;B vs MLflow comparison, complete containerization with Docker, and production-ready API architecture with FastAPI."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"MLOps Guides","item":"https://carlosdanieljimenez.com/mlops/"},{"@type":"ListItem","position":2,"name":"Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure","item":"https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-part-2-en/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure","name":"Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure","description":"Part 2: CI/CD with GitHub Actions, W\u0026B vs MLflow comparison, complete containerization with Docker, and production-ready API architecture with FastAPI.","keywords":["mlops","ci-cd","docker","fastapi","github-actions","wandb","mlflow"],"articleBody":" Complete MLOps Series: ← Part 1: Pipeline | Part 2 (current) | Part 3: Production →\n8. CI/CD with GitHub Actions: Complete Pipeline Automation Why CI/CD Is Critical in MLOps As an MLOps engineer, one of the biggest friction points is manual deployment. You’ve trained an excellent model on your laptop, but getting it to production requires:\nSSH to a server Manually copy files Install dependencies Cross your fingers Debug when something explodes GitHub Actions eliminates this. Every commit triggers an automated pipeline that:\nRuns tests Validates that code meets standards Trains the model (optional, in simple pipelines) Builds Docker images Deploys to Cloud Run/ECS/Kubernetes The CI/CD Architecture For This Project This project implements two separate workflows:\n1. PR Validation Workflow Trigger: Every pull request to main\nPurpose: Ensure code is production-ready before merging\n# .github/workflows/pr_validation.yaml name: PR Validation - Tests \u0026 Linting on: pull_request: branches: [main, master] paths: - 'src/**' - 'api/**' - 'tests/**' - 'pyproject.toml' - 'requirements.txt' jobs: lint: name: Lint Code runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Install uv run: pip install uv - name: Install dependencies run: | uv venv uv pip install -e . uv pip install ruff pytest pytest-cov - name: Run Ruff linter run: | source .venv/bin/activate ruff check src/ tests/ api/ - name: Run Ruff formatter check run: | source .venv/bin/activate ruff format --check src/ tests/ api/ unit-tests: name: Unit Tests runs-on: ubuntu-latest env: GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }} WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }} steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Install dependencies run: | pip install uv uv venv uv pip install -e . uv pip install pytest pytest-cov pytest-mock - name: Run unit tests with coverage run: | source .venv/bin/activate pytest tests/ -v \\ --cov=src \\ --cov=api/app \\ --cov-report=xml \\ --cov-report=term-missing \\ --cov-fail-under=70 - name: Upload coverage to Codecov uses: codecov/codecov-action@v4 with: file: ./coverage.xml flags: unittests name: codecov-umbrella integration-tests: name: Integration Tests (Pipeline E2E) runs-on: ubuntu-latest env: GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }} WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }} MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }} steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Authenticate to Google Cloud uses: google-github-actions/auth@v2 with: credentials_json: ${{ secrets.GCP_SA_KEY }} - name: Install dependencies run: | pip install uv uv venv uv pip install -e . - name: Run integration test (Steps 01-04) run: | source .venv/bin/activate python main.py main.execute_steps=[01_download_data,02_preprocessing_and_imputation,03_feature_engineering,04_segregation] timeout-minutes: 30 - name: Verify artifacts were created run: | gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/train/train.parquet gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/test/test.parquet Value for the MLOps engineer:\nPrevents broken merges: If tests fail, the PR cannot be merged Code standards: Ruff guarantees consistency (important when you have 5+ contributors) Coverage tracking: Codecov shows what percentage of code is covered by tests Fast feedback: You know in 5 minutes if your change broke something, not 3 hours later 2. Deployment Workflow Trigger: Push to main (after PR merge)\nPurpose: Build and deploy the API to production\n# .github/workflows/deploy_api.yaml name: Deploy API to Cloud Run on: push: branches: [main] paths: - 'api/**' - 'models/**' - '.github/workflows/deploy_api.yaml' env: PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} SERVICE_NAME: housing-price-api REGION: us-central1 jobs: build-and-deploy: name: Build Docker Image \u0026 Deploy runs-on: ubuntu-latest permissions: contents: read id-token: write steps: - name: Checkout code uses: actions/checkout@v4 - name: Authenticate to Google Cloud uses: google-github-actions/auth@v2 with: workload_identity_provider: ${{ secrets.WIF_PROVIDER }} service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }} - name: Set up Cloud SDK uses: google-github-actions/setup-gcloud@v2 - name: Configure Docker for GCR run: gcloud auth configure-docker gcr.io - name: Download trained model from GCS run: | mkdir -p api/models/trained gsutil cp gs://${{ secrets.GCS_BUCKET_NAME }}/models/trained/housing_price_model.pkl \\ api/models/trained/housing_price_model.pkl - name: Build Docker image run: | cd api docker build \\ --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \\ --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest \\ . - name: Push Docker image to GCR run: | docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest - name: Deploy to Cloud Run run: | gcloud run deploy ${{ env.SERVICE_NAME }} \\ --image gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \\ --platform managed \\ --region ${{ env.REGION }} \\ --allow-unauthenticated \\ --set-env-vars=\"GCS_BUCKET=${{ secrets.GCS_BUCKET_NAME }},WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}\" \\ --memory 2Gi \\ --cpu 2 \\ --max-instances 10 \\ --min-instances 1 \\ --timeout 300 - name: Get Cloud Run URL id: deploy-url run: | URL=$(gcloud run services describe ${{ env.SERVICE_NAME }} \\ --platform managed \\ --region ${{ env.REGION }} \\ --format 'value(status.url)') echo \"url=$URL\" \u003e\u003e $GITHUB_OUTPUT - name: Run smoke test run: | curl -X POST \"${{ steps.deploy-url.outputs.url }}/api/v1/predict\" \\ -H \"Content-Type: application/json\" \\ -d '{\"instances\":[{\"longitude\":-122.23,\"latitude\":37.88,\"housing_median_age\":41,\"total_rooms\":880,\"total_bedrooms\":129,\"population\":322,\"households\":126,\"median_income\":8.3252,\"ocean_proximity\":\"NEAR BAY\"}]}' - name: Notify deployment success if: success() run: | echo \"Deployment successful! API available at: ${{ steps.deploy-url.outputs.url }}\" Value for the MLOps engineer:\nZero-downtime deployment: Cloud Run does rolling updates automatically Easy rollback: If something explodes, you do gcloud run services update-traffic --to-revisions=PREVIOUS=100 Automatic smoke test: Verifies the API responds after deploy Image versioning: Each commit has its own Docker image tagged with SHA Secrets and Security CRITICAL: Never commit secrets to the repo. GitHub Actions uses GitHub Secrets to store:\nGCP_PROJECT_ID: GCP project ID GCS_BUCKET_NAME: GCS bucket name WANDB_API_KEY: W\u0026B API key GCP_SA_KEY: Service account key (JSON) for GCP authentication WIF_PROVIDER / WIF_SERVICE_ACCOUNT: Workload Identity Federation (more secure than SA keys) Configuration in GitHub:\nGo to repo → Settings → Secrets and variables → Actions Create each secret Workflows access them with ${{ secrets.SECRET_NAME }} Deployment Monitoring How to know if a deployment failed?\nGitHub Actions sends notifications to:\nEmail (configured in GitHub profile) Slack (with GitHub app) Discord/Teams (with webhooks) Post-deployment monitoring:\n# Add post-deploy validation step - name: Run API health check run: | for i in {1..5}; do STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" \"${{ steps.deploy-url.outputs.url }}/health\") if [ $STATUS -eq 200 ]; then echo \"Health check passed\" exit 0 fi echo \"Attempt $i failed, retrying...\" sleep 10 done echo \"Health check failed after 5 attempts\" exit 1 Advanced CI/CD Strategies 1. Automatic Retraining Pipeline Trigger: Cron schedule (example: weekly)\non: schedule: - cron: '0 2 * * 0' # Every Sunday at 2 AM UTC jobs: retrain-model: runs-on: ubuntu-latest steps: - name: Run full pipeline run: python main.py - name: Compare metrics with production model run: | NEW_MAPE=$(python scripts/get_latest_mape.py) PROD_MAPE=$(python scripts/get_production_mape.py) if (( $(echo \"$NEW_MAPE \u003c $PROD_MAPE\" | bc -l) )); then echo \"New model is better, promoting to Production\" mlflow models transition --name housing_price_model --version latest --stage Production else echo \"New model is worse, keeping current Production model\" fi Value: The model automatically retrains with new data. If it improves, it’s promoted to Production. If it worsens, it’s discarded.\n2. Canary Deployments Problem: A new model may have subtle bugs that don’t appear in tests.\nSolution: Deploy the new model to only 10% of traffic, monitor for 1 hour, then migrate 100% if there are no errors.\n- name: Deploy canary (10% traffic) run: | gcloud run services update-traffic ${{ env.SERVICE_NAME }} \\ --to-revisions=LATEST=10,PREVIOUS=90 - name: Wait and monitor run: sleep 3600 # 1 hour - name: Check error rate run: | ERROR_RATE=$(python scripts/check_error_rate.py --minutes=60) if (( $(echo \"$ERROR_RATE \u003e 0.05\" | bc -l) )); then echo \"Error rate too high, rolling back\" gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=PREVIOUS=100 exit 1 fi - name: Promote to 100% traffic run: | gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=LATEST=100 What CI/CD Solves in MLOps Without CI/CD:\nManual deployment prone to errors “Works on my machine” syndrome Inconsistent testing Rollback requires panic debugging No history of what was deployed when With CI/CD:\nAutomatic deployment on every merge Tests guarantee code works Rollback is a command Complete history in GitHub Actions UI Every deployment is reproducible The Real Value For the MLOps Engineer It’s not about automating for the sake of automating. It’s about:\nReducing toil: You spend time solving interesting problems, not manually copying files Confidence: You know the code works before it reaches production Speed: From commit to production in \u003c10 minutes Auditing: Every change is logged in GitHub Collaboration: Your team can deploy without depending on you An MLOps engineer without CI/CD is like a software engineer without git—technically possible, but fundamentally broken.\n9. The Value of MLOps: Why This Matters The Central Question “Why should I invest time in all this when I can train a model in a notebook in 30 minutes?”\nThis is the question every MLOps engineer has heard. The short answer: because notebooks don’t scale.\nThe long answer is what this section covers.\nThe Real Problem: Research Code vs Production Code Research Code (Notebook) # notebook.ipynb # Cell 1 import pandas as pd df = pd.read_csv('housing.csv') # Cell 2 df = df.dropna() # Cell 3 from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor(n_estimators=100) model.fit(X_train, y_train) # Cell 4 import pickle pickle.dump(model, open('model.pkl', 'wb')) # Cell 5 # Wait, did I drop the right columns? # Let me rerun cell 2... oh no, I ran it twice # Now I have 0 rows, what happened? Problems:\nNot reproducible (execution order matters) Not testable Not versionable (git diffs are unreadable) Not scalable (what happens with 100GB of data?) Not auditable (what params did you use?) Production Code (This Pipeline) # src/model/05_model_selection/main.py @hydra.main(config_path=\".\", config_name=\"config\") def train(config: DictConfig) -\u003e None: \"\"\"Train model with versioned configuration.\"\"\" # Load data from GCS (single source of truth) df = load_from_gcs(config.gcs_train_path) # Apply serialized preprocessing pipeline pipeline = joblib.load('artifacts/preprocessing_pipeline.pkl') X = pipeline.transform(df) # Train with config params model = RandomForestRegressor(**config.hyperparameters) model.fit(X, y) # Log to MLflow mlflow.log_params(config.hyperparameters) mlflow.log_metrics(evaluate(model, X_test, y_test)) mlflow.sklearn.log_model(model, \"model\") return model Benefits:\nReproducible (same config = same output) Testable (pure functions, mocking) Versionable (readable git diff) Scalable (runs locally or on cluster) Auditable (MLflow tracking) Value #1: Code Modularization Why It Matters Scenario: Your model has a bug in preprocessing. In a notebook, preprocessing is mixed with feature engineering, training, and evaluation in 300 lines.\nIn this pipeline:\n# Bug is in preprocessing → only edit src/data/02_preprocessing/ # Tests fail → pytest tests/test_preprocessor.py # Fix → re-run only steps 02-07, not 01 Time saved: Hours per bug.\nSeparation of Concerns This pipeline separates:\nData steps (01-04): Produce reusable artifacts Model steps (05-07): Consume artifacts, produce models API: Consumes models, produces predictions Frontend: Consumes API, produces UX Benefit: Teams can work in parallel. The data scientist modifies feature engineering without touching the API. The frontend engineer modifies UI without understanding Random Forests.\nValue #2: Working with Artifacts The Problem: “Where is model_final_v3.pkl?” Without artifact management:\nmodels/ ├── model_v1.pkl ├── model_v2.pkl ├── model_final.pkl ├── model_final_FINAL.pkl ├── model_final_REAL.pkl ├── model_production_2024_01_15.pkl # Is this the production one? └── model_old_backup.pkl # Can I delete this? Problems:\nYou don’t know what hyperparameters each one uses You don’t know what metrics it achieved You don’t know what data it was trained on Rollback = searching for the correct file The Solution: Artifact Storage + Metadata 1. Google Cloud Storage for data:\ngs://bucket-name/ ├── data/ │ ├── 01-raw/housing.parquet # Immutable │ ├── 02-processed/housing_processed.parquet # Versioned by date │ ├── 03-features/housing_features.parquet │ └── 04-split/ │ ├── train/train.parquet │ └── test/test.parquet ├── artifacts/ │ ├── imputer.pkl # Preprocessing artifacts │ ├── preprocessing_pipeline.pkl │ └── scaler.pkl └── models/ └── trained/housing_price_model.pkl # Latest trained Benefits:\nImmutability: 01-raw/ never changes, you can always re-run the pipeline Versioning: Each run has a timestamp, you can compare versions Sharing: The whole team accesses the same data, not “send me the CSV via Slack” 2. MLflow for models:\n# Register model mlflow.sklearn.log_model(model, \"model\") # MLflow automatically saves: # - The model pickle # - The hyperparameters (n_estimators=200, max_depth=20) # - The metrics (MAPE=7.8%, R²=0.87) # - Metadata (date, duration, user) # - Code (git commit SHA) # Load model in production model = mlflow.pyfunc.load_model(\"models:/housing_price_model/Production\") Benefits:\nSemantic versioning: v1, v2, v3 with stages (Staging/Production) Rich metadata: You know exactly what each version is Trivial rollback: transition v2 to Production Comparison: MLflow UI shows table comparing all versions 3. W\u0026B for experiments:\n# Each sweep run logs: wandb.log({ \"hyperparameters/n_estimators\": 200, \"hyperparameters/max_depth\": 20, \"metrics/mape\": 7.8, \"metrics/r2\": 0.87, \"plots/feature_importances\": wandb.Image(fig), \"dataset/train_size\": 16512, }) # W\u0026B dashboard: # - Table with 50 sweep runs # - Filter by MAPE \u003c 8% # - Parallel coordinates plot showing relationship between hyperparameters and MAPE # - Compare top 5 runs side-by-side Benefits:\nVisualization: Interactive plots of how each hyperparameter affects metrics Collaboration: Your team sees your experiments in real-time Reproducibility: Each run has a permanent link with all context Value #3: Pipeline Architecture Why A Pipeline, Not A Script Single script (run_all.py):\n# run_all.py (500 lines) def main(): # Download data df = download_data() # Preprocess df = preprocess(df) # Feature engineering df = add_features(df) # Train model model = train_model(df) # Deploy deploy_model(model) Problems:\nIf it fails in train_model(), you re-run EVERYTHING (including slow download) You can’t run just feature engineering to experiment Changing preprocessing requires retraining everything No intermediate checkpoints Modular pipeline:\n# Run everything make run-pipeline # Run only preprocessing make run-preprocessing # Run from feature engineering onwards python main.py main.execute_steps=[03_feature_engineering,04_segregation,05_model_selection] # Debugging: run only the step that failed python src/data/03_feature_engineering/main.py --debug Benefits:\nSelective execution: Only re-run what changed Fast debugging: Test a step in isolation Parallelization: Independent steps can run in parallel Checkpointing: If step 05 fails, steps 01-04 are already done The Contract Between Steps Each step:\nInput: Path to artifact in GCS (example: data/02-processed/housing_processed.parquet) Output: Path to new artifact in GCS (example: data/03-features/housing_features.parquet) Side effects: Logs to MLflow/W\u0026B # Step 03: Feature Engineering def run(config): # Input df = load_from_gcs(config.gcs_input_path) # Transform df_transformed = apply_feature_engineering(df) # Output save_to_gcs(df_transformed, config.gcs_output_path) # Side effects mlflow.log_artifact(\"preprocessing_pipeline.pkl\") wandb.log({\"optimization/optimal_k\": 8}) This contract allows each step to be:\nTested independently Developed by different people Replaced without affecting other steps Value #4: Production-Ready vs Research Code Production-Ready Checklist Feature Research Code This Pipeline Versioning Git (badly, notebooks) Git + GCS + MLflow Testing Manual (“ran it once”) pytest + CI Configuration Hardcoded Versioned YAML Secrets Exposed in code .env + GitHub Secrets Logs print() statements Structured logging Monitoring “Hope it works” W\u0026B + MLflow tracking Deployment Manual Automatic CI/CD Rollback Panic debugging Transition in MLflow Documentation Outdated README Self-documented code + Markdown in MLflow Collaboration “Run these 10 cells in order” make run-pipeline The Real Cost of Not Doing MLOps Scenario: A model in production has a bug that causes incorrect predictions.\nWithout MLOps (Research Code):\nDetect the bug: User reports → 2 hours Reproduce the bug: Search for what code/data was used → 4 hours Fix: Run notebook locally → 1 hour Deploy: SSH, copy pickle, restart server → 30 min Verify: Run manual tests → 1 hour Total: 8.5 hours of downtime With MLOps (This Pipeline):\nDetect the bug: Automatic monitoring alerts → 5 min Rollback: transition v3 to Archived + transition v2 to Production → 2 min Fix: Identify issue with MLflow metadata, fix code → 1 hour Deploy: Push to GitHub → Automatic CI/CD → 10 min Verify: Automatic smoke tests pass → 1 min Total: 1 hour 18 min of downtime (\u003e85% reduction) Annual savings: If this happens 4 times a year, you save 29 hours of engineer time.\nValue #5: Data-Driven Decisions The Anti-Pattern “I used Random Forest with n_estimators=100 because that’s what everyone does.”\nProblem: You have no evidence it’s the best choice.\nThis Pipeline Every decision has quantifiable metrics:\n1. Imputation:\nCompared 4 strategies (Simple median, Simple mean, KNN, IterativeImputer) IterativeImputer won with RMSE=0.52 (vs 0.78 for median) Comparison plot in W\u0026B: wandb.ai/project/run/imputation_comparison 2. Feature Engineering:\nOptimized K from 5 to 15 K=8 maximized silhouette score (0.64) Elbow method plot in W\u0026B 3. Hyperparameter Tuning:\nBayesian sweep of 50 runs Optimal config: n_estimators=200, max_depth=20 MAPE improved from 8.5% to 7.8% Link to sweep: wandb.ai/project/sweeps/abc123 Benefit: Six months later, when the stakeholder asks “why do we use this model?”, you open W\u0026B/MLflow and the answer is there with plots and metrics.\nThe ROI of MLOps Initial investment:\nSetup of GCS, MLflow, W\u0026B, CI/CD: 2-3 days Refactoring code to modular pipeline: 1-2 weeks Return:\nDeployment time: 8 hours → 10 minutes (48x faster) Debugging time: 4 hours → 30 min (8x faster) Onboarding new engineers: 1 week → 1 day Team confidence: “Hope it works” → “I know it works” For a team of 5 people, breakeven is ~1 month.\nThe Final Lesson For MLOps Engineers It’s not about the tools. You can replace:\nGCS → S3 → Azure Blob MLflow → Neptune → Comet W\u0026B → TensorBoard → MLflow GitHub Actions → GitLab CI → Jenkins It’s about the principles:\nModularization: Code in testable modules, not monolithic notebooks Artifact Management: Versioned data and models, not model_final_v3.pkl Automation: CI/CD eliminates toil Observability: Logs, metrics, tracking Reproducibility: Same input → same output Data-driven decisions: Every choice backed by metrics When you understand this, you’re an MLOps engineer. When you implement it, you’re a good MLOps engineer.\n9.5. W\u0026B vs MLflow: Why Both, Not One or the Other The Uncomfortable Question “Why do you have Weights \u0026 Biases AND MLflow? Aren’t they the same?”\nThis question reveals a fundamental misunderstanding about what each tool does. They’re not competitors—they’re allies with different responsibilities. Understanding this separates a data scientist who experiments from an MLOps engineer who builds systems.\nThe short answer: W\u0026B is your research lab. MLflow is your production line.\nThe long answer is what this section covers, with examples from this project’s actual code.\nThe Real Problem: Experimentation vs Governance Phase 1: Experimentation (50-100 runs/day) When you’re in experimentation phase:\nYou run 50 sweep runs testing hyperparameter combinations You need to see in real-time how each run evolves You want to visually compare 20 runs simultaneously You need to see convergence plots, feature distributions, confusion matrices Logging overhead must be minimal (asynchronous logging) Correct tool: Weights \u0026 Biases\nPhase 2: Governance and Deployment (1-2 models/week) When you promote a model to production:\nYou need semantic versioning (v1, v2, v3) You need stages (Staging → Production) You need rich metadata (what hyperparameters? what data? what commit?) You need an API to load models (models:/housing_price_model/Production) You need trivial rollback (transition v2 to Production) Correct tool: MLflow Model Registry\nThe uncomfortable truth: No tool does both things well.\nHow This Project Uses W\u0026B 1. Hyperparameter Sweep (Step 06): Bayesian Optimization # src/model/06_sweep/main.py # Sweep configuration (Bayesian optimization) sweep_config = { \"method\": \"bayes\", # Bayesian \u003e Grid \u003e Random \"metric\": { \"name\": \"wmape\", \"goal\": \"minimize\" }, \"early_terminate\": { \"type\": \"hyperband\", \"min_iter\": 3 }, \"parameters\": { \"n_estimators\": {\"min\": 50, \"max\": 300}, \"max_depth\": {\"min\": 5, \"max\": 30}, \"min_samples_split\": {\"min\": 2, \"max\": 20}, \"min_samples_leaf\": {\"min\": 1, \"max\": 10} } } # Initialize sweep sweep_id = wandb.sweep(sweep=sweep_config, project=\"housing-mlops-gcp\") # Training function that W\u0026B calls 50 times def train(): run = wandb.init() # W\u0026B automatically assigns hyperparameters # Get hyperparameters suggested by Bayesian optimizer config = wandb.config # Train model model = RandomForestRegressor( n_estimators=config.n_estimators, max_depth=config.max_depth, # ... ) model.fit(X_train, y_train) # Evaluate metrics = evaluate_model(model, X_test, y_test) # Log to W\u0026B (asynchronous, non-blocking) wandb.log({ \"hyperparameters/n_estimators\": config.n_estimators, \"hyperparameters/max_depth\": config.max_depth, \"metrics/mape\": metrics['mape'], \"metrics/wmape\": metrics['wmape'], # Optimizer uses this \"metrics/r2\": metrics['r2'], \"plots/feature_importances\": wandb.Image(fig), }) run.finish() # Run 50 runs with Bayesian optimization wandb.agent(sweep_id, function=train, count=50) What W\u0026B does here that MLflow can’t:\nBayesian Optimization: W\u0026B suggests the next hyperparameters based on previous runs. It’s not random—it uses Gaussian Processes to efficiently explore the space.\nRun 1: n_estimators=100, max_depth=15 → wMAPE=8.5% Run 2: n_estimators=200, max_depth=20 → wMAPE=7.9% # Better Run 3: n_estimators=250, max_depth=22 → wMAPE=7.8% # W\u0026B suggests values close to Run 2 Early Termination (Hyperband): If a run is performing badly in the first 3 iterations (epochs), W\u0026B kills it automatically and tries other hyperparameters. Saves ~40% of compute.\n\"early_terminate\": { \"type\": \"hyperband\", \"min_iter\": 3 # Minimum 3 iterations before terminating } Parallel Coordinates Plot: Interactive visualization showing which hyperparameter combination produces the best wMAPE.\nInterpretation: Blue lines (runs with low wMAPE) converge at n_estimators=200-250 and max_depth=20-25. This visually tells you where the optimum is.\nAsynchronous Logging: wandb.log() doesn’t block. While the model trains, W\u0026B uploads metrics in the background. Total overhead: \u003c1% of training time.\nMLflow doesn’t have:\nBayesian optimization (only Grid/Random search via scikit-learn) Intelligent early termination Parallel coordinates plots Asynchronous logging (mlflow.log is synchronous) 2. Real-Time Monitoring: See Runs While They Run # In W\u0026B dashboard (web UI): # - See 50 simultaneous runs in interactive table # - Filter by \"wmape \u003c 8.0%\" → shows only 12 runs # - Compare top 5 runs side-by-side # - See convergence plots (MAPE vs iteration) Real use case: You start a 50-run sweep at 9 AM. At 10 AM, from your laptop at the coffee shop:\nOpen W\u0026B dashboard See that 30 runs have finished Filter by wmape \u003c 8.0% → 8 runs qualify Compare those 8 runs → identify that max_depth=20 appears in all Decision: Cancel the sweep, adjust max_depth range to [18, 25], restart Value: Immediate feedback without SSH to the server, without reading terminal logs. Experimentation is interactive, not batch.\n3. Lightweight Artifact Tracking (References to GCS) # src/model/05_model_selection/main.py # Upload model to GCS model_gcs_uri = upload_model_to_gcs(model, \"models/05-selection/randomforest_best.pkl\") # gs://bucket/models/05-selection/randomforest_best.pkl # Log reference in W\u0026B (does NOT upload the pickle, only the URI) artifact = wandb.Artifact( name=\"best_model_selection\", type=\"model\", description=\"Best model selected: RandomForest\" ) artifact.add_reference(model_gcs_uri, name=\"best_model.pkl\") # Only the URI run.log_artifact(artifact) W\u0026B doesn’t store the model—it only saves the URI gs://.... The model lives in GCS.\nAdvantage: You don’t pay for double storage (GCS + W\u0026B). W\u0026B is the index, GCS is the warehouse.\nHow This Project Uses MLflow 1. Model Registry (Step 07): Versioning and Stages # src/model/07_registration/main.py with mlflow.start_run(run_name=\"model_registration\"): # Log model mlflow.sklearn.log_model(model, \"model\") # Log params and metrics mlflow.log_params({ \"n_estimators\": 200, \"max_depth\": 20, \"min_samples_split\": 2 }) mlflow.log_metrics({ \"mape\": 7.82, \"r2\": 0.8654 }) # Register in Model Registry client = MlflowClient() # Create registered model (if it doesn't exist) client.create_registered_model( name=\"housing_price_model\", description=\"Housing price prediction - Random Forest\" ) # Create new version model_version = client.create_model_version( name=\"housing_price_model\", source=f\"runs:/{run_id}/model\", run_id=run_id ) # Result: housing_price_model/v3 # Transition to stage client.transition_model_version_stage( name=\"housing_price_model\", version=model_version.version, stage=\"Staging\" # Staging → Production when validated ) What MLflow does here that W\u0026B can’t:\nSemantic Versioning: Each model is housing_price_model/v1, v2, v3. They’re not random IDs—they’re incremental versions.\nStages: A model goes through None → Staging → Production → Archived. This lifecycle is explicit.\nv1: Production (current in API) v2: Staging (being validated) v3: None (just trained) v4: Archived (deprecated) Model-as-Code API: Loading a model in the API is trivial:\n# api/app/core/model_loader.py model = mlflow.pyfunc.load_model(\"models:/housing_price_model/Production\") You don’t need to know:\nWhere the pickle is physically located What version it is (MLflow resolves “Production” → v1) How to deserialize it (mlflow.pyfunc abstracts this) Rollback in 10 Seconds:\n# Problem: v3 in Production has a bug # Rollback to v2: mlflow models transition \\ --name housing_price_model \\ --version 2 \\ --stage Production # API detects the change and automatically reloads v2 Rich Metadata with Tags and Description:\n# Add searchable tags client.set_model_version_tag( \"housing_price_model\", version, \"training_date\", \"2026-01-13\" ) client.set_model_version_tag( \"housing_price_model\", version, \"sweep_id\", \"abc123xyz\" # Link to W\u0026B sweep ) # Description in Markdown client.update_model_version( name=\"housing_price_model\", version=version, description=\"\"\" # Housing Price Model v3 **Trained:** 2026-01-13 **Algorithm:** Random Forest **Metrics:** MAPE=7.8%, R²=0.865 **Sweep:** [W\u0026B Link](https://wandb.ai/project/sweeps/abc123) \"\"\" ) Result: 6 months later, when a stakeholder asks “what model is in Production?”, you open MLflow UI and all the info is there—not in a lost Slack thread.\nW\u0026B doesn’t have:\nModel Registry (only basic artifact tracking) Stages (Staging/Production) Load API (models:/name/stage) Transition history (who changed v2 to Production, when, why) 2. Pipeline Orchestration (main.py) # main.py @hydra.main(config_path=\".\", config_name=\"config\") def go(config: DictConfig) -\u003e None: # MLflow orchestrates steps as sub-runs # Step 01: Download mlflow.run( uri=\"src/data/01_download_data\", entry_point=\"main\", parameters={ \"file_url\": config.download_data.file_url, \"gcs_output_path\": config.download_data.gcs_output_path, # ... } ) # Step 02: Preprocessing mlflow.run( uri=\"src/data/02_preprocessing_and_imputation\", entry_point=\"main\", parameters={ \"gcs_input_path\": config.preprocessing.gcs_input_path, # ... } ) # ... Steps 03-07 MLflow creates a hierarchical run:\nParent Run: end_to_end_pipeline ├── Child Run: 01_download_data │ ├── params: file_url, gcs_output_path │ └── artifacts: housing.parquet ├── Child Run: 02_preprocessing_and_imputation │ ├── params: imputation_strategy │ └── artifacts: imputer.pkl, housing_processed.parquet ├── Child Run: 03_feature_engineering │ └── ... └── Child Run: 07_registration └── artifacts: model.pkl, model_config.yaml Value: In MLflow UI, you see the entire pipeline execution as a tree. Each step is auditable—what params it used, how long it took, what artifacts it produced.\nW\u0026B doesn’t have pipeline orchestration—only tracking of individual runs.\nThe Division of Labor in This Project Responsibility W\u0026B MLflow Reason Bayesian hyperparameter optimization ✓ ✗ W\u0026B has intelligent sweep, MLflow only Grid/Random Real-time dashboards ✓ ✗ W\u0026B UI is interactive, MLflow UI is static Parallel coordinates plots ✓ ✗ W\u0026B has advanced visualizations Early termination (Hyperband) ✓ ✗ W\u0026B implements Hyperband/ASHA/Median stopping Model Registry with stages ✗ ✓ MLflow has Staging/Production, W\u0026B doesn’t Model-as-code API ✗ ✓ mlflow.pyfunc.load_model() is the standard Model rollback ✗ ✓ MLflow transition, W\u0026B has no stage concept Pipeline orchestration ✗ ✓ mlflow.run() executes nested steps Artifact storage (physical) ✗ ✗ Both point to GCS, don’t duplicate storage Asynchronous logging ✓ ✗ W\u0026B doesn’t block training, MLflow does Searchable metadata ✓ ✓ Both allow tags/search, different implementations The Complete Flow: W\u0026B → MLflow Day 1-3: Experimentation (W\u0026B)\n# Run 50-run sweep make run-sweep # W\u0026B dashboard shows: # - 50 runs in table # - Parallel coordinates plot # - Best run: n_estimators=200, max_depth=20, wMAPE=7.8% # - Sweep ID: abc123xyz Output: src/model/06_sweep/best_params.yaml\nhyperparameters: n_estimators: 200 max_depth: 20 min_samples_split: 2 min_samples_leaf: 1 metrics: mape: 7.82 wmape: 7.76 r2: 0.8654 sweep_id: abc123xyz # Link to W\u0026B best_run_id: def456ghi Day 4: Registration (MLflow)\n# Step 07 reads best_params.yaml python main.py main.execute_steps=[07_registration] # MLflow: # 1. Trains model with best_params # 2. Registers as housing_price_model/v3 # 3. Transitions to Staging # 4. Saves metadata (including sweep_id) Day 5-7: Staging Validation\n# API runs with model in Staging docker run -p 8080:8080 \\ -e MLFLOW_MODEL_NAME=housing_price_model \\ -e MLFLOW_MODEL_STAGE=Staging \\ housing-api:latest # Run tests, validate metrics, review predictions Day 8: Promotion to Production\nmlflow models transition \\ --name housing_price_model \\ --version 3 \\ --stage Production # Production API auto-reloads v3 # v2 becomes fallback (stage: Archived) If something fails:\n# Rollback in 10 seconds mlflow models transition \\ --name housing_price_model \\ --version 2 \\ --stage Production Why Both, Definitely Question: “Can I use only W\u0026B?”\nAnswer: You can, but you lose:\nModel Registry (versioning, stages, rollback) Standard API for loading models in production Pipeline orchestration with hierarchical runs Result: You end up building your own model versioning system with custom scripts—reinventing the wheel badly.\nQuestion: “Can I use only MLflow?”\nAnswer: You can, but you lose:\nBayesian optimization (you’ll have to do slow Grid Search) Interactive visualizations (parallel coordinates, real-time dashboards) Intelligent early termination (you waste compute) Result: Your sweeps take 3x longer, and you have no visual feedback on what works.\nThe Real Cost W\u0026B:\nFree tier: 100GB storage, unlimited collaborators Team tier: $50/user/month (for teams \u003e5 people) MLflow:\nOpen source, free Cost: Hosting the tracking server (Cloud Run: ~$20/month for moderate use) Storage: GCS (you already pay for data) Total for team of 5: ~$20-50/month (if using W\u0026B free tier) or ~$270/month (if using W\u0026B Team).\nROI: If a more efficient sweep saves 30 minutes of compute/day:\nCompute saved: ~15 hours/month In GCP: 15 hours × $2/hour (GPU) = $30/month saved in compute alone Plus engineer time (more valuable) Breakeven in \u003c1 month.\nThe Lesson For MLOps Engineers Don’t choose tools by hype or popularity. Choose by clear responsibilities:\nFast, interactive experimentation: W\u0026B, Neptune, Comet Governance and deployment: MLflow, Seldon, BentoML Artifact storage: GCS, S3, Azure Blob (not tracking tools) This project uses:\nW\u0026B: Because it needs efficient Bayesian sweep MLflow: Because it needs production-ready Model Registry GCS: Because it needs high-availability storage There’s no redundancy—there’s specialization.\nWhen you understand this, you stop asking “W\u0026B or MLflow?” and start asking “what problem am I solving?”\nThat’s the difference between using tools and building systems.\n10. Docker and MLflow: Containerizing the Complete Ecosystem The Three-Container Architecture This project uses three distinct Dockerfiles, each optimized for its specific purpose:\nPipeline Container (Dockerfile): Runs the complete training pipeline with MLflow tracking API Container (api/Dockerfile): Serves predictions with FastAPI in production Streamlit Container (streamlit_app/Dockerfile): Provides interactive web interface This separation is not accidental—it’s an architectural decision that reflects the different requirements of each component. 1. Pipeline Container: Training with MLflow Tracking Pipeline Dockerfile # ================================================================= # Dockerfile for MLOps Pipeline Execution # Purpose: Run the complete training pipeline in containerized environment # ================================================================= FROM python:3.12-slim LABEL maintainer=\"danieljimenez88m@gmail.com\" LABEL description=\"Housing Price Prediction - MLOps Pipeline\" # Set working directory WORKDIR /app # Install system dependencies RUN apt-get update \u0026\u0026 apt-get install -y \\ gcc \\ g++ \\ git \\ curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* # Copy requirements first for better caching COPY pyproject.toml ./ COPY requirements.txt* ./ # Install Python dependencies RUN pip install --no-cache-dir --upgrade pip \u0026\u0026 \\ pip install --no-cache-dir uv \u0026\u0026 \\ uv pip install --system -e . # Copy application code COPY . . # Set environment variables ENV PYTHONUNBUFFERED=1 ENV PYTHONDONTWRITEBYTECODE=1 # Create necessary directories RUN mkdir -p mlruns outputs models # Default command runs the pipeline CMD [\"python\", \"main.py\"] Critical Technical Decisions 1. Why gcc and g++\nRUN apt-get install -y gcc g++ git curl Many ML packages (numpy, scipy, scikit-learn) compile C/C++ extensions during installation. Without these compilers, pip install fails with cryptic errors like “error: command ‘gcc’ failed”.\nTrade-off: Larger image (~500MB vs ~150MB of pure Python slim), but guarantees all dependencies install correctly.\n2. Layer Caching Strategy\n# Copy requirements first for better caching COPY pyproject.toml ./ COPY requirements.txt* ./ RUN pip install ... # Copy application code AFTER COPY . . Docker caches layers. If you change Python code but not dependencies, Docker reuses the pip install layer (takes 5 minutes) and only recopies the code (10 seconds).\nWithout this optimization: Every code change requires reinstalling all dependencies.\n3. Directory Creation for MLflow\nRUN mkdir -p mlruns outputs models MLflow writes artifacts to mlruns/ by default if a remote tracking server isn’t configured. If this directory doesn’t exist with correct permissions, MLflow fails silently.\noutputs/: For plots and intermediate analysis models/: For model checkpoints before uploading to GCS\nHow to Enable MLflow Tracking Option 1: Local MLflow (Default)\nWhen you run the pipeline in this container, MLflow writes to mlruns/ inside the container:\ndocker run --env-file .env housing-pipeline:latest # MLflow writes to /app/mlruns/ # To see the UI: docker exec -it mlflow ui --host 0.0.0.0 --port 5000 Limitation: Runs are lost when the container stops.\nOption 2: MLflow Remote Tracking Server\nTo persist runs, configure a separate MLflow server:\n# docker-compose.yaml services: mlflow: image: ghcr.io/mlflow/mlflow:v2.9.2 container_name: mlflow-server ports: - \"5000:5000\" environment: - BACKEND_STORE_URI=sqlite:///mlflow.db - DEFAULT_ARTIFACT_ROOT=gs://your-bucket/mlflow-artifacts volumes: - mlflow-data:/mlflow command: \u003e mlflow server --backend-store-uri sqlite:///mlflow/mlflow.db --default-artifact-root gs://your-bucket/mlflow-artifacts --host 0.0.0.0 --port 5000 pipeline: build: . environment: - MLFLOW_TRACKING_URI=http://mlflow:5000 - GCP_PROJECT_ID=${GCP_PROJECT_ID} - GCS_BUCKET_NAME=${GCS_BUCKET_NAME} - WANDB_API_KEY=${WANDB_API_KEY} depends_on: - mlflow volumes: mlflow-data: Configuration in code:\n# main.py import os import mlflow # If MLFLOW_TRACKING_URI is configured, use that server mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\") mlflow.set_tracking_uri(mlflow_uri) mlflow.set_experiment(\"housing_price_prediction\") with mlflow.start_run(): # Log params, metrics, artifacts mlflow.log_param(\"n_estimators\", 200) mlflow.log_metric(\"mape\", 7.82) mlflow.sklearn.log_model(model, \"model\") Option 3: MLflow in Cloud (Production)\nFor production, use a managed MLflow server:\n# Deploy MLflow to Cloud Run (serverless) gcloud run deploy mlflow-server \\ --image ghcr.io/mlflow/mlflow:v2.9.2 \\ --platform managed \\ --region us-central1 \\ --set-env-vars=\"BACKEND_STORE_URI=postgresql://user:pass@host/mlflow_db,DEFAULT_ARTIFACT_ROOT=gs://bucket/mlflow\" \\ --allow-unauthenticated # Get URL MLFLOW_URL=$(gcloud run services describe mlflow-server --format 'value(status.url)') # Configure in pipeline export MLFLOW_TRACKING_URI=$MLFLOW_URL Pipeline Container Execution # Build docker build -t housing-pipeline:latest . # Run with env vars docker run \\ --env-file .env \\ -v $(pwd)/mlruns:/app/mlruns \\ housing-pipeline:latest # Run with specific steps docker run \\ --env-file .env \\ housing-pipeline:latest \\ python main.py main.execute_steps=[03_feature_engineering,05_model_selection] # See logs in real-time docker logs -f Volume Mount (-v): Mounts mlruns/ from the host to the container to persist MLflow runs even after the container stops.\n2. API Container: Production Inference API Dockerfile # ================================================================= # Dockerfile for Housing Price Prediction API # Purpose: Production-ready FastAPI service for Cloud Run deployment # ================================================================= FROM python:3.12-slim LABEL maintainer=\"danieljimenez88m@gmail.com\" LABEL description=\"Housing Price Prediction API - FastAPI Service\" WORKDIR /app # Install system dependencies (only curl for healthcheck) RUN apt-get update \u0026\u0026 apt-get install -y \\ curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* # Copy requirements first for better caching COPY requirements.txt . # Install Python dependencies RUN pip install --no-cache-dir --upgrade pip \u0026\u0026 \\ pip install --no-cache-dir -r requirements.txt # Copy application code COPY app/ ./app/ # Create models directory RUN mkdir -p models # Set environment variables ENV PYTHONUNBUFFERED=1 ENV PYTHONDONTWRITEBYTECODE=1 ENV PORT=8080 # Expose port EXPOSE 8080 # Health check HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 # Run the application CMD exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT} Critical Technical Decisions 1. Lighter Image\nCompared to the pipeline container:\nDoesn’t need gcc/g++: Dependencies are already compiled in wheels Doesn’t need git: Doesn’t clone repos Only curl: For the healthcheck Result: Image of ~200MB vs ~500MB for the pipeline.\nWhy it matters: Cloud Run charges for memory usage. A smaller image = less memory = less cost.\n2. Native Health Check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 Docker marks the container as “unhealthy” if the /health endpoint fails 3 consecutive times.\nCloud Run and Kubernetes use this to:\nNot send traffic to unhealthy containers Restart failing containers Uptime reporting start-period=40s: Gives the API 40 seconds to load the model before starting health checks.\n3. Flexible Port Configuration\nENV PORT=8080 CMD exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT} Cloud Run injects PORT as an env var (can be 8080, 8081, etc.). The API must read this value, not hardcode it.\nexec: Replaces the shell process with uvicorn, allowing Docker to send signals (SIGTERM) directly to uvicorn for graceful shutdown.\nHow the API Loads the Model The API has three model loading strategies with automatic fallback:\n# api/app/core/model_loader.py class ModelLoader: \"\"\"Load model from MLflow → GCS → Local with fallback.\"\"\" def load_model(self) -\u003e Any: \"\"\"Priority: MLflow \u003e GCS \u003e Local\"\"\" # Strategy 1: From MLflow Registry if self.mlflow_model_name: try: model_uri = f\"models:/{self.mlflow_model_name}/{self.mlflow_stage}\" self._model = mlflow.pyfunc.load_model(model_uri) logger.info(f\"Loaded from MLflow: {model_uri}\") return self._model except Exception as e: logger.warning(f\"MLflow load failed: {e}, trying GCS...\") # Strategy 2: From GCS if self.gcs_model_path: try: storage_client = storage.Client() bucket = storage_client.bucket(self.gcs_bucket) blob = bucket.blob(self.gcs_model_path) model_bytes = blob.download_as_bytes() self._model = pickle.loads(model_bytes) logger.info(f\"Loaded from GCS: gs://{self.gcs_bucket}/{self.gcs_model_path}\") return self._model except Exception as e: logger.warning(f\"GCS load failed: {e}, trying local...\") # Strategy 3: From local file (fallback) if self.local_model_path and Path(self.local_model_path).exists(): with open(self.local_model_path, 'rb') as f: self._model = pickle.load(f) logger.info(f\"Loaded from local: {self.local_model_path}\") return self._model raise RuntimeError(\"No model could be loaded from any source\") Configuration with env vars:\n# Production: Load from MLflow docker run -p 8080:8080 \\ -e MLFLOW_TRACKING_URI=https://mlflow.example.com \\ -e MLFLOW_MODEL_NAME=housing_price_model \\ -e MLFLOW_MODEL_STAGE=Production \\ housing-api:latest # Staging: Load from GCS docker run -p 8080:8080 \\ -e GCS_BUCKET=my-bucket \\ -e GCS_MODEL_PATH=models/trained/housing_price_model.pkl \\ housing-api:latest # Development: Load from local docker run -p 8080:8080 \\ -v $(pwd)/models:/app/models \\ -e LOCAL_MODEL_PATH=/app/models/trained/housing_price_model.pkl \\ housing-api:latest 3. Streamlit Container: Interactive Frontend Streamlit Dockerfile # ================================================================= # Dockerfile for Streamlit Frontend # Purpose: Interactive web interface for housing price predictions # ================================================================= FROM python:3.12-slim LABEL maintainer=\"danieljimenez88m@gmail.com\" LABEL description=\"Housing Price Prediction - Streamlit Frontend\" WORKDIR /app RUN apt-get update \u0026\u0026 apt-get install -y curl \u0026\u0026 rm -rf /var/lib/apt/lists/* COPY requirements.txt . RUN pip install --no-cache-dir --upgrade pip \u0026\u0026 \\ pip install --no-cache-dir -r requirements.txt COPY app.py . # Create .streamlit directory for config RUN mkdir -p .streamlit # Streamlit configuration RUN echo '\\ [server]\\n\\ port = 8501\\n\\ address = \"0.0.0.0\"\\n\\ headless = true\\n\\ enableCORS = false\\n\\ enableXsrfProtection = true\\n\\ \\n\\ [browser]\\n\\ gatherUsageStats = false\\n\\ \\n\\ [theme]\\n\\ primaryColor = \"#FF4B4B\"\\n\\ backgroundColor = \"#FFFFFF\"\\n\\ secondaryBackgroundColor = \"#F0F2F6\"\\n\\ textColor = \"#262730\"\\n\\ font = \"sans serif\"\\n\\ ' \u003e .streamlit/config.toml ENV PYTHONUNBUFFERED=1 ENV PYTHONDONTWRITEBYTECODE=1 EXPOSE 8501 HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\ CMD curl -f http://localhost:8501/_stcore/health || exit 1 CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"] Critical Technical Decisions 1. Embedded Configuration\nRUN echo '...' \u003e .streamlit/config.toml Streamlit requires configuration to run in containers (headless mode, CORS, etc.). Instead of committing a config.toml file to the repo, we generate it at build time.\nAdvantages:\nOne less file in the repo Configuration versioned with the Dockerfile No risk of forgetting to commit the config 2. Streamlit Health Check\nHEALTHCHECK CMD curl -f http://localhost:8501/_stcore/health || exit 1 Streamlit automatically exposes /_stcore/health. This endpoint returns 200 if the app is running.\n3. Custom Theme\n[theme] primaryColor = \"#FF4B4B\" backgroundColor = \"#FFFFFF\" secondaryBackgroundColor = \"#F0F2F6\" textColor = \"#262730\" The theme defines button colors, backgrounds, etc. This gives visual consistency without needing custom CSS in each component.\nHow Streamlit Connects to the API # streamlit_app/app.py import os import requests import streamlit as st # Read API URL from environment variable API_URL = os.getenv(\"API_URL\", \"http://localhost:8080\") API_PREDICT_ENDPOINT = f\"{API_URL}/api/v1/predict\" def make_prediction(features: Dict[str, Any]) -\u003e Dict[str, Any]: \"\"\"Call API to get prediction.\"\"\" payload = {\"instances\": [features]} try: response = requests.post( API_PREDICT_ENDPOINT, json=payload, timeout=10 ) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: st.error(f\"API Error: {e}\") return None # Streamlit UI st.title(\"Housing Price Prediction\") with st.form(\"prediction_form\"): longitude = st.number_input(\"Longitude\", value=-122.23) latitude = st.number_input(\"Latitude\", value=37.88) # ... more inputs submitted = st.form_submit_button(\"Predict\") if submitted: features = { \"longitude\": longitude, \"latitude\": latitude, # ... } result = make_prediction(features) if result: prediction = result[\"predictions\"][0][\"median_house_value\"] st.success(f\"Predicted Price: ${prediction:,.2f}\") API URL configuration:\n# Docker Compose: Use service name docker-compose up # Streamlit automatically receives API_URL=http://api:8080 # Local development: Use localhost API_URL=http://localhost:8080 streamlit run app.py # Production: Use Cloud Run URL API_URL=https://housing-api-xyz.run.app streamlit run app.py Docker Compose: Orchestrating the Three Containers # docker-compose.yaml services: api: build: context: ./api dockerfile: Dockerfile container_name: housing-price-api ports: - \"8080:8080\" environment: - PORT=8080 - LOCAL_MODEL_PATH=/app/models/trained/housing_price_model.pkl - WANDB_API_KEY=${WANDB_API_KEY} volumes: - ./models:/app/models:ro restart: unless-stopped healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"] interval: 30s timeout: 10s retries: 3 networks: - mlops-network streamlit: build: context: ./streamlit_app dockerfile: Dockerfile container_name: housing-streamlit ports: - \"8501:8501\" environment: - API_URL=http://api:8080 depends_on: - api restart: unless-stopped healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8501/_stcore/health\"] interval: 30s timeout: 10s retries: 3 networks: - mlops-network networks: mlops-network: driver: bridge name: housing-mlops-network Critical Decisions:\n1. Network Isolation\nnetworks: - mlops-network Both containers are in the same Docker network, allowing Streamlit to call the API using http://api:8080 (service name as hostname).\nWithout this: You’d have to use http://host.docker.internal:8080 (only works on Docker Desktop) or the host IP.\n2. Read-Only Volume Mount\nvolumes: - ./models:/app/models:ro The API mounts models/ in read-only mode (:ro). The container can read the model but not modify it.\nWhy: Security. If the container is compromised, an attacker can’t overwrite the model with a malicious one.\n3. Dependency Order\ndepends_on: - api Docker Compose starts the API before Streamlit. This prevents Streamlit from failing when trying to connect to an API that’s not yet running.\nLimitation: depends_on only waits for the container to start, not for the API to be ready (healthcheck pass). For that, you need an init container or retry logic in Streamlit.\nComplete Execution Commands # 1. Build all images docker-compose build # 2. Train the model (pipeline container) docker run --env-file .env -v $(pwd)/models:/app/models housing-pipeline:latest # 3. Start API + Streamlit docker-compose up -d # 4. Verify health curl http://localhost:8080/health curl http://localhost:8501/_stcore/health # 5. See logs docker-compose logs -f # 6. Stop everything docker-compose down What This Architecture Solves Without containers:\n“Works on my machine” syndrome Conflicting dependencies (Python 3.9 vs 3.12) Manual setup in every environment (dev, staging, prod) With this architecture:\nReproducibility: Same container runs on laptop, CI/CD, and production Isolation: API doesn’t interfere with Streamlit, pipeline doesn’t interfere with API Deployment: docker push → gcloud run deploy in \u003c5 minutes Rollback: docker pull previous-image → restart Observability: Automatic health checks, centralized logs The real value: A data scientist without DevOps experience can deploy to production without knowing how to configure nginx, systemd, or virtual environments. Docker abstracts all that complexity.\n10.5. API Architecture: FastAPI in Production Why This Section Matters You’ve seen training pipelines, hyperparameter sweeps, and model registry. But 90% of the time, your model isn’t training—it’s serving predictions in production.\nA poorly designed API is the bottleneck between an excellent model and a useful product. This section breaks down how this project builds a production-ready API, not a tutorial prototype.\nThe General Architecture api/ ├── app/ │ ├── main.py # FastAPI app + lifespan management │ ├── core/ │ │ ├── config.py # Pydantic Settings (env vars) │ │ ├── model_loader.py # Multi-source model loading │ │ ├── wandb_logger.py # Prediction logging │ │ └── preprocessor.py # Feature engineering │ ├── routers/ │ │ └── predict.py # Prediction endpoints │ └── models/ │ └── schemas.py # Pydantic request/response models ├── requirements.txt ├── Dockerfile └── tests/ Architectural decision: Separation of concerns by layers:\nCore: Business logic (load model, logging, config) Routers: HTTP endpoints (routes, request validation) Models: Data schemas (Pydantic) Why not everything in main.py? Because when the API grows (adding authentication, rate limiting, multiple models), each layer extends independently without touching the rest.\n1. Lifespan Management: The Pattern That Avoids First-Request Latency The Problem It Solves Common anti-pattern:\n# BAD: Load model on every request @app.post(\"/predict\") def predict(features): model = pickle.load(open(\"model.pkl\", \"rb\")) # 5 seconds every request return model.predict(features) Problems:\nFirst request takes 5 seconds (load model) Every subsequent request also (no caching) If 10 concurrent requests → 10 model loads (50 seconds total) The Solution: asynccontextmanager # api/app/main.py @asynccontextmanager async def lifespan(app: FastAPI): \"\"\" Lifecycle manager for the FastAPI application. Loads the model on startup and cleans up on shutdown. \"\"\" logger.info(\"Starting up API...\") # STARTUP: Load model ONCE wandb_logger = WandBLogger( project=settings.WANDB_PROJECT, enabled=True ) model_loader = ModelLoader( local_model_path=settings.LOCAL_MODEL_PATH, gcs_bucket=settings.GCS_BUCKET, gcs_model_path=settings.GCS_MODEL_PATH, mlflow_model_name=settings.MLFLOW_MODEL_NAME, mlflow_model_stage=settings.MLFLOW_MODEL_STAGE, mlflow_tracking_uri=settings.MLFLOW_TRACKING_URI ) try: logger.info(\"Loading model...\") model_loader.load_model() # Takes 5 seconds, but ONLY once logger.info(f\"Model loaded: {model_loader.model_version}\") # Save in app state (available to all endpoints) app.state.model_loader = model_loader app.state.wandb_logger = wandb_logger except Exception as e: logger.error(f\"Failed to load model: {str(e)}\") logger.warning(\"API will start but predictions will fail\") yield # API runs here # SHUTDOWN: Cleanup logger.info(\"Shutting down API...\") wandb_logger.close() # Use lifespan in FastAPI app = FastAPI( title=\"Housing Price Prediction API\", version=\"1.0.0\", lifespan=lifespan # CRITICAL ) What it does:\nStartup (before yield):\nLoad model into memory (5 seconds, only once) Initialize W\u0026B logger Save both in app.state (singleton pattern) Running (after yield):\nAll requests use the cached model in app.state.model_loader Latency per request: \u003c50ms (only inference, no I/O) Shutdown (after context manager):\nClose W\u0026B run (flush pending logs) Free resources Result:\nFirst request: \u003c50ms (model already loaded) Subsequent requests: \u003c50ms 10 concurrent requests: \u003c100ms average (parallelizable) Trade-off: 5-10 second startup time. Acceptable for production—better than 5 seconds per request.\n2. Configuration Management: Pydantic Settings with Priorities The Pattern: Settings-as-Code # api/app/core/config.py from pydantic_settings import BaseSettings class Settings(BaseSettings): PROJECT_NAME: str = \"Housing Price Prediction API\" VERSION: str = \"1.0.0\" API_V1_STR: str = \"/api/v1\" # Model - MLflow (priority 1) MLFLOW_MODEL_NAME: str = \"\" MLFLOW_MODEL_STAGE: str = \"Production\" MLFLOW_TRACKING_URI: str = \"\" # Model - GCS (priority 2) GCS_BUCKET: str = \"\" GCS_MODEL_PATH: str = \"models/trained/housing_price_model.pkl\" # Model - Local (priority 3, fallback) LOCAL_MODEL_PATH: str = \"models/trained/housing_price_model.pkl\" # Weights \u0026 Biases WANDB_API_KEY: str = \"\" WANDB_PROJECT: str = \"housing-mlops-api\" class Config: env_file = \".env\" # Read from .env automatically case_sensitive = True # MLFLOW_MODEL_NAME != mlflow_model_name Why Pydantic Settings:\nType Safety: settings.VERSION is str, not Optional[Any] Validation: If MLFLOW_MODEL_STAGE is not a string, fails at startup (not on first request) Auto .env loading: You don’t need python-dotenv manually Default values: LOCAL_MODEL_PATH has a default, MLFLOW_MODEL_NAME doesn’t Usage in code:\nfrom app.core.config import Settings settings = Settings() # Read env vars + .env if settings.MLFLOW_MODEL_NAME: # Type-safe check model = load_from_mlflow(settings.MLFLOW_MODEL_NAME) The Priority Strategy (Cascade Fallback) Try to load from: 1. MLflow Registry (if MLFLOW_MODEL_NAME is configured) ↓ If fails 2. GCS (if GCS_BUCKET is configured) ↓ If fails 3. Local filesystem (always available as last resort) ↓ If fails 4. API starts but `/predict` returns 500 Configuration by environment:\n# Production (.env.production) MLFLOW_MODEL_NAME=housing_price_model MLFLOW_MODEL_STAGE=Production MLFLOW_TRACKING_URI=https://mlflow.company.com # GCS and Local stay empty → not used # Staging (.env.staging) MLFLOW_MODEL_NAME=housing_price_model MLFLOW_MODEL_STAGE=Staging # Same setup, different stage # Local development (.env.local) LOCAL_MODEL_PATH=models/trained/housing_price_model.pkl # Without MLflow or GCS → load from local directly Value: One codebase, multiple environments. No if ENVIRONMENT == \"production\" in the code.\n3. Model Loader: Multi-Source with Intelligent Fallback The Loader Architecture # api/app/core/model_loader.py class ModelLoader: \"\"\"Handles loading ML models from various sources.\"\"\" def __init__( self, local_model_path: Optional[str] = None, gcs_bucket: Optional[str] = None, gcs_model_path: Optional[str] = None, mlflow_model_name: Optional[str] = None, mlflow_model_stage: Optional[str] = None, mlflow_tracking_uri: Optional[str] = None ): self.local_model_path = local_model_path self.gcs_bucket = gcs_bucket self.gcs_model_path = gcs_model_path self.mlflow_model_name = mlflow_model_name self.mlflow_model_stage = mlflow_model_stage self.mlflow_tracking_uri = mlflow_tracking_uri self._model: Optional[Any] = None # Cached in memory self._model_version: str = \"unknown\" self._preprocessor = HousingPreprocessor() def load_model(self) -\u003e Any: \"\"\"Load model with cascade fallback strategy.\"\"\" # Priority 1: MLflow Registry if self.mlflow_model_name: try: logger.info(f\"Attempting MLflow load: {self.mlflow_model_name}/{self.mlflow_model_stage}\") self._model = self.load_from_mlflow( self.mlflow_model_name, self.mlflow_model_stage, self.mlflow_tracking_uri ) return self._model except Exception as e: logger.warning(f\"MLflow load failed: {str(e)}, trying GCS...\") # Priority 2: GCS if self.gcs_bucket and self.gcs_model_path: try: logger.info(f\"Attempting GCS load: gs://{self.gcs_bucket}/{self.gcs_model_path}\") self._model = self.load_from_gcs(self.gcs_bucket, self.gcs_model_path) return self._model except Exception as e: logger.warning(f\"GCS load failed: {str(e)}, trying local...\") # Priority 3: Local filesystem if self.local_model_path and Path(self.local_model_path).exists(): logger.info(f\"Attempting local load: {self.local_model_path}\") self._model = self.load_from_local(self.local_model_path) return self._model # All strategies failed raise RuntimeError( \"Could not load model from any source. \" \"Check MLflow/GCS/local configuration.\" ) def predict(self, features: pd.DataFrame) -\u003e np.ndarray: \"\"\"Make predictions with preprocessing.\"\"\" if not self.is_loaded: raise RuntimeError(\"Model not loaded\") # Apply same preprocessing as the training pipeline processed_features = self._preprocessor.transform(features) # Predict predictions = self._model.predict(processed_features) return predictions @property def is_loaded(self) -\u003e bool: \"\"\"Check if model is loaded.\"\"\" return self._model is not None Critical Technical Decisions 1. Why MLflow Is Priority 1\n# MLflow load model = mlflow.sklearn.load_model(\"models:/housing_price_model/Production\") Advantages over GCS/Local:\nModel URI abstracts storage: The model can be in S3, GCS, HDFS—MLflow resolves it Stage resolution: Production automatically resolves to the correct version (v1, v2, etc.) Metadata included: MLflow also loads conda.yaml, requirements.txt, feature metadata Trivial rollback: Change stage in MLflow UI, API automatically reloads on next restart 2. GCS As Fallback (Not Primary)\n# GCS load from google.cloud import storage client = storage.Client() bucket = client.bucket(\"my-bucket\") blob = bucket.blob(\"models/trained/housing_price_model.pkl\") model_bytes = blob.download_as_bytes() model = pickle.loads(model_bytes) Why not primary:\nNo versioning: models/trained/housing_price_model.pkl is always the “latest”—you can’t load v1 vs v2 without changing the path No metadata: You only get the pickle, you don’t know what hyperparameters/features it expects No stages: No concept of Staging vs Production When to use GCS as primary:\nMLflow is unavailable (outage) Simple setup (only one model, don’t need registry) Budget constraint (avoid hosting MLflow) 3. Local As Last Resort\n# Local load with open(\"models/trained/housing_price_model.pkl\", \"rb\") as f: model = pickle.load(f) Only for:\nLocal development (don’t want to depend on GCS/MLflow) Debugging (broken model in GCS, test with local copy) CI/CD tests (GitHub Actions doesn’t have access to GCS) Never for real production—if GCS and MLflow are down, you have bigger problems than the model.\n4. Embedded Preprocessing Pipeline\nself._preprocessor = HousingPreprocessor() def predict(self, features: pd.DataFrame) -\u003e np.ndarray: processed_features = self._preprocessor.transform(features) predictions = self._model.predict(processed_features) return predictions Why critical: The model expects processed features (one-hot encoding of ocean_proximity, cluster feature engineering). If the client sends raw features, the model fails.\nImplementation options:\nA) Preprocessing in the API (this project):\n# Client sends raw features {\"ocean_proximity\": \"NEAR BAY\", \"longitude\": -122.23, ...} # API applies preprocessing processed = preprocessor.transform(raw_features) # Model receives processed features predictions = model.predict(processed) B) Preprocessing in the client (bad for public APIs):\n# Client must know exact preprocessing processed = client_side_preprocessing(raw_features) # What does this do? # API only does inference predictions = model.predict(processed) Trade-offs:\nApproach Advantage Disadvantage Preprocessing in API Client doesn’t need to know preprocessing More complex API, +5ms latency Preprocessing in client Simple API, low latency Client must replicate exact preprocessing For public APIs: Always preprocessing in the API. Clients shouldn’t know internal model details.\nFor internal APIs: Depends. If the client is another service you control, you can do preprocessing there to reduce latency.\n4. Request/Response Validation: Pydantic Schemas The Anti-Pattern: Manual Validation # BAD: Manual validation prone to errors @app.post(\"/predict\") def predict(request: dict): if \"longitude\" not in request: return {\"error\": \"missing longitude\"} if not isinstance(request[\"longitude\"], (int, float)): return {\"error\": \"longitude must be number\"} if request[\"longitude\"] \u003c -180 or request[\"longitude\"] \u003e 180: return {\"error\": \"longitude out of range\"} # ... 50 more lines of manual validation Problems:\nRepetitive and fragile code Inconsistent errors (\"missing longitude\" vs \"longitude is required\") No automatic documentation (OpenAPI) Hard to test The Solution: Pydantic Schemas # api/app/models/schemas.py from pydantic import BaseModel, Field, field_validator class HousingFeatures(BaseModel): \"\"\"Input features for housing price prediction.\"\"\" longitude: float = Field( ..., # Required description=\"Longitude coordinate\", ge=-180, # greater or equal le=180 # less or equal ) latitude: float = Field(..., description=\"Latitude coordinate\", ge=-90, le=90) housing_median_age: float = Field(..., description=\"Median age of houses\", ge=0) total_rooms: float = Field(..., description=\"Total number of rooms\", ge=0) total_bedrooms: float = Field(..., description=\"Total number of bedrooms\", ge=0) population: float = Field(..., description=\"Block population\", ge=0) households: float = Field(..., description=\"Number of households\", ge=0) median_income: float = Field(..., description=\"Median income\", ge=0) ocean_proximity: str = Field(..., description=\"Proximity to ocean\") @field_validator('ocean_proximity') @classmethod def validate_ocean_proximity(cls, v: str) -\u003e str: \"\"\"Validate ocean proximity values.\"\"\" valid_values = ['\u003c1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'] if v.upper() not in valid_values: raise ValueError( f\"ocean_proximity must be one of: {', '.join(valid_values)}\" ) return v.upper() # Normalize to uppercase model_config = { \"json_schema_extra\": { \"examples\": [{ \"longitude\": -122.23, \"latitude\": 37.88, \"housing_median_age\": 41.0, \"total_rooms\": 880.0, \"total_bedrooms\": 129.0, \"population\": 322.0, \"households\": 126.0, \"median_income\": 8.3252, \"ocean_proximity\": \"NEAR BAY\" }] } } What this automatically provides:\nType validation:\n{\"longitude\": \"not a number\"} // Rejected: ValidationError Range validation:\n{\"longitude\": -200} // Rejected: must be \u003e= -180 Custom validation:\n{\"ocean_proximity\": \"INVALID\"} // Rejected: must be one of [...] Automatic documentation at /docs:\nSwagger UI shows all fields Descriptions, constraints, examples Try-it-out works out-of-the-box Type-safe serialization:\nfeatures = HousingFeatures(**request_json) features.longitude # Type: float (not Optional[Any]) Batch Predictions Support class PredictionRequest(BaseModel): \"\"\"Request model for single or batch predictions.\"\"\" instances: List[HousingFeatures] = Field( ..., description=\"List of housing features for prediction\", min_length=1 # At least one instance ) Usage:\n{ \"instances\": [ {\"longitude\": -122.23, ...}, // Predict house 1 {\"longitude\": -118.45, ...}, // Predict house 2 {\"longitude\": -121.89, ...} // Predict house 3 ] } Why support batch:\nReduced latency: 3 individual requests = 150ms. 1 batch of 3 = 60ms. Reduced cost: Less HTTP overhead (headers, handshake, etc.) Efficient inference: The model can vectorize operations Trade-off: Very large batch size (\u003e1000) can cause timeouts. Implement limit:\ninstances: List[HousingFeatures] = Field( ..., min_length=1, max_length=100 # Maximum 100 predictions per request ) Response Schema class PredictionResult(BaseModel): \"\"\"Individual prediction result.\"\"\" predicted_price: float = Field(..., description=\"Predicted median house value\") confidence_interval: Optional[dict] = Field( None, description=\"Confidence interval (if available)\" ) class PredictionResponse(BaseModel): \"\"\"Response model for predictions.\"\"\" predictions: List[PredictionResult] = Field(..., description=\"List of predictions\") model_version: str = Field(..., description=\"Model version used\") model_config = { \"json_schema_extra\": { \"examples\": [{ \"predictions\": [{ \"predicted_price\": 452600.0, \"confidence_interval\": None }], \"model_version\": \"randomforest_v1\" }] } } model_version in response: Crucial for debugging. If a client reports incorrect predictions, the model_version tells you which model was used (v1, v2, Production, etc.).\n5. Router Pattern: Endpoints and Error Handling The Router Structure # api/app/routers/predict.py from fastapi import APIRouter, HTTPException, status router = APIRouter(prefix=\"/api/v1\", tags=[\"predictions\"]) # Global instances (set by main.py) model_loader: ModelLoader = None wandb_logger: WandBLogger = None def set_model_loader(loader: ModelLoader) -\u003e None: \"\"\"Dependency injection pattern.\"\"\" global model_loader model_loader = loader Why prefix=\"/api/v1\":\n/api/v1/predict ← Version 1 of the API /api/v2/predict ← Version 2 (breaking changes) You can run both versions simultaneously during migration:\nLegacy clients use /api/v1/ New clients use /api/v2/ Deprecate v1 after 6 months Without versioning: Breaking change → all clients break at the same time.\nThe Main Endpoint: POST /api/v1/predict @router.post( \"/predict\", response_model=PredictionResponse, status_code=status.HTTP_200_OK, responses={ 400: {\"model\": ErrorResponse, \"description\": \"Invalid input data\"}, 500: {\"model\": ErrorResponse, \"description\": \"Prediction failed\"}, }, summary=\"Predict housing prices\", description=\"Make predictions for housing prices based on input features\" ) async def predict(request: PredictionRequest) -\u003e PredictionResponse: \"\"\" Predict housing prices for given features. \"\"\" # 1. Check model loaded if model_loader is None or not model_loader.is_loaded: raise HTTPException( status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=\"Model not loaded\" ) start_time = time.time() try: # 2. Convert Pydantic models to DataFrame features_list = [instance.model_dump() for instance in request.instances] df = pd.DataFrame(features_list) # 3. Make predictions predictions = model_loader.predict(df) # 4. Calculate metrics response_time_ms = (time.time() - start_time) * 1000 # 5. Format response results = [ PredictionResult(predicted_price=float(pred)) for pred in predictions ] # 6. Log to W\u0026B (async, non-blocking) if wandb_logger: wandb_logger.log_prediction( features=features_list, predictions=[float(p) for p in predictions], model_version=model_loader.model_version, response_time_ms=response_time_ms ) return PredictionResponse( predictions=results, model_version=model_loader.model_version ) except ValueError as e: # Validation error (e.g., feature out of expected range) if wandb_logger: wandb_logger.log_error(\"validation_error\", str(e), features_list) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=f\"Invalid input data: {str(e)}\" ) except Exception as e: # Unexpected error (e.g., corrupted model, OOM) if wandb_logger: wandb_logger.log_error(\"prediction_error\", str(e)) raise HTTPException( status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Prediction failed: {str(e)}\" ) Error handling decisions:\ntry: # Inference except ValueError: # Client sent invalid data → 400 Bad Request # Log to W\u0026B for analysis return 400 except Exception: # Unexpected error (bug in code/model) → 500 Internal Server Error # Log to W\u0026B for alerting return 500 Why distinguish 400 vs 500:\n400: Client’s fault. No automatic retries. 500: Server’s fault. Client can retry. Error logging to W\u0026B: Allows detecting patterns. If you see 1000 validation_error for ocean_proximity=\"INVALID\", add a clearer error message.\n6. W\u0026B Logging: Production Observability Why Log Predictions Question: “Why log every prediction if I already have uvicorn logs?”\nAnswer: Uvicorn logs tell you:\nWhat endpoint was called HTTP status code How long it took W\u0026B logs tell you:\nWhat features were used What prediction was made Distribution of predictions (are they all in $200k-$500k? are there outliers?) Average latency per request Error rate (how many requests fail?) Real use case: Stakeholder reports “predictions are too high lately”. You open W\u0026B dashboard:\nprediction/mean: $450k (before: $380k) features/median_income: 9.2 (before: 7.5) Conclusion: No bug—clients are simply querying houses in more expensive areas (median_income higher). Without W\u0026B, you’d be debugging code for hours.\nThe Implementation # api/app/core/wandb_logger.py class WandBLogger: def __init__(self, project: str = \"housing-mlops-api\", enabled: bool = True): self.enabled = enabled and bool(os.getenv(\"WANDB_API_KEY\")) if self.enabled: self._run = wandb.init( project=self.project, job_type=\"api-inference\", # Distinguish from training runs config={ \"environment\": os.getenv(\"ENVIRONMENT\", \"production\"), \"model_version\": os.getenv(\"MODEL_VERSION\", \"unknown\") }, reinit=True # Allow multiple init() in same process ) def log_prediction( self, features: List[Dict], predictions: List[float], model_version: str, response_time_ms: float ) -\u003e None: if not self.enabled: return # Aggregated metrics wandb.log({ \"prediction/count\": len(predictions), \"prediction/mean\": sum(predictions) / len(predictions), \"prediction/min\": min(predictions), \"prediction/max\": max(predictions), \"performance/response_time_ms\": response_time_ms, \"model/version\": model_version, \"timestamp\": datetime.now().isoformat() }) # Feature distributions (sample first 100) if len(features) \u003c= 100: for i, (feat, pred) in enumerate(zip(features, predictions)): wandb.log({ f\"features/instance_{i}/median_income\": feat[\"median_income\"], f\"predictions/instance_{i}\": pred }) Why job_type=\"api-inference\":\nIn W\u0026B dashboard, you can filter by job type:\ntraining: Training pipeline runs sweep: Hyperparameter sweep runs api-inference: Production predictions Why reinit=True: A uvicorn process can live for days. reinit=True allows creating multiple W\u0026B runs within the same process (one per startup/restart).\nWhy sample first 100: Logging 10,000 individual features per request would be too much overhead. Sampling 100 gives a representative distribution without killing performance.\nW\u0026B Dashboard in Production # Metrics to monitor: prediction/count: Requests per minute (RPM) - Expected: 100-500 RPM - Alert: \u003c10 RPM (is it down?) or \u003e2000 RPM (DDoS?) prediction/mean: Average predicted price - Expected: $300k-$450k (depending on market) - Alert: \u003e$1M (broken model) or \u003c$50k (data drift) performance/response_time_ms: Latency - Expected: 30-60ms - Alert: \u003e200ms (slow model or CPU throttling) error/count: Errors per minute - Expected: 0-5 errors/min - Alert: \u003e50 errors/min (investigate immediately) 7. CORS and Security # api/app/main.py app.add_middleware( CORSMiddleware, allow_origins=[ \"http://localhost:3000\", # Local frontend (React/Streamlit) \"http://localhost:8080\", \"https://app.company.com\", # Production frontend ], allow_credentials=False, # No cookies (API is stateless) allow_methods=[\"GET\", \"POST\"], # Only necessary methods allow_headers=[\"Content-Type\", \"Authorization\"], max_age=3600, # Cache preflight requests for 1 hour ) Why restricted origins:\nAnti-pattern (permissive):\nallow_origins=[\"*\"] # BAD: Any site can call your API Problem: A malicious site evil.com can make requests to your API from the user’s browser, potentially:\nConsuming your GCP quota (if no auth) Making spam predictions DoS attack Correct pattern (restrictive):\nallow_origins=[\"https://app.company.com\"] # Only your frontend For local development: Add http://localhost:3000 temporarily, remove in production.\nWhy allow_credentials=False: This API is stateless—doesn’t use cookies or sessions. allow_credentials=True would be unnecessary and an additional attack surface.\n8. The Complete Flow of A Request Request:\ncurl -X POST http://localhost:8080/api/v1/predict \\ -H \"Content-Type: application/json\" \\ -d '{ \"instances\": [{ \"longitude\": -122.23, \"latitude\": 37.88, \"housing_median_age\": 41, \"total_rooms\": 880, \"total_bedrooms\": 129, \"population\": 322, \"households\": 126, \"median_income\": 8.3252, \"ocean_proximity\": \"NEAR BAY\" }] }' The internal journey (\u003c 50ms):\n1. FastAPI receives request (1ms) ├─ CORS middleware validates origin └─ Router match: POST /api/v1/predict 2. Pydantic validation (2ms) ├─ Parse JSON → PredictionRequest object ├─ Validate types (longitude: float ✓) ├─ Validate ranges (longitude: -122.23, within [-180, 180] ✓) └─ Custom validator (ocean_proximity: \"NEAR BAY\" → valid ✓) 3. Endpoint handler: predict() (40ms) ├─ Check model_loader.is_loaded (0.1ms) ├─ Convert Pydantic → DataFrame (1ms) ├─ Preprocessing (5ms) │ ├─ One-hot encode ocean_proximity │ ├─ Compute cluster similarity features │ └─ Scale numerical features ├─ Model inference (30ms) │ └─ RandomForest.predict(processed_features) ├─ Format response (1ms) └─ Log to W\u0026B (async, \u003c1ms non-blocking) 4. FastAPI serializes response (2ms) └─ PredictionResponse → JSON 5. HTTP response sent (1ms) Total: ~50ms Response:\n{ \"predictions\": [{ \"predicted_price\": 452600.0, \"confidence_interval\": null }], \"model_version\": \"models:/housing_price_model/Production\" } 9. What This Architecture Achieves Without this architecture (naive API):\nLoad model on every request (5 seconds/request) Manual validation prone to errors No observability (debugging is guessing) No API versioning (breaking changes break clients) Open CORS (vulnerability) With this architecture:\nLatency: \u003c50ms per prediction (cached model) Reliability: Pydantic guarantees valid requests before reaching the model Observability: W\u0026B dashboard shows prediction distribution, latency, errors Maintainability: Separation of concerns (core/routers/models) Security: Restrictive CORS, robust error handling Versioning: /api/v1/ allows evolving the API without breaking clients The real value: This API can scale from 10 requests/min to 10,000 requests/min without code changes—just add more containers with a load balancer. The architecture is already ready.\n11. Model Selection and Parameter Strategies Navigation ← Part 1: Pipeline and Orchestration | Part 3: Production and Best Practices →\nIn Part 3 we will cover:\nModel selection and parameter strategies Testing: Fixtures, mocking, and real coverage Production patterns (Transform Pattern, Data Drift, Feature Stores) Production Readiness Checklist ","wordCount":"9669","inLanguage":"en","datePublished":"2026-01-13T00:00:00Z","dateModified":"2026-01-13T00:00:00Z","author":{"@type":"Person","name":"Carlos Daniel Jiménez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-part-2-en/"},"publisher":{"@type":"Organization","name":"The Probability Engine","logo":{"@type":"ImageObject","url":"https://carlosdanieljimenez.com/img/icon.jpeg"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://carlosdanieljimenez.com/ accesskey=h title="The Probability Engine (Alt + H)">The Probability Engine</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://carlosdanieljimenez.com/mlops/ title=MLOps><span>MLOps</span></a></li><li><a href=https://carlosdanieljimenez.com/agentic-ai/ title="Agentic AI"><span>Agentic AI</span></a></li><li><a href=https://carlosdanieljimenez.com/tidytuesday/ title=TidyTuesday><span>TidyTuesday</span></a></li><li><a href=https://carlosdanieljimenez.com/post/ title=Posts><span>Posts</span></a></li><li><a href=https://carlosdanieljimenez.com/edge-computing/ title="Edge Computing"><span>Edge Computing</span></a></li><li><a href=https://carlosdanieljimenez.com/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://carlosdanieljimenez.com/about/ title=About><span>About</span></a></li><li><a href=https://carlosdanieljimenez.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Anatomy of an MLOps Pipeline - Part 2: Deployment and Infrastructure</h1><div class=post-description>Part 2: CI/CD with GitHub Actions, W&amp;B vs MLflow comparison, complete containerization with Docker, and production-ready API architecture with FastAPI.</div><div class=post-meta><span title='2026-01-13 00:00:00 +0000 UTC'>January 13, 2026</span>&nbsp;·&nbsp;<span>Carlos Daniel Jiménez</span></div></header><div class=post-content><blockquote><p><strong>Complete MLOps Series:</strong> <a href=/mlops/anatomia-pipeline-mlops-parte-1/>← Part 1: Pipeline</a> | <strong>Part 2 (current)</strong> | <a href=/mlops/anatomia-pipeline-mlops-parte-3/>Part 3: Production →</a></p></blockquote><p><a name=github-actions></a></p><h2 id=8-cicd-with-github-actions-complete-pipeline-automation>8. CI/CD with GitHub Actions: Complete Pipeline Automation<a hidden class=anchor aria-hidden=true href=#8-cicd-with-github-actions-complete-pipeline-automation>#</a></h2><h3 id=why-cicd-is-critical-in-mlops>Why CI/CD Is Critical in MLOps<a hidden class=anchor aria-hidden=true href=#why-cicd-is-critical-in-mlops>#</a></h3><p>As an MLOps engineer, one of the biggest friction points is manual deployment. You&rsquo;ve trained an excellent model on your laptop, but getting it to production requires:</p><ol><li>SSH to a server</li><li>Manually copy files</li><li>Install dependencies</li><li>Cross your fingers</li><li>Debug when something explodes</li></ol><p><strong>GitHub Actions eliminates this.</strong> Every commit triggers an automated pipeline that:</p><ul><li>Runs tests</li><li>Validates that code meets standards</li><li>Trains the model (optional, in simple pipelines)</li><li>Builds Docker images</li><li>Deploys to Cloud Run/ECS/Kubernetes</li></ul><h3 id=the-cicd-architecture-for-this-project>The CI/CD Architecture For This Project<a hidden class=anchor aria-hidden=true href=#the-cicd-architecture-for-this-project>#</a></h3><p>This project implements <strong>two separate workflows</strong>:</p><h4 id=1-pr-validation-workflow>1. PR Validation Workflow<a hidden class=anchor aria-hidden=true href=#1-pr-validation-workflow>#</a></h4><p><strong>Trigger:</strong> Every pull request to <code>main</code></p><p><strong>Purpose:</strong> Ensure code is production-ready before merging</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># .github/workflows/pr_validation.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>PR Validation - Tests &amp; Linting</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>pull_request</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>branches</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>main, master]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;src/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;api/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;tests/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;pyproject.toml&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;requirements.txt&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>lint</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Lint Code</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Python 3.12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/setup-python@v5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>python-version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.12&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install uv</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>pip install uv</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install dependencies</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          uv venv
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install -e .
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install ruff pytest pytest-cov</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run Ruff linter</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          ruff check src/ tests/ api/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run Ruff formatter check</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          ruff format --check src/ tests/ api/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>unit-tests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Unit Tests</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCP_PROJECT_ID</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_PROJECT_ID }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCS_BUCKET_NAME</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCS_BUCKET_NAME }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>WANDB_API_KEY</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WANDB_API_KEY }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Python 3.12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/setup-python@v5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>python-version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.12&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install dependencies</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          pip install uv
</span></span></span><span class=line><span class=cl><span class=sd>          uv venv
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install -e .
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install pytest pytest-cov pytest-mock</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run unit tests with coverage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          pytest tests/ -v \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov=src \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov=api/app \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov-report=xml \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov-report=term-missing \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov-fail-under=70</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Upload coverage to Codecov</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>codecov/codecov-action@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>file</span><span class=p>:</span><span class=w> </span><span class=l>./coverage.xml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>flags</span><span class=p>:</span><span class=w> </span><span class=l>unittests</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>codecov-umbrella</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>integration-tests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Integration Tests (Pipeline E2E)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCP_PROJECT_ID</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_PROJECT_ID }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCS_BUCKET_NAME</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCS_BUCKET_NAME }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>WANDB_API_KEY</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WANDB_API_KEY }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>MLFLOW_TRACKING_URI</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.MLFLOW_TRACKING_URI }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Python 3.12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/setup-python@v5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>python-version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.12&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Authenticate to Google Cloud</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>google-github-actions/auth@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>credentials_json</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_SA_KEY }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install dependencies</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          pip install uv
</span></span></span><span class=line><span class=cl><span class=sd>          uv venv
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install -e .</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run integration test (Steps 01-04)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          python main.py main.execute_steps=[01_download_data,02_preprocessing_and_imputation,03_feature_engineering,04_segregation]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>timeout-minutes</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Verify artifacts were created</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/train/train.parquet
</span></span></span><span class=line><span class=cl><span class=sd>          gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/test/test.parquet</span><span class=w>
</span></span></span></code></pre></div><p><strong>Value for the MLOps engineer:</strong></p><ul><li><strong>Prevents broken merges:</strong> If tests fail, the PR cannot be merged</li><li><strong>Code standards:</strong> Ruff guarantees consistency (important when you have 5+ contributors)</li><li><strong>Coverage tracking:</strong> Codecov shows what percentage of code is covered by tests</li><li><strong>Fast feedback:</strong> You know in 5 minutes if your change broke something, not 3 hours later</li></ul><h4 id=2-deployment-workflow>2. Deployment Workflow<a hidden class=anchor aria-hidden=true href=#2-deployment-workflow>#</a></h4><p><strong>Trigger:</strong> Push to <code>main</code> (after PR merge)</p><p><strong>Purpose:</strong> Build and deploy the API to production</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># .github/workflows/deploy_api.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Deploy API to Cloud Run</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>push</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>branches</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>main]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;api/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;models/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;.github/workflows/deploy_api.yaml&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>PROJECT_ID</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_PROJECT_ID }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>SERVICE_NAME</span><span class=p>:</span><span class=w> </span><span class=l>housing-price-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>REGION</span><span class=p>:</span><span class=w> </span><span class=l>us-central1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>build-and-deploy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Build Docker Image &amp; Deploy</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>permissions</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>contents</span><span class=p>:</span><span class=w> </span><span class=l>read</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>id-token</span><span class=p>:</span><span class=w> </span><span class=l>write</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Checkout code</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Authenticate to Google Cloud</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>google-github-actions/auth@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>workload_identity_provider</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WIF_PROVIDER }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>service_account</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WIF_SERVICE_ACCOUNT }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Cloud SDK</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>google-github-actions/setup-gcloud@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Configure Docker for GCR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>gcloud auth configure-docker gcr.io</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Download trained model from GCS</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          mkdir -p api/models/trained
</span></span></span><span class=line><span class=cl><span class=sd>          gsutil cp gs://${{ secrets.GCS_BUCKET_NAME }}/models/trained/housing_price_model.pkl \
</span></span></span><span class=line><span class=cl><span class=sd>            api/models/trained/housing_price_model.pkl</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Build Docker image</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          cd api
</span></span></span><span class=line><span class=cl><span class=sd>          docker build \
</span></span></span><span class=line><span class=cl><span class=sd>            --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest \
</span></span></span><span class=line><span class=cl><span class=sd>            .</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Push Docker image to GCR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }}
</span></span></span><span class=line><span class=cl><span class=sd>          docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Deploy to Cloud Run</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          gcloud run deploy ${{ env.SERVICE_NAME }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --image gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --platform managed \
</span></span></span><span class=line><span class=cl><span class=sd>            --region ${{ env.REGION }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --allow-unauthenticated \
</span></span></span><span class=line><span class=cl><span class=sd>            --set-env-vars=&#34;GCS_BUCKET=${{ secrets.GCS_BUCKET_NAME }},WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}&#34; \
</span></span></span><span class=line><span class=cl><span class=sd>            --memory 2Gi \
</span></span></span><span class=line><span class=cl><span class=sd>            --cpu 2 \
</span></span></span><span class=line><span class=cl><span class=sd>            --max-instances 10 \
</span></span></span><span class=line><span class=cl><span class=sd>            --min-instances 1 \
</span></span></span><span class=line><span class=cl><span class=sd>            --timeout 300</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Get Cloud Run URL</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>deploy-url</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          URL=$(gcloud run services describe ${{ env.SERVICE_NAME }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --platform managed \
</span></span></span><span class=line><span class=cl><span class=sd>            --region ${{ env.REGION }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --format &#39;value(status.url)&#39;)
</span></span></span><span class=line><span class=cl><span class=sd>          echo &#34;url=$URL&#34; &gt;&gt; $GITHUB_OUTPUT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run smoke test</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          curl -X POST &#34;${{ steps.deploy-url.outputs.url }}/api/v1/predict&#34; \
</span></span></span><span class=line><span class=cl><span class=sd>            -H &#34;Content-Type: application/json&#34; \
</span></span></span><span class=line><span class=cl><span class=sd>            -d &#39;{&#34;instances&#34;:[{&#34;longitude&#34;:-122.23,&#34;latitude&#34;:37.88,&#34;housing_median_age&#34;:41,&#34;total_rooms&#34;:880,&#34;total_bedrooms&#34;:129,&#34;population&#34;:322,&#34;households&#34;:126,&#34;median_income&#34;:8.3252,&#34;ocean_proximity&#34;:&#34;NEAR BAY&#34;}]}&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Notify deployment success</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>if</span><span class=p>:</span><span class=w> </span><span class=l>success()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          echo &#34;Deployment successful! API available at: ${{ steps.deploy-url.outputs.url }}&#34;</span><span class=w>
</span></span></span></code></pre></div><p><strong>Value for the MLOps engineer:</strong></p><ul><li><strong>Zero-downtime deployment:</strong> Cloud Run does rolling updates automatically</li><li><strong>Easy rollback:</strong> If something explodes, you do <code>gcloud run services update-traffic --to-revisions=PREVIOUS=100</code></li><li><strong>Automatic smoke test:</strong> Verifies the API responds after deploy</li><li><strong>Image versioning:</strong> Each commit has its own Docker image tagged with SHA</li></ul><h3 id=secrets-and-security>Secrets and Security<a hidden class=anchor aria-hidden=true href=#secrets-and-security>#</a></h3><p><strong>CRITICAL:</strong> Never commit secrets to the repo. GitHub Actions uses <strong>GitHub Secrets</strong> to store:</p><ul><li><code>GCP_PROJECT_ID</code>: GCP project ID</li><li><code>GCS_BUCKET_NAME</code>: GCS bucket name</li><li><code>WANDB_API_KEY</code>: W&amp;B API key</li><li><code>GCP_SA_KEY</code>: Service account key (JSON) for GCP authentication</li><li><code>WIF_PROVIDER</code> / <code>WIF_SERVICE_ACCOUNT</code>: Workload Identity Federation (more secure than SA keys)</li></ul><p><strong>Configuration in GitHub:</strong></p><ol><li>Go to repo → Settings → Secrets and variables → Actions</li><li>Create each secret</li><li>Workflows access them with <code>${{ secrets.SECRET_NAME }}</code></li></ol><h3 id=deployment-monitoring>Deployment Monitoring<a hidden class=anchor aria-hidden=true href=#deployment-monitoring>#</a></h3><p><strong>How to know if a deployment failed?</strong></p><p>GitHub Actions sends notifications to:</p><ul><li>Email (configured in GitHub profile)</li><li>Slack (with GitHub app)</li><li>Discord/Teams (with webhooks)</li></ul><p><strong>Post-deployment monitoring:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># Add post-deploy validation step</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run API health check</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    for i in {1..5}; do
</span></span></span><span class=line><span class=cl><span class=sd>      STATUS=$(curl -s -o /dev/null -w &#34;%{http_code}&#34; &#34;${{ steps.deploy-url.outputs.url }}/health&#34;)
</span></span></span><span class=line><span class=cl><span class=sd>      if [ $STATUS -eq 200 ]; then
</span></span></span><span class=line><span class=cl><span class=sd>        echo &#34;Health check passed&#34;
</span></span></span><span class=line><span class=cl><span class=sd>        exit 0
</span></span></span><span class=line><span class=cl><span class=sd>      fi
</span></span></span><span class=line><span class=cl><span class=sd>      echo &#34;Attempt $i failed, retrying...&#34;
</span></span></span><span class=line><span class=cl><span class=sd>      sleep 10
</span></span></span><span class=line><span class=cl><span class=sd>    done
</span></span></span><span class=line><span class=cl><span class=sd>    echo &#34;Health check failed after 5 attempts&#34;
</span></span></span><span class=line><span class=cl><span class=sd>    exit 1</span><span class=w>
</span></span></span></code></pre></div><h3 id=advanced-cicd-strategies>Advanced CI/CD Strategies<a hidden class=anchor aria-hidden=true href=#advanced-cicd-strategies>#</a></h3><h4 id=1-automatic-retraining-pipeline>1. Automatic Retraining Pipeline<a hidden class=anchor aria-hidden=true href=#1-automatic-retraining-pipeline>#</a></h4><p><strong>Trigger:</strong> Cron schedule (example: weekly)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>schedule</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>cron</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;0 2 * * 0&#39;</span><span class=w>  </span><span class=c># Every Sunday at 2 AM UTC</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>retrain-model</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run full pipeline</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>python main.py</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Compare metrics with production model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          NEW_MAPE=$(python scripts/get_latest_mape.py)
</span></span></span><span class=line><span class=cl><span class=sd>          PROD_MAPE=$(python scripts/get_production_mape.py)
</span></span></span><span class=line><span class=cl><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          if (( $(echo &#34;$NEW_MAPE &lt; $PROD_MAPE&#34; | bc -l) )); then
</span></span></span><span class=line><span class=cl><span class=sd>            echo &#34;New model is better, promoting to Production&#34;
</span></span></span><span class=line><span class=cl><span class=sd>            mlflow models transition --name housing_price_model --version latest --stage Production
</span></span></span><span class=line><span class=cl><span class=sd>          else
</span></span></span><span class=line><span class=cl><span class=sd>            echo &#34;New model is worse, keeping current Production model&#34;
</span></span></span><span class=line><span class=cl><span class=sd>          fi</span><span class=w>
</span></span></span></code></pre></div><p><strong>Value:</strong> The model automatically retrains with new data. If it improves, it&rsquo;s promoted to Production. If it worsens, it&rsquo;s discarded.</p><h4 id=2-canary-deployments>2. Canary Deployments<a hidden class=anchor aria-hidden=true href=#2-canary-deployments>#</a></h4><p><strong>Problem:</strong> A new model may have subtle bugs that don&rsquo;t appear in tests.</p><p><strong>Solution:</strong> Deploy the new model to only 10% of traffic, monitor for 1 hour, then migrate 100% if there are no errors.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Deploy canary (10% traffic)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    gcloud run services update-traffic ${{ env.SERVICE_NAME }} \
</span></span></span><span class=line><span class=cl><span class=sd>      --to-revisions=LATEST=10,PREVIOUS=90</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Wait and monitor</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>sleep 3600 </span><span class=w> </span><span class=c># 1 hour</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Check error rate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    ERROR_RATE=$(python scripts/check_error_rate.py --minutes=60)
</span></span></span><span class=line><span class=cl><span class=sd>    if (( $(echo &#34;$ERROR_RATE &gt; 0.05&#34; | bc -l) )); then
</span></span></span><span class=line><span class=cl><span class=sd>      echo &#34;Error rate too high, rolling back&#34;
</span></span></span><span class=line><span class=cl><span class=sd>      gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=PREVIOUS=100
</span></span></span><span class=line><span class=cl><span class=sd>      exit 1
</span></span></span><span class=line><span class=cl><span class=sd>    fi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Promote to 100% traffic</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=LATEST=100</span><span class=w>
</span></span></span></code></pre></div><h3 id=what-cicd-solves-in-mlops>What CI/CD Solves in MLOps<a hidden class=anchor aria-hidden=true href=#what-cicd-solves-in-mlops>#</a></h3><p><strong>Without CI/CD:</strong></p><ul><li>Manual deployment prone to errors</li><li>&ldquo;Works on my machine&rdquo; syndrome</li><li>Inconsistent testing</li><li>Rollback requires panic debugging</li><li>No history of what was deployed when</li></ul><p><strong>With CI/CD:</strong></p><ul><li>Automatic deployment on every merge</li><li>Tests guarantee code works</li><li>Rollback is a command</li><li>Complete history in GitHub Actions UI</li><li>Every deployment is reproducible</li></ul><h3 id=the-real-value-for-the-mlops-engineer>The Real Value For the MLOps Engineer<a hidden class=anchor aria-hidden=true href=#the-real-value-for-the-mlops-engineer>#</a></h3><p><strong>It&rsquo;s not about automating for the sake of automating.</strong> It&rsquo;s about:</p><ol><li><strong>Reducing toil:</strong> You spend time solving interesting problems, not manually copying files</li><li><strong>Confidence:</strong> You know the code works before it reaches production</li><li><strong>Speed:</strong> From commit to production in &lt;10 minutes</li><li><strong>Auditing:</strong> Every change is logged in GitHub</li><li><strong>Collaboration:</strong> Your team can deploy without depending on you</li></ol><p><strong>An MLOps engineer without CI/CD is like a software engineer without git—technically possible, but fundamentally broken.</strong></p><hr><p><a name=mlops-value-proposition></a></p><h2 id=9-the-value-of-mlops-why-this-matters>9. The Value of MLOps: Why This Matters<a hidden class=anchor aria-hidden=true href=#9-the-value-of-mlops-why-this-matters>#</a></h2><h3 id=the-central-question>The Central Question<a hidden class=anchor aria-hidden=true href=#the-central-question>#</a></h3><p>&ldquo;Why should I invest time in all this when I can train a model in a notebook in 30 minutes?&rdquo;</p><p>This is the question every MLOps engineer has heard. The short answer: <strong>because notebooks don&rsquo;t scale.</strong></p><p>The long answer is what this section covers.</p><h3 id=the-real-problem-research-code-vs-production-code>The Real Problem: Research Code vs Production Code<a hidden class=anchor aria-hidden=true href=#the-real-problem-research-code-vs-production-code>#</a></h3><h4 id=research-code-notebook>Research Code (Notebook)<a hidden class=anchor aria-hidden=true href=#research-code-notebook>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># notebook.ipynb</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 1</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;housing.csv&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 2</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 3</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestRegressor</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 4</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pickle</span>
</span></span><span class=line><span class=cl><span class=n>pickle</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;model.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;wb&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 5</span>
</span></span><span class=line><span class=cl><span class=c1># Wait, did I drop the right columns?</span>
</span></span><span class=line><span class=cl><span class=c1># Let me rerun cell 2... oh no, I ran it twice</span>
</span></span><span class=line><span class=cl><span class=c1># Now I have 0 rows, what happened?</span>
</span></span></code></pre></div><p><strong>Problems:</strong></p><ul><li>Not reproducible (execution order matters)</li><li>Not testable</li><li>Not versionable (git diffs are unreadable)</li><li>Not scalable (what happens with 100GB of data?)</li><li>Not auditable (what params did you use?)</li></ul><h4 id=production-code-this-pipeline>Production Code (This Pipeline)<a hidden class=anchor aria-hidden=true href=#production-code-this-pipeline>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/05_model_selection/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@hydra.main</span><span class=p>(</span><span class=n>config_path</span><span class=o>=</span><span class=s2>&#34;.&#34;</span><span class=p>,</span> <span class=n>config_name</span><span class=o>=</span><span class=s2>&#34;config&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Train model with versioned configuration.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Load data from GCS (single source of truth)</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>load_from_gcs</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>gcs_train_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Apply serialized preprocessing pipeline</span>
</span></span><span class=line><span class=cl>    <span class=n>pipeline</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;artifacts/preprocessing_pipeline.pkl&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Train with config params</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>hyperparameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Log to MLflow</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_params</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hyperparameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>(</span><span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></div><p><strong>Benefits:</strong></p><ul><li>Reproducible (same config = same output)</li><li>Testable (pure functions, mocking)</li><li>Versionable (readable git diff)</li><li>Scalable (runs locally or on cluster)</li><li>Auditable (MLflow tracking)</li></ul><h3 id=value-1-code-modularization>Value #1: Code Modularization<a hidden class=anchor aria-hidden=true href=#value-1-code-modularization>#</a></h3><h4 id=why-it-matters>Why It Matters<a hidden class=anchor aria-hidden=true href=#why-it-matters>#</a></h4><p><strong>Scenario:</strong> Your model has a bug in preprocessing. In a notebook, preprocessing is mixed with feature engineering, training, and evaluation in 300 lines.</p><p><strong>In this pipeline:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Bug is in preprocessing → only edit src/data/02_preprocessing/</span>
</span></span><span class=line><span class=cl><span class=c1># Tests fail → pytest tests/test_preprocessor.py</span>
</span></span><span class=line><span class=cl><span class=c1># Fix → re-run only steps 02-07, not 01</span>
</span></span></code></pre></div><p><strong>Time saved:</strong> Hours per bug.</p><h4 id=separation-of-concerns>Separation of Concerns<a hidden class=anchor aria-hidden=true href=#separation-of-concerns>#</a></h4><p>This pipeline separates:</p><ol><li><strong>Data steps (01-04):</strong> Produce reusable artifacts</li><li><strong>Model steps (05-07):</strong> Consume artifacts, produce models</li><li><strong>API:</strong> Consumes models, produces predictions</li><li><strong>Frontend:</strong> Consumes API, produces UX</li></ol><p><strong>Benefit:</strong> Teams can work in parallel. The data scientist modifies feature engineering without touching the API. The frontend engineer modifies UI without understanding Random Forests.</p><h3 id=value-2-working-with-artifacts>Value #2: Working with Artifacts<a hidden class=anchor aria-hidden=true href=#value-2-working-with-artifacts>#</a></h3><h4 id=the-problem-where-is-model_final_v3pkl>The Problem: &ldquo;Where is model_final_v3.pkl?&rdquo;<a hidden class=anchor aria-hidden=true href=#the-problem-where-is-model_final_v3pkl>#</a></h4><p>Without artifact management:</p><pre tabindex=0><code>models/
├── model_v1.pkl
├── model_v2.pkl
├── model_final.pkl
├── model_final_FINAL.pkl
├── model_final_REAL.pkl
├── model_production_2024_01_15.pkl  # Is this the production one?
└── model_old_backup.pkl  # Can I delete this?
</code></pre><p><strong>Problems:</strong></p><ul><li>You don&rsquo;t know what hyperparameters each one uses</li><li>You don&rsquo;t know what metrics it achieved</li><li>You don&rsquo;t know what data it was trained on</li><li>Rollback = searching for the correct file</li></ul><h4 id=the-solution-artifact-storage--metadata>The Solution: Artifact Storage + Metadata<a hidden class=anchor aria-hidden=true href=#the-solution-artifact-storage--metadata>#</a></h4><p><strong>1. Google Cloud Storage for data:</strong></p><pre tabindex=0><code>gs://bucket-name/
├── data/
│   ├── 01-raw/housing.parquet                    # Immutable
│   ├── 02-processed/housing_processed.parquet    # Versioned by date
│   ├── 03-features/housing_features.parquet
│   └── 04-split/
│       ├── train/train.parquet
│       └── test/test.parquet
├── artifacts/
│   ├── imputer.pkl                               # Preprocessing artifacts
│   ├── preprocessing_pipeline.pkl
│   └── scaler.pkl
└── models/
    └── trained/housing_price_model.pkl           # Latest trained
</code></pre><p><strong>Benefits:</strong></p><ul><li><strong>Immutability:</strong> <code>01-raw/</code> never changes, you can always re-run the pipeline</li><li><strong>Versioning:</strong> Each run has a timestamp, you can compare versions</li><li><strong>Sharing:</strong> The whole team accesses the same data, not &ldquo;send me the CSV via Slack&rdquo;</li></ul><p><strong>2. MLflow for models:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Register model</span>
</span></span><span class=line><span class=cl><span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MLflow automatically saves:</span>
</span></span><span class=line><span class=cl><span class=c1># - The model pickle</span>
</span></span><span class=line><span class=cl><span class=c1># - The hyperparameters (n_estimators=200, max_depth=20)</span>
</span></span><span class=line><span class=cl><span class=c1># - The metrics (MAPE=7.8%, R²=0.87)</span>
</span></span><span class=line><span class=cl><span class=c1># - Metadata (date, duration, user)</span>
</span></span><span class=line><span class=cl><span class=c1># - Code (git commit SHA)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load model in production</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>pyfunc</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Benefits:</strong></p><ul><li><strong>Semantic versioning:</strong> v1, v2, v3 with stages (Staging/Production)</li><li><strong>Rich metadata:</strong> You know exactly what each version is</li><li><strong>Trivial rollback:</strong> <code>transition v2 to Production</code></li><li><strong>Comparison:</strong> MLflow UI shows table comparing all versions</li></ul><p><strong>3. W&amp;B for experiments:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Each sweep run logs:</span>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;hyperparameters/n_estimators&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;hyperparameters/max_depth&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metrics/mape&#34;</span><span class=p>:</span> <span class=mf>7.8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metrics/r2&#34;</span><span class=p>:</span> <span class=mf>0.87</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;plots/feature_importances&#34;</span><span class=p>:</span> <span class=n>wandb</span><span class=o>.</span><span class=n>Image</span><span class=p>(</span><span class=n>fig</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;dataset/train_size&#34;</span><span class=p>:</span> <span class=mi>16512</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># W&amp;B dashboard:</span>
</span></span><span class=line><span class=cl><span class=c1># - Table with 50 sweep runs</span>
</span></span><span class=line><span class=cl><span class=c1># - Filter by MAPE &lt; 8%</span>
</span></span><span class=line><span class=cl><span class=c1># - Parallel coordinates plot showing relationship between hyperparameters and MAPE</span>
</span></span><span class=line><span class=cl><span class=c1># - Compare top 5 runs side-by-side</span>
</span></span></code></pre></div><p><strong>Benefits:</strong></p><ul><li><strong>Visualization:</strong> Interactive plots of how each hyperparameter affects metrics</li><li><strong>Collaboration:</strong> Your team sees your experiments in real-time</li><li><strong>Reproducibility:</strong> Each run has a permanent link with all context</li></ul><h3 id=value-3-pipeline-architecture>Value #3: Pipeline Architecture<a hidden class=anchor aria-hidden=true href=#value-3-pipeline-architecture>#</a></h3><h4 id=why-a-pipeline-not-a-script>Why A Pipeline, Not A Script<a hidden class=anchor aria-hidden=true href=#why-a-pipeline-not-a-script>#</a></h4><p><strong>Single script (run_all.py):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># run_all.py (500 lines)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1># Download data</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>download_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Preprocess</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>preprocess</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Feature engineering</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>add_features</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Train model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>train_model</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Deploy</span>
</span></span><span class=line><span class=cl>    <span class=n>deploy_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Problems:</strong></p><ul><li>If it fails in train_model(), you re-run EVERYTHING (including slow download)</li><li>You can&rsquo;t run just feature engineering to experiment</li><li>Changing preprocessing requires retraining everything</li><li>No intermediate checkpoints</li></ul><p><strong>Modular pipeline:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Run everything</span>
</span></span><span class=line><span class=cl>make run-pipeline
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run only preprocessing</span>
</span></span><span class=line><span class=cl>make run-preprocessing
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run from feature engineering onwards</span>
</span></span><span class=line><span class=cl>python main.py main.execute_steps<span class=o>=[</span>03_feature_engineering,04_segregation,05_model_selection<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Debugging: run only the step that failed</span>
</span></span><span class=line><span class=cl>python src/data/03_feature_engineering/main.py --debug
</span></span></code></pre></div><p><strong>Benefits:</strong></p><ul><li><strong>Selective execution:</strong> Only re-run what changed</li><li><strong>Fast debugging:</strong> Test a step in isolation</li><li><strong>Parallelization:</strong> Independent steps can run in parallel</li><li><strong>Checkpointing:</strong> If step 05 fails, steps 01-04 are already done</li></ul><h4 id=the-contract-between-steps>The Contract Between Steps<a hidden class=anchor aria-hidden=true href=#the-contract-between-steps>#</a></h4><p>Each step:</p><ul><li><strong>Input:</strong> Path to artifact in GCS (example: <code>data/02-processed/housing_processed.parquet</code>)</li><li><strong>Output:</strong> Path to new artifact in GCS (example: <code>data/03-features/housing_features.parquet</code>)</li><li><strong>Side effects:</strong> Logs to MLflow/W&amp;B</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Step 03: Feature Engineering</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Input</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>load_from_gcs</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>gcs_input_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Transform</span>
</span></span><span class=line><span class=cl>    <span class=n>df_transformed</span> <span class=o>=</span> <span class=n>apply_feature_engineering</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Output</span>
</span></span><span class=line><span class=cl>    <span class=n>save_to_gcs</span><span class=p>(</span><span class=n>df_transformed</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>gcs_output_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Side effects</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_artifact</span><span class=p>(</span><span class=s2>&#34;preprocessing_pipeline.pkl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span><span class=s2>&#34;optimization/optimal_k&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>})</span>
</span></span></code></pre></div><p>This <strong>contract</strong> allows each step to be:</p><ul><li>Tested independently</li><li>Developed by different people</li><li>Replaced without affecting other steps</li></ul><h3 id=value-4-production-ready-vs-research-code>Value #4: Production-Ready vs Research Code<a hidden class=anchor aria-hidden=true href=#value-4-production-ready-vs-research-code>#</a></h3><h4 id=production-ready-checklist>Production-Ready Checklist<a hidden class=anchor aria-hidden=true href=#production-ready-checklist>#</a></h4><table><thead><tr><th>Feature</th><th>Research Code</th><th>This Pipeline</th></tr></thead><tbody><tr><td><strong>Versioning</strong></td><td>Git (badly, notebooks)</td><td>Git + GCS + MLflow</td></tr><tr><td><strong>Testing</strong></td><td>Manual (&ldquo;ran it once&rdquo;)</td><td>pytest + CI</td></tr><tr><td><strong>Configuration</strong></td><td>Hardcoded</td><td>Versioned YAML</td></tr><tr><td><strong>Secrets</strong></td><td>Exposed in code</td><td>.env + GitHub Secrets</td></tr><tr><td><strong>Logs</strong></td><td>print() statements</td><td>Structured logging</td></tr><tr><td><strong>Monitoring</strong></td><td>&ldquo;Hope it works&rdquo;</td><td>W&amp;B + MLflow tracking</td></tr><tr><td><strong>Deployment</strong></td><td>Manual</td><td>Automatic CI/CD</td></tr><tr><td><strong>Rollback</strong></td><td>Panic debugging</td><td>Transition in MLflow</td></tr><tr><td><strong>Documentation</strong></td><td>Outdated README</td><td>Self-documented code + Markdown in MLflow</td></tr><tr><td><strong>Collaboration</strong></td><td>&ldquo;Run these 10 cells in order&rdquo;</td><td><code>make run-pipeline</code></td></tr></tbody></table><h4 id=the-real-cost-of-not-doing-mlops>The Real Cost of Not Doing MLOps<a hidden class=anchor aria-hidden=true href=#the-real-cost-of-not-doing-mlops>#</a></h4><p><strong>Scenario:</strong> A model in production has a bug that causes incorrect predictions.</p><p><strong>Without MLOps (Research Code):</strong></p><ol><li>Detect the bug: User reports → 2 hours</li><li>Reproduce the bug: Search for what code/data was used → 4 hours</li><li>Fix: Run notebook locally → 1 hour</li><li>Deploy: SSH, copy pickle, restart server → 30 min</li><li>Verify: Run manual tests → 1 hour</li><li><strong>Total: 8.5 hours of downtime</strong></li></ol><p><strong>With MLOps (This Pipeline):</strong></p><ol><li>Detect the bug: Automatic monitoring alerts → 5 min</li><li>Rollback: <code>transition v3 to Archived</code> + <code>transition v2 to Production</code> → 2 min</li><li>Fix: Identify issue with MLflow metadata, fix code → 1 hour</li><li>Deploy: Push to GitHub → Automatic CI/CD → 10 min</li><li>Verify: Automatic smoke tests pass → 1 min</li><li><strong>Total: 1 hour 18 min of downtime (>85% reduction)</strong></li></ol><p><strong>Annual savings:</strong> If this happens 4 times a year, you save 29 hours of engineer time.</p><h3 id=value-5-data-driven-decisions>Value #5: Data-Driven Decisions<a hidden class=anchor aria-hidden=true href=#value-5-data-driven-decisions>#</a></h3><h4 id=the-anti-pattern>The Anti-Pattern<a hidden class=anchor aria-hidden=true href=#the-anti-pattern>#</a></h4><p>&ldquo;I used Random Forest with <code>n_estimators=100</code> because that&rsquo;s what everyone does.&rdquo;</p><p><strong>Problem:</strong> You have no evidence it&rsquo;s the best choice.</p><h4 id=this-pipeline>This Pipeline<a hidden class=anchor aria-hidden=true href=#this-pipeline>#</a></h4><p>Every decision has quantifiable metrics:</p><p><strong>1. Imputation:</strong></p><ul><li>Compared 4 strategies (Simple median, Simple mean, KNN, IterativeImputer)</li><li>IterativeImputer won with RMSE=0.52 (vs 0.78 for median)</li><li>Comparison plot in W&amp;B: <code>wandb.ai/project/run/imputation_comparison</code></li></ul><p><strong>2. Feature Engineering:</strong></p><ul><li>Optimized K from 5 to 15</li><li>K=8 maximized silhouette score (0.64)</li><li>Elbow method plot in W&amp;B</li></ul><p><strong>3. Hyperparameter Tuning:</strong></p><ul><li>Bayesian sweep of 50 runs</li><li>Optimal config: <code>n_estimators=200, max_depth=20</code></li><li>MAPE improved from 8.5% to 7.8%</li><li>Link to sweep: <code>wandb.ai/project/sweeps/abc123</code></li></ul><p><strong>Benefit:</strong> Six months later, when the stakeholder asks &ldquo;why do we use this model?&rdquo;, you open W&amp;B/MLflow and the answer is there with plots and metrics.</p><h3 id=the-roi-of-mlops>The ROI of MLOps<a hidden class=anchor aria-hidden=true href=#the-roi-of-mlops>#</a></h3><p><strong>Initial investment:</strong></p><ul><li>Setup of GCS, MLflow, W&amp;B, CI/CD: 2-3 days</li><li>Refactoring code to modular pipeline: 1-2 weeks</li></ul><p><strong>Return:</strong></p><ul><li>Deployment time: 8 hours → 10 minutes (48x faster)</li><li>Debugging time: 4 hours → 30 min (8x faster)</li><li>Onboarding new engineers: 1 week → 1 day</li><li>Team confidence: &ldquo;Hope it works&rdquo; → &ldquo;I know it works&rdquo;</li></ul><p><strong>For a team of 5 people, breakeven is ~1 month.</strong></p><h3 id=the-final-lesson-for-mlops-engineers>The Final Lesson For MLOps Engineers<a hidden class=anchor aria-hidden=true href=#the-final-lesson-for-mlops-engineers>#</a></h3><p><strong>It&rsquo;s not about the tools.</strong> You can replace:</p><ul><li>GCS → S3 → Azure Blob</li><li>MLflow → Neptune → Comet</li><li>W&amp;B → TensorBoard → MLflow</li><li>GitHub Actions → GitLab CI → Jenkins</li></ul><p><strong>It&rsquo;s about the principles:</strong></p><ol><li><strong>Modularization:</strong> Code in testable modules, not monolithic notebooks</li><li><strong>Artifact Management:</strong> Versioned data and models, not <code>model_final_v3.pkl</code></li><li><strong>Automation:</strong> CI/CD eliminates toil</li><li><strong>Observability:</strong> Logs, metrics, tracking</li><li><strong>Reproducibility:</strong> Same input → same output</li><li><strong>Data-driven decisions:</strong> Every choice backed by metrics</li></ol><p><strong>When you understand this, you&rsquo;re an MLOps engineer. When you implement it, you&rsquo;re a good MLOps engineer.</strong></p><hr><p><a name=wandb-vs-mlflow></a></p><h2 id=95-wb-vs-mlflow-why-both-not-one-or-the-other>9.5. W&amp;B vs MLflow: Why Both, Not One or the Other<a hidden class=anchor aria-hidden=true href=#95-wb-vs-mlflow-why-both-not-one-or-the-other>#</a></h2><h3 id=the-uncomfortable-question>The Uncomfortable Question<a hidden class=anchor aria-hidden=true href=#the-uncomfortable-question>#</a></h3><p>&ldquo;Why do you have Weights & Biases AND MLflow? Aren&rsquo;t they the same?&rdquo;</p><p>This question reveals a fundamental misunderstanding about what each tool does. They&rsquo;re not competitors—they&rsquo;re <strong>allies with different responsibilities</strong>. Understanding this separates a data scientist who experiments from an MLOps engineer who builds systems.</p><p>The short answer: <strong>W&amp;B is your research lab. MLflow is your production line.</strong></p><p>The long answer is what this section covers, with examples from this project&rsquo;s actual code.</p><hr><h3 id=the-real-problem-experimentation-vs-governance>The Real Problem: Experimentation vs Governance<a hidden class=anchor aria-hidden=true href=#the-real-problem-experimentation-vs-governance>#</a></h3><h4 id=phase-1-experimentation-50-100-runsday>Phase 1: Experimentation (50-100 runs/day)<a hidden class=anchor aria-hidden=true href=#phase-1-experimentation-50-100-runsday>#</a></h4><p>When you&rsquo;re in experimentation phase:</p><ul><li>You run 50 sweep runs testing hyperparameter combinations</li><li>You need to see <strong>in real-time</strong> how each run evolves</li><li>You want to visually compare 20 runs simultaneously</li><li>You need to see convergence plots, feature distributions, confusion matrices</li><li>Logging overhead must be minimal (asynchronous logging)</li></ul><p><strong>Correct tool:</strong> Weights & Biases</p><h4 id=phase-2-governance-and-deployment-1-2-modelsweek>Phase 2: Governance and Deployment (1-2 models/week)<a hidden class=anchor aria-hidden=true href=#phase-2-governance-and-deployment-1-2-modelsweek>#</a></h4><p>When you promote a model to production:</p><ul><li>You need semantic versioning (v1, v2, v3)</li><li>You need stages (Staging → Production)</li><li>You need rich metadata (what hyperparameters? what data? what commit?)</li><li>You need an API to load models (<code>models:/housing_price_model/Production</code>)</li><li>You need trivial rollback (transition v2 to Production)</li></ul><p><strong>Correct tool:</strong> MLflow Model Registry</p><p><strong>The uncomfortable truth:</strong> No tool does both things well.</p><hr><h3 id=how-this-project-uses-wb>How This Project Uses W&amp;B<a hidden class=anchor aria-hidden=true href=#how-this-project-uses-wb>#</a></h3><h4 id=1-hyperparameter-sweep-step-06-bayesian-optimization>1. Hyperparameter Sweep (Step 06): Bayesian Optimization<a hidden class=anchor aria-hidden=true href=#1-hyperparameter-sweep-step-06-bayesian-optimization>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/06_sweep/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Sweep configuration (Bayesian optimization)</span>
</span></span><span class=line><span class=cl><span class=n>sweep_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;method&#34;</span><span class=p>:</span> <span class=s2>&#34;bayes&#34;</span><span class=p>,</span>  <span class=c1># Bayesian &gt; Grid &gt; Random</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metric&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;wmape&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;goal&#34;</span><span class=p>:</span> <span class=s2>&#34;minimize&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;early_terminate&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;hyperband&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_iter&#34;</span><span class=p>:</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;parameters&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>50</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>300</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>30</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_samples_leaf&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize sweep</span>
</span></span><span class=line><span class=cl><span class=n>sweep_id</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>sweep</span><span class=p>(</span><span class=n>sweep</span><span class=o>=</span><span class=n>sweep_config</span><span class=p>,</span> <span class=n>project</span><span class=o>=</span><span class=s2>&#34;housing-mlops-gcp&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Training function that W&amp;B calls 50 times</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>run</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>init</span><span class=p>()</span>  <span class=c1># W&amp;B automatically assigns hyperparameters</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Get hyperparameters suggested by Bayesian optimizer</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Train model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>n_estimators</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>n_estimators</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_depth</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>max_depth</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=c1># ...</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Evaluate</span>
</span></span><span class=line><span class=cl>    <span class=n>metrics</span> <span class=o>=</span> <span class=n>evaluate_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Log to W&amp;B (asynchronous, non-blocking)</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;hyperparameters/n_estimators&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>n_estimators</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;hyperparameters/max_depth&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>max_depth</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;metrics/mape&#34;</span><span class=p>:</span> <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;mape&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;metrics/wmape&#34;</span><span class=p>:</span> <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;wmape&#39;</span><span class=p>],</span>  <span class=c1># Optimizer uses this</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;metrics/r2&#34;</span><span class=p>:</span> <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;r2&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;plots/feature_importances&#34;</span><span class=p>:</span> <span class=n>wandb</span><span class=o>.</span><span class=n>Image</span><span class=p>(</span><span class=n>fig</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>run</span><span class=o>.</span><span class=n>finish</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run 50 runs with Bayesian optimization</span>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>agent</span><span class=p>(</span><span class=n>sweep_id</span><span class=p>,</span> <span class=n>function</span><span class=o>=</span><span class=n>train</span><span class=p>,</span> <span class=n>count</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>What W&amp;B does here that MLflow can&rsquo;t:</strong></p><ol><li><p><strong>Bayesian Optimization</strong>: W&amp;B suggests the next hyperparameters based on previous runs. It&rsquo;s not random—it uses Gaussian Processes to efficiently explore the space.</p><pre tabindex=0><code>Run 1: n_estimators=100, max_depth=15 → wMAPE=8.5%
Run 2: n_estimators=200, max_depth=20 → wMAPE=7.9%  # Better
Run 3: n_estimators=250, max_depth=22 → wMAPE=7.8%  # W&amp;B suggests values close to Run 2
</code></pre></li><li><p><strong>Early Termination (Hyperband)</strong>: If a run is performing badly in the first 3 iterations (epochs), W&amp;B kills it automatically and tries other hyperparameters. Saves ~40% of compute.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;early_terminate&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;hyperband&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;min_iter&#34;</span><span class=p>:</span> <span class=mi>3</span>  <span class=c1># Minimum 3 iterations before terminating</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><p><strong>Parallel Coordinates Plot</strong>: Interactive visualization showing which hyperparameter combination produces the best wMAPE.</p><p><img alt="W&amp;B Parallel Coordinates" loading=lazy src=https://docs.wandb.ai/assets/images/parallel-coordinates.png></p><p><strong>Interpretation:</strong> Blue lines (runs with low wMAPE) converge at <code>n_estimators=200-250</code> and <code>max_depth=20-25</code>. This visually tells you where the optimum is.</p></li><li><p><strong>Asynchronous Logging</strong>: <code>wandb.log()</code> doesn&rsquo;t block. While the model trains, W&amp;B uploads metrics in the background. Total overhead: &lt;1% of training time.</p></li></ol><p><strong>MLflow doesn&rsquo;t have:</strong></p><ul><li>Bayesian optimization (only Grid/Random search via scikit-learn)</li><li>Intelligent early termination</li><li>Parallel coordinates plots</li><li>Asynchronous logging (mlflow.log is synchronous)</li></ul><hr><h4 id=2-real-time-monitoring-see-runs-while-they-run>2. Real-Time Monitoring: See Runs While They Run<a hidden class=anchor aria-hidden=true href=#2-real-time-monitoring-see-runs-while-they-run>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># In W&amp;B dashboard (web UI):</span>
</span></span><span class=line><span class=cl><span class=c1># - See 50 simultaneous runs in interactive table</span>
</span></span><span class=line><span class=cl><span class=c1># - Filter by &#34;wmape &lt; 8.0%&#34; → shows only 12 runs</span>
</span></span><span class=line><span class=cl><span class=c1># - Compare top 5 runs side-by-side</span>
</span></span><span class=line><span class=cl><span class=c1># - See convergence plots (MAPE vs iteration)</span>
</span></span></code></pre></div><p><strong>Real use case:</strong> You start a 50-run sweep at 9 AM. At 10 AM, from your laptop at the coffee shop:</p><ol><li>Open W&amp;B dashboard</li><li>See that 30 runs have finished</li><li>Filter by <code>wmape &lt; 8.0%</code> → 8 runs qualify</li><li>Compare those 8 runs → identify that <code>max_depth=20</code> appears in all</li><li><strong>Decision:</strong> Cancel the sweep, adjust <code>max_depth</code> range to [18, 25], restart</li></ol><p><strong>Value:</strong> Immediate feedback without SSH to the server, without reading terminal logs. Experimentation is <strong>interactive</strong>, not batch.</p><hr><h4 id=3-lightweight-artifact-tracking-references-to-gcs>3. Lightweight Artifact Tracking (References to GCS)<a hidden class=anchor aria-hidden=true href=#3-lightweight-artifact-tracking-references-to-gcs>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/05_model_selection/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Upload model to GCS</span>
</span></span><span class=line><span class=cl><span class=n>model_gcs_uri</span> <span class=o>=</span> <span class=n>upload_model_to_gcs</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;models/05-selection/randomforest_best.pkl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># gs://bucket/models/05-selection/randomforest_best.pkl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Log reference in W&amp;B (does NOT upload the pickle, only the URI)</span>
</span></span><span class=line><span class=cl><span class=n>artifact</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>Artifact</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=o>=</span><span class=s2>&#34;best_model_selection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nb>type</span><span class=o>=</span><span class=s2>&#34;model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Best model selected: RandomForest&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>artifact</span><span class=o>.</span><span class=n>add_reference</span><span class=p>(</span><span class=n>model_gcs_uri</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;best_model.pkl&#34;</span><span class=p>)</span>  <span class=c1># Only the URI</span>
</span></span><span class=line><span class=cl><span class=n>run</span><span class=o>.</span><span class=n>log_artifact</span><span class=p>(</span><span class=n>artifact</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>W&amp;B doesn&rsquo;t store the model</strong>—it only saves the URI <code>gs://...</code>. The model lives in GCS.</p><p><strong>Advantage:</strong> You don&rsquo;t pay for double storage (GCS + W&amp;B). W&amp;B is the index, GCS is the warehouse.</p><hr><h3 id=how-this-project-uses-mlflow>How This Project Uses MLflow<a hidden class=anchor aria-hidden=true href=#how-this-project-uses-mlflow>#</a></h3><h4 id=1-model-registry-step-07-versioning-and-stages>1. Model Registry (Step 07): Versioning and Stages<a hidden class=anchor aria-hidden=true href=#1-model-registry-step-07-versioning-and-stages>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/07_registration/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>(</span><span class=n>run_name</span><span class=o>=</span><span class=s2>&#34;model_registration&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Log model</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Log params and metrics</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_params</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;mape&#34;</span><span class=p>:</span> <span class=mf>7.82</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;r2&#34;</span><span class=p>:</span> <span class=mf>0.8654</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Register in Model Registry</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span> <span class=o>=</span> <span class=n>MlflowClient</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Create registered model (if it doesn&#39;t exist)</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span><span class=o>.</span><span class=n>create_registered_model</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Housing price prediction - Random Forest&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Create new version</span>
</span></span><span class=line><span class=cl>    <span class=n>model_version</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>create_model_version</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>source</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;runs:/</span><span class=si>{</span><span class=n>run_id</span><span class=si>}</span><span class=s2>/model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>run_id</span><span class=o>=</span><span class=n>run_id</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Result: housing_price_model/v3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Transition to stage</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span><span class=o>.</span><span class=n>transition_model_version_stage</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>version</span><span class=o>=</span><span class=n>model_version</span><span class=o>.</span><span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>stage</span><span class=o>=</span><span class=s2>&#34;Staging&#34;</span>  <span class=c1># Staging → Production when validated</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p><strong>What MLflow does here that W&amp;B can&rsquo;t:</strong></p><ol><li><p><strong>Semantic Versioning</strong>: Each model is <code>housing_price_model/v1</code>, <code>v2</code>, <code>v3</code>. They&rsquo;re not random IDs—they&rsquo;re incremental versions.</p></li><li><p><strong>Stages</strong>: A model goes through <code>None → Staging → Production → Archived</code>. This lifecycle is explicit.</p><pre tabindex=0><code>v1: Production (current in API)
v2: Staging (being validated)
v3: None (just trained)
v4: Archived (deprecated)
</code></pre></li><li><p><strong>Model-as-Code API</strong>: Loading a model in the API is trivial:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/model_loader.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>pyfunc</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>You don&rsquo;t need to know:</strong></p><ul><li>Where the pickle is physically located</li><li>What version it is (MLflow resolves &ldquo;Production&rdquo; → v1)</li><li>How to deserialize it (mlflow.pyfunc abstracts this)</li></ul></li><li><p><strong>Rollback in 10 Seconds</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Problem: v3 in Production has a bug</span>
</span></span><span class=line><span class=cl><span class=c1># Rollback to v2:</span>
</span></span><span class=line><span class=cl>mlflow models transition <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stage Production
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API detects the change and automatically reloads v2</span>
</span></span></code></pre></div></li><li><p><strong>Rich Metadata with Tags and Description</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Add searchable tags</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>set_model_version_tag</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;training_date&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;2026-01-13&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>set_model_version_tag</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sweep_id&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;abc123xyz&#34;</span>  <span class=c1># Link to W&amp;B sweep</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Description in Markdown</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>update_model_version</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=o>=</span><span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    # Housing Price Model v3
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    **Trained:** 2026-01-13
</span></span></span><span class=line><span class=cl><span class=s2>    **Algorithm:** Random Forest
</span></span></span><span class=line><span class=cl><span class=s2>    **Metrics:** MAPE=7.8%, R²=0.865
</span></span></span><span class=line><span class=cl><span class=s2>    **Sweep:** [W&amp;B Link](https://wandb.ai/project/sweeps/abc123)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>Result:</strong> 6 months later, when a stakeholder asks &ldquo;what model is in Production?&rdquo;, you open MLflow UI and all the info is there—not in a lost Slack thread.</p></li></ol><p><strong>W&amp;B doesn&rsquo;t have:</strong></p><ul><li>Model Registry (only basic artifact tracking)</li><li>Stages (Staging/Production)</li><li>Load API (<code>models:/name/stage</code>)</li><li>Transition history (who changed v2 to Production, when, why)</li></ul><hr><h4 id=2-pipeline-orchestration-mainpy>2. Pipeline Orchestration (main.py)<a hidden class=anchor aria-hidden=true href=#2-pipeline-orchestration-mainpy>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@hydra.main</span><span class=p>(</span><span class=n>config_path</span><span class=o>=</span><span class=s2>&#34;.&#34;</span><span class=p>,</span> <span class=n>config_name</span><span class=o>=</span><span class=s2>&#34;config&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>go</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># MLflow orchestrates steps as sub-runs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 01: Download</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>run</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>uri</span><span class=o>=</span><span class=s2>&#34;src/data/01_download_data&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>entry_point</span><span class=o>=</span><span class=s2>&#34;main&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>parameters</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;file_url&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>download_data</span><span class=o>.</span><span class=n>file_url</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;gcs_output_path&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>download_data</span><span class=o>.</span><span class=n>gcs_output_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 02: Preprocessing</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>run</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>uri</span><span class=o>=</span><span class=s2>&#34;src/data/02_preprocessing_and_imputation&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>entry_point</span><span class=o>=</span><span class=s2>&#34;main&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>parameters</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;gcs_input_path&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>gcs_input_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># ... Steps 03-07</span>
</span></span></code></pre></div><p><strong>MLflow creates a hierarchical run:</strong></p><pre tabindex=0><code>Parent Run: end_to_end_pipeline
├── Child Run: 01_download_data
│   ├── params: file_url, gcs_output_path
│   └── artifacts: housing.parquet
├── Child Run: 02_preprocessing_and_imputation
│   ├── params: imputation_strategy
│   └── artifacts: imputer.pkl, housing_processed.parquet
├── Child Run: 03_feature_engineering
│   └── ...
└── Child Run: 07_registration
    └── artifacts: model.pkl, model_config.yaml
</code></pre><p><strong>Value:</strong> In MLflow UI, you see the entire pipeline execution as a tree. Each step is auditable—what params it used, how long it took, what artifacts it produced.</p><p><strong>W&amp;B doesn&rsquo;t have pipeline orchestration</strong>—only tracking of individual runs.</p><hr><h3 id=the-division-of-labor-in-this-project>The Division of Labor in This Project<a hidden class=anchor aria-hidden=true href=#the-division-of-labor-in-this-project>#</a></h3><table><thead><tr><th>Responsibility</th><th>W&amp;B</th><th>MLflow</th><th>Reason</th></tr></thead><tbody><tr><td><strong>Bayesian hyperparameter optimization</strong></td><td>✓</td><td>✗</td><td>W&amp;B has intelligent sweep, MLflow only Grid/Random</td></tr><tr><td><strong>Real-time dashboards</strong></td><td>✓</td><td>✗</td><td>W&amp;B UI is interactive, MLflow UI is static</td></tr><tr><td><strong>Parallel coordinates plots</strong></td><td>✓</td><td>✗</td><td>W&amp;B has advanced visualizations</td></tr><tr><td><strong>Early termination (Hyperband)</strong></td><td>✓</td><td>✗</td><td>W&amp;B implements Hyperband/ASHA/Median stopping</td></tr><tr><td><strong>Model Registry with stages</strong></td><td>✗</td><td>✓</td><td>MLflow has Staging/Production, W&amp;B doesn&rsquo;t</td></tr><tr><td><strong>Model-as-code API</strong></td><td>✗</td><td>✓</td><td><code>mlflow.pyfunc.load_model()</code> is the standard</td></tr><tr><td><strong>Model rollback</strong></td><td>✗</td><td>✓</td><td>MLflow transition, W&amp;B has no stage concept</td></tr><tr><td><strong>Pipeline orchestration</strong></td><td>✗</td><td>✓</td><td><code>mlflow.run()</code> executes nested steps</td></tr><tr><td><strong>Artifact storage (physical)</strong></td><td>✗</td><td>✗</td><td>Both point to GCS, don&rsquo;t duplicate storage</td></tr><tr><td><strong>Asynchronous logging</strong></td><td>✓</td><td>✗</td><td>W&amp;B doesn&rsquo;t block training, MLflow does</td></tr><tr><td><strong>Searchable metadata</strong></td><td>✓</td><td>✓</td><td>Both allow tags/search, different implementations</td></tr></tbody></table><hr><h3 id=the-complete-flow-wb--mlflow>The Complete Flow: W&amp;B → MLflow<a hidden class=anchor aria-hidden=true href=#the-complete-flow-wb--mlflow>#</a></h3><p><strong>Day 1-3: Experimentation (W&amp;B)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Run 50-run sweep</span>
</span></span><span class=line><span class=cl>make run-sweep
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># W&amp;B dashboard shows:</span>
</span></span><span class=line><span class=cl><span class=c1># - 50 runs in table</span>
</span></span><span class=line><span class=cl><span class=c1># - Parallel coordinates plot</span>
</span></span><span class=line><span class=cl><span class=c1># - Best run: n_estimators=200, max_depth=20, wMAPE=7.8%</span>
</span></span><span class=line><span class=cl><span class=c1># - Sweep ID: abc123xyz</span>
</span></span></code></pre></div><p><strong>Output:</strong> <code>src/model/06_sweep/best_params.yaml</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>hyperparameters</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>n_estimators</span><span class=p>:</span><span class=w> </span><span class=m>200</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_depth</span><span class=p>:</span><span class=w> </span><span class=m>20</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>min_samples_split</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>min_samples_leaf</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mape</span><span class=p>:</span><span class=w> </span><span class=m>7.82</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>wmape</span><span class=p>:</span><span class=w> </span><span class=m>7.76</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>r2</span><span class=p>:</span><span class=w> </span><span class=m>0.8654</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>sweep_id</span><span class=p>:</span><span class=w> </span><span class=l>abc123xyz </span><span class=w> </span><span class=c># Link to W&amp;B</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>best_run_id</span><span class=p>:</span><span class=w> </span><span class=l>def456ghi</span><span class=w>
</span></span></span></code></pre></div><p><strong>Day 4: Registration (MLflow)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Step 07 reads best_params.yaml</span>
</span></span><span class=line><span class=cl>python main.py main.execute_steps<span class=o>=[</span>07_registration<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MLflow:</span>
</span></span><span class=line><span class=cl><span class=c1># 1. Trains model with best_params</span>
</span></span><span class=line><span class=cl><span class=c1># 2. Registers as housing_price_model/v3</span>
</span></span><span class=line><span class=cl><span class=c1># 3. Transitions to Staging</span>
</span></span><span class=line><span class=cl><span class=c1># 4. Saves metadata (including sweep_id)</span>
</span></span></code></pre></div><p><strong>Day 5-7: Staging Validation</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># API runs with model in Staging</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Staging <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run tests, validate metrics, review predictions</span>
</span></span></code></pre></div><p><strong>Day 8: Promotion to Production</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mlflow models transition <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stage Production
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Production API auto-reloads v3</span>
</span></span><span class=line><span class=cl><span class=c1># v2 becomes fallback (stage: Archived)</span>
</span></span></code></pre></div><p><strong>If something fails:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Rollback in 10 seconds</span>
</span></span><span class=line><span class=cl>mlflow models transition <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stage Production
</span></span></code></pre></div><hr><h3 id=why-both-definitely>Why Both, Definitely<a hidden class=anchor aria-hidden=true href=#why-both-definitely>#</a></h3><p><strong>Question:</strong> &ldquo;Can I use only W&amp;B?&rdquo;</p><p><strong>Answer:</strong> You can, but you lose:</p><ul><li>Model Registry (versioning, stages, rollback)</li><li>Standard API for loading models in production</li><li>Pipeline orchestration with hierarchical runs</li></ul><p><strong>Result:</strong> You end up building your own model versioning system with custom scripts—reinventing the wheel badly.</p><p><strong>Question:</strong> &ldquo;Can I use only MLflow?&rdquo;</p><p><strong>Answer:</strong> You can, but you lose:</p><ul><li>Bayesian optimization (you&rsquo;ll have to do slow Grid Search)</li><li>Interactive visualizations (parallel coordinates, real-time dashboards)</li><li>Intelligent early termination (you waste compute)</li></ul><p><strong>Result:</strong> Your sweeps take 3x longer, and you have no visual feedback on what works.</p><hr><h3 id=the-real-cost>The Real Cost<a hidden class=anchor aria-hidden=true href=#the-real-cost>#</a></h3><p><strong>W&amp;B:</strong></p><ul><li>Free tier: 100GB storage, unlimited collaborators</li><li>Team tier: $50/user/month (for teams >5 people)</li></ul><p><strong>MLflow:</strong></p><ul><li>Open source, free</li><li>Cost: Hosting the tracking server (Cloud Run: ~$20/month for moderate use)</li><li>Storage: GCS (you already pay for data)</li></ul><p><strong>Total for team of 5:</strong> ~$20-50/month (if using W&amp;B free tier) or ~$270/month (if using W&amp;B Team).</p><p><strong>ROI:</strong> If a more efficient sweep saves 30 minutes of compute/day:</p><ul><li>Compute saved: ~15 hours/month</li><li>In GCP: 15 hours × $2/hour (GPU) = $30/month saved in compute alone</li><li>Plus engineer time (more valuable)</li></ul><p><strong>Breakeven in &lt;1 month.</strong></p><hr><h3 id=the-lesson-for-mlops-engineers>The Lesson For MLOps Engineers<a hidden class=anchor aria-hidden=true href=#the-lesson-for-mlops-engineers>#</a></h3><p><strong>Don&rsquo;t choose tools by hype or popularity.</strong> Choose by <strong>clear responsibilities</strong>:</p><ol><li><strong>Fast, interactive experimentation:</strong> W&amp;B, Neptune, Comet</li><li><strong>Governance and deployment:</strong> MLflow, Seldon, BentoML</li><li><strong>Artifact storage:</strong> GCS, S3, Azure Blob (not tracking tools)</li></ol><p><strong>This project uses:</strong></p><ul><li><strong>W&amp;B:</strong> Because it needs efficient Bayesian sweep</li><li><strong>MLflow:</strong> Because it needs production-ready Model Registry</li><li><strong>GCS:</strong> Because it needs high-availability storage</li></ul><p><strong>There&rsquo;s no redundancy—there&rsquo;s specialization.</strong></p><p>When you understand this, you stop asking &ldquo;W&amp;B or MLflow?&rdquo; and start asking &ldquo;what problem am I solving?&rdquo;</p><p><strong>That&rsquo;s the difference between using tools and building systems.</strong></p><hr><p><a name=docker-mlflow></a></p><h2 id=10-docker-and-mlflow-containerizing-the-complete-ecosystem>10. Docker and MLflow: Containerizing the Complete Ecosystem<a hidden class=anchor aria-hidden=true href=#10-docker-and-mlflow-containerizing-the-complete-ecosystem>#</a></h2><h3 id=the-three-container-architecture>The Three-Container Architecture<a hidden class=anchor aria-hidden=true href=#the-three-container-architecture>#</a></h3><p>This project uses <strong>three distinct Dockerfiles</strong>, each optimized for its specific purpose:</p><ol><li><strong>Pipeline Container (<code>Dockerfile</code>)</strong>: Runs the complete training pipeline with MLflow tracking</li><li><strong>API Container (<code>api/Dockerfile</code>)</strong>: Serves predictions with FastAPI in production</li><li><strong>Streamlit Container (<code>streamlit_app/Dockerfile</code>)</strong>: Provides interactive web interface</li></ol><h2>This separation is not accidental—it&rsquo;s an architectural decision that reflects the different requirements of each component.
<img loading=lazy src=img/app1.png></h2><h3 id=1-pipeline-container-training-with-mlflow-tracking>1. Pipeline Container: Training with MLflow Tracking<a hidden class=anchor aria-hidden=true href=#1-pipeline-container-training-with-mlflow-tracking>#</a></h3><h4 id=pipeline-dockerfile>Pipeline Dockerfile<a hidden class=anchor aria-hidden=true href=#pipeline-dockerfile>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Dockerfile for MLOps Pipeline Execution</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Purpose: Run the complete training pipeline in containerized environment</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> python:3.12-slim</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>maintainer</span><span class=o>=</span><span class=s2>&#34;danieljimenez88m@gmail.com&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>description</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction - MLOps Pipeline&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set working directory</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install system dependencies</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    gcc <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    g++ <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    git <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy requirements first for better caching</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> pyproject.toml ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt* ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install Python dependencies</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir --upgrade pip <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    pip install --no-cache-dir uv <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    uv pip install --system -e .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy application code</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> . .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set environment variables</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># Create necessary directories</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p mlruns outputs models<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Default command runs the pipeline</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;python&#34;</span><span class=p>,</span> <span class=s2>&#34;main.py&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><h4 id=critical-technical-decisions>Critical Technical Decisions<a hidden class=anchor aria-hidden=true href=#critical-technical-decisions>#</a></h4><p><strong>1. Why <code>gcc</code> and <code>g++</code></strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> apt-get install -y gcc g++ git curl<span class=err>
</span></span></span></code></pre></div><p>Many ML packages (numpy, scipy, scikit-learn) compile C/C++ extensions during installation. Without these compilers, <code>pip install</code> fails with cryptic errors like &ldquo;error: command &lsquo;gcc&rsquo; failed&rdquo;.</p><p><strong>Trade-off:</strong> Larger image (~500MB vs ~150MB of pure Python slim), but guarantees all dependencies install correctly.</p><p><strong>2. Layer Caching Strategy</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># Copy requirements first for better caching</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> pyproject.toml ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt* ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install ...<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy application code AFTER</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> . .<span class=err>
</span></span></span></code></pre></div><p>Docker caches layers. If you change Python code but not dependencies, Docker reuses the <code>pip install</code> layer (takes 5 minutes) and only recopies the code (10 seconds).</p><p><strong>Without this optimization:</strong> Every code change requires reinstalling all dependencies.</p><p><strong>3. Directory Creation for MLflow</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> mkdir -p mlruns outputs models<span class=err>
</span></span></span></code></pre></div><p>MLflow writes artifacts to <code>mlruns/</code> by default if a remote tracking server isn&rsquo;t configured. If this directory doesn&rsquo;t exist with correct permissions, MLflow fails silently.</p><p><strong><code>outputs/</code></strong>: For plots and intermediate analysis
<strong><code>models/</code></strong>: For model checkpoints before uploading to GCS</p><h4 id=how-to-enable-mlflow-tracking>How to Enable MLflow Tracking<a hidden class=anchor aria-hidden=true href=#how-to-enable-mlflow-tracking>#</a></h4><p><strong>Option 1: Local MLflow (Default)</strong></p><p>When you run the pipeline in this container, MLflow writes to <code>mlruns/</code> inside the container:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run --env-file .env housing-pipeline:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MLflow writes to /app/mlruns/</span>
</span></span><span class=line><span class=cl><span class=c1># To see the UI:</span>
</span></span><span class=line><span class=cl>docker <span class=nb>exec</span> -it &lt;container-id&gt; mlflow ui --host 0.0.0.0 --port <span class=m>5000</span>
</span></span></code></pre></div><p><strong>Limitation:</strong> Runs are lost when the container stops.</p><p><strong>Option 2: MLflow Remote Tracking Server</strong></p><p>To persist runs, configure a separate MLflow server:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># docker-compose.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mlflow</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>ghcr.io/mlflow/mlflow:v2.9.2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>mlflow-server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;5000:5000&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>BACKEND_STORE_URI=sqlite:///mlflow.db</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>DEFAULT_ARTIFACT_ROOT=gs://your-bucket/mlflow-artifacts</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlflow-data:/mlflow</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>&gt;</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      mlflow server
</span></span></span><span class=line><span class=cl><span class=sd>      --backend-store-uri sqlite:///mlflow/mlflow.db
</span></span></span><span class=line><span class=cl><span class=sd>      --default-artifact-root gs://your-bucket/mlflow-artifacts
</span></span></span><span class=line><span class=cl><span class=sd>      --host 0.0.0.0
</span></span></span><span class=line><span class=cl><span class=sd>      --port 5000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>pipeline</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w> </span><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>MLFLOW_TRACKING_URI=http://mlflow:5000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>GCP_PROJECT_ID=${GCP_PROJECT_ID}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>GCS_BUCKET_NAME=${GCS_BUCKET_NAME}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>WANDB_API_KEY=${WANDB_API_KEY}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlflow</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mlflow-data</span><span class=p>:</span><span class=w>
</span></span></span></code></pre></div><p><strong>Configuration in code:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># main.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>mlflow</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># If MLFLOW_TRACKING_URI is configured, use that server</span>
</span></span><span class=line><span class=cl><span class=n>mlflow_uri</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;MLFLOW_TRACKING_URI&#34;</span><span class=p>,</span> <span class=s2>&#34;file:./mlruns&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mlflow</span><span class=o>.</span><span class=n>set_tracking_uri</span><span class=p>(</span><span class=n>mlflow_uri</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mlflow</span><span class=o>.</span><span class=n>set_experiment</span><span class=p>(</span><span class=s2>&#34;housing_price_prediction&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1># Log params, metrics, artifacts</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_param</span><span class=p>(</span><span class=s2>&#34;n_estimators&#34;</span><span class=p>,</span> <span class=mi>200</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metric</span><span class=p>(</span><span class=s2>&#34;mape&#34;</span><span class=p>,</span> <span class=mf>7.82</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Option 3: MLflow in Cloud (Production)</strong></p><p>For production, use a managed MLflow server:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Deploy MLflow to Cloud Run (serverless)</span>
</span></span><span class=line><span class=cl>gcloud run deploy mlflow-server <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --image ghcr.io/mlflow/mlflow:v2.9.2 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --platform managed <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --region us-central1 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set-env-vars<span class=o>=</span><span class=s2>&#34;BACKEND_STORE_URI=postgresql://user:pass@host/mlflow_db,DEFAULT_ARTIFACT_ROOT=gs://bucket/mlflow&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --allow-unauthenticated
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get URL</span>
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_URL</span><span class=o>=</span><span class=k>$(</span>gcloud run services describe mlflow-server --format <span class=s1>&#39;value(status.url)&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Configure in pipeline</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MLFLOW_TRACKING_URI</span><span class=o>=</span><span class=nv>$MLFLOW_URL</span>
</span></span></code></pre></div><h4 id=pipeline-container-execution>Pipeline Container Execution<a hidden class=anchor aria-hidden=true href=#pipeline-container-execution>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Build</span>
</span></span><span class=line><span class=cl>docker build -t housing-pipeline:latest .
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run with env vars</span>
</span></span><span class=line><span class=cl>docker run <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --env-file .env <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/mlruns:/app/mlruns <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-pipeline:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run with specific steps</span>
</span></span><span class=line><span class=cl>docker run <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --env-file .env <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-pipeline:latest <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  python main.py main.execute_steps<span class=o>=[</span>03_feature_engineering,05_model_selection<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># See logs in real-time</span>
</span></span><span class=line><span class=cl>docker logs -f &lt;container-id&gt;
</span></span></code></pre></div><p><strong>Volume Mount (<code>-v</code>)</strong>: Mounts <code>mlruns/</code> from the host to the container to persist MLflow runs even after the container stops.</p><hr><h3 id=2-api-container-production-inference>2. API Container: Production Inference<a hidden class=anchor aria-hidden=true href=#2-api-container-production-inference>#</a></h3><h4 id=api-dockerfile>API Dockerfile<a hidden class=anchor aria-hidden=true href=#api-dockerfile>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Dockerfile for Housing Price Prediction API</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Purpose: Production-ready FastAPI service for Cloud Run deployment</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> python:3.12-slim</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>maintainer</span><span class=o>=</span><span class=s2>&#34;danieljimenez88m@gmail.com&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>description</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction API - FastAPI Service&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install system dependencies (only curl for healthcheck)</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy requirements first for better caching</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install Python dependencies</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir --upgrade pip <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    pip install --no-cache-dir -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy application code</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> app/ ./app/<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Create models directory</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p models<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set environment variables</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PORT</span><span class=o>=</span><span class=m>8080</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># Expose port</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>EXPOSE</span><span class=s> 8080</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Health check</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>HEALTHCHECK --interval=30s --timeout=10s </span>--start-period<span class=o>=</span>40s --retries<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    CMD curl -f http://localhost:8080/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Run the application</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=nb>exec</span> uvicorn app.main:app --host 0.0.0.0 --port <span class=si>${</span><span class=nv>PORT</span><span class=si>}</span><span class=err>
</span></span></span></code></pre></div><h4 id=critical-technical-decisions-1>Critical Technical Decisions<a hidden class=anchor aria-hidden=true href=#critical-technical-decisions-1>#</a></h4><p><strong>1. Lighter Image</strong></p><p>Compared to the pipeline container:</p><ul><li><strong>Doesn&rsquo;t need <code>gcc</code>/<code>g++</code></strong>: Dependencies are already compiled in wheels</li><li><strong>Doesn&rsquo;t need <code>git</code></strong>: Doesn&rsquo;t clone repos</li><li><strong>Only <code>curl</code></strong>: For the healthcheck</li></ul><p><strong>Result:</strong> Image of ~200MB vs ~500MB for the pipeline.</p><p><strong>Why it matters:</strong> Cloud Run charges for memory usage. A smaller image = less memory = less cost.</p><p><strong>2. Native Health Check</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=err>HEALTHCHECK --interval=30s --timeout=10s </span>--start-period<span class=o>=</span>40s --retries<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    CMD curl -f http://localhost:8080/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span></code></pre></div><p>Docker marks the container as &ldquo;unhealthy&rdquo; if the <code>/health</code> endpoint fails 3 consecutive times.</p><p><strong>Cloud Run</strong> and <strong>Kubernetes</strong> use this to:</p><ul><li>Not send traffic to unhealthy containers</li><li>Restart failing containers</li><li>Uptime reporting</li></ul><p><strong>start-period=40s</strong>: Gives the API 40 seconds to load the model before starting health checks.</p><p><strong>3. Flexible Port Configuration</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PORT</span><span class=o>=</span><span class=m>8080</span>
</span></span><span class=line><span class=cl><span class=k>CMD</span> <span class=nb>exec</span> uvicorn app.main:app --host 0.0.0.0 --port <span class=si>${</span><span class=nv>PORT</span><span class=si>}</span><span class=err>
</span></span></span></code></pre></div><p>Cloud Run injects <code>PORT</code> as an env var (can be 8080, 8081, etc.). The API must read this value, not hardcode it.</p><p><strong><code>exec</code></strong>: Replaces the shell process with uvicorn, allowing Docker to send signals (SIGTERM) directly to uvicorn for graceful shutdown.</p><h4 id=how-the-api-loads-the-model>How the API Loads the Model<a hidden class=anchor aria-hidden=true href=#how-the-api-loads-the-model>#</a></h4><p>The API has <strong>three model loading strategies</strong> with automatic fallback:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/model_loader.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ModelLoader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Load model from MLflow → GCS → Local with fallback.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>load_model</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Any</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Priority: MLflow &gt; GCS &gt; Local&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Strategy 1: From MLflow Registry</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>model_uri</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;models:/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_stage</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>pyfunc</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=n>model_uri</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loaded from MLflow: </span><span class=si>{</span><span class=n>model_uri</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MLflow load failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>, trying GCS...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Strategy 2: From GCS</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>storage_client</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>bucket</span> <span class=o>=</span> <span class=n>storage_client</span><span class=o>.</span><span class=n>bucket</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>blob</span> <span class=o>=</span> <span class=n>bucket</span><span class=o>.</span><span class=n>blob</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>model_bytes</span> <span class=o>=</span> <span class=n>blob</span><span class=o>.</span><span class=n>download_as_bytes</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>model_bytes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loaded from GCS: gs://</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GCS load failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>, trying local...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Strategy 3: From local file (fallback)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=ow>and</span> <span class=n>Path</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span><span class=o>.</span><span class=n>exists</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>,</span> <span class=s1>&#39;rb&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loaded from local: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;No model could be loaded from any source&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Configuration with env vars:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Production: Load from MLflow</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_TRACKING_URI</span><span class=o>=</span>https://mlflow.example.com <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Production <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Staging: Load from GCS</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>GCS_BUCKET</span><span class=o>=</span>my-bucket <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>GCS_MODEL_PATH</span><span class=o>=</span>models/trained/housing_price_model.pkl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Development: Load from local</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/models:/app/models <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>LOCAL_MODEL_PATH</span><span class=o>=</span>/app/models/trained/housing_price_model.pkl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span></code></pre></div><hr><h3 id=3-streamlit-container-interactive-frontend>3. Streamlit Container: Interactive Frontend<a hidden class=anchor aria-hidden=true href=#3-streamlit-container-interactive-frontend>#</a></h3><h4 id=streamlit-dockerfile>Streamlit Dockerfile<a hidden class=anchor aria-hidden=true href=#streamlit-dockerfile>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Dockerfile for Streamlit Frontend</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Purpose: Interactive web interface for housing price predictions</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> python:3.12-slim</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>maintainer</span><span class=o>=</span><span class=s2>&#34;danieljimenez88m@gmail.com&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>description</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction - Streamlit Frontend&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y curl <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir --upgrade pip <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    pip install --no-cache-dir -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> app.py .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Create .streamlit directory for config</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p .streamlit<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Streamlit configuration</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> <span class=nb>echo</span> <span class=s1>&#39;\
</span></span></span><span class=line><span class=cl><span class=s1>[server]\n\
</span></span></span><span class=line><span class=cl><span class=s1>port = 8501\n\
</span></span></span><span class=line><span class=cl><span class=s1>address = &#34;0.0.0.0&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>headless = true\n\
</span></span></span><span class=line><span class=cl><span class=s1>enableCORS = false\n\
</span></span></span><span class=line><span class=cl><span class=s1>enableXsrfProtection = true\n\
</span></span></span><span class=line><span class=cl><span class=s1>\n\
</span></span></span><span class=line><span class=cl><span class=s1>[browser]\n\
</span></span></span><span class=line><span class=cl><span class=s1>gatherUsageStats = false\n\
</span></span></span><span class=line><span class=cl><span class=s1>\n\
</span></span></span><span class=line><span class=cl><span class=s1>[theme]\n\
</span></span></span><span class=line><span class=cl><span class=s1>primaryColor = &#34;#FF4B4B&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>backgroundColor = &#34;#FFFFFF&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>secondaryBackgroundColor = &#34;#F0F2F6&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>textColor = &#34;#262730&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>font = &#34;sans serif&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>&#39;</span> &gt; .streamlit/config.toml<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>EXPOSE</span><span class=s> 8501</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>HEALTHCHECK --interval=30s --timeout=10s </span>--start-period<span class=o>=</span>40s --retries<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    CMD curl -f http://localhost:8501/_stcore/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;streamlit&#34;</span><span class=p>,</span> <span class=s2>&#34;run&#34;</span><span class=p>,</span> <span class=s2>&#34;app.py&#34;</span><span class=p>,</span> <span class=s2>&#34;--server.port=8501&#34;</span><span class=p>,</span> <span class=s2>&#34;--server.address=0.0.0.0&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><h4 id=critical-technical-decisions-2>Critical Technical Decisions<a hidden class=anchor aria-hidden=true href=#critical-technical-decisions-2>#</a></h4><p><strong>1. Embedded Configuration</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> <span class=nb>echo</span> <span class=s1>&#39;...&#39;</span> &gt; .streamlit/config.toml<span class=err>
</span></span></span></code></pre></div><p>Streamlit requires configuration to run in containers (headless mode, CORS, etc.). Instead of committing a <code>config.toml</code> file to the repo, we generate it at build time.</p><p><strong>Advantages:</strong></p><ul><li>One less file in the repo</li><li>Configuration versioned with the Dockerfile</li><li>No risk of forgetting to commit the config</li></ul><p><strong>2. Streamlit Health Check</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=err>HEALTHCHECK</span> CMD curl -f http://localhost:8501/_stcore/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span></code></pre></div><p>Streamlit automatically exposes <code>/_stcore/health</code>. This endpoint returns 200 if the app is running.</p><p><strong>3. Custom Theme</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-toml data-lang=toml><span class=line><span class=cl><span class=p>[</span><span class=nx>theme</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nx>primaryColor</span> <span class=p>=</span> <span class=s2>&#34;#FF4B4B&#34;</span>
</span></span><span class=line><span class=cl><span class=nx>backgroundColor</span> <span class=p>=</span> <span class=s2>&#34;#FFFFFF&#34;</span>
</span></span><span class=line><span class=cl><span class=nx>secondaryBackgroundColor</span> <span class=p>=</span> <span class=s2>&#34;#F0F2F6&#34;</span>
</span></span><span class=line><span class=cl><span class=nx>textColor</span> <span class=p>=</span> <span class=s2>&#34;#262730&#34;</span>
</span></span></code></pre></div><p>The theme defines button colors, backgrounds, etc. This gives visual consistency without needing custom CSS in each component.</p><h4 id=how-streamlit-connects-to-the-api>How Streamlit Connects to the API<a hidden class=anchor aria-hidden=true href=#how-streamlit-connects-to-the-api>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># streamlit_app/app.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>streamlit</span> <span class=k>as</span> <span class=nn>st</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Read API URL from environment variable</span>
</span></span><span class=line><span class=cl><span class=n>API_URL</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;API_URL&#34;</span><span class=p>,</span> <span class=s2>&#34;http://localhost:8080&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>API_PREDICT_ENDPOINT</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>API_URL</span><span class=si>}</span><span class=s2>/api/v1/predict&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>make_prediction</span><span class=p>(</span><span class=n>features</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Call API to get prediction.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>payload</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;instances&#34;</span><span class=p>:</span> <span class=p>[</span><span class=n>features</span><span class=p>]}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>API_PREDICT_ENDPOINT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>json</span><span class=o>=</span><span class=n>payload</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>timeout</span><span class=o>=</span><span class=mi>10</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span><span class=o>.</span><span class=n>raise_for_status</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=n>requests</span><span class=o>.</span><span class=n>exceptions</span><span class=o>.</span><span class=n>RequestException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>st</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;API Error: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Streamlit UI</span>
</span></span><span class=line><span class=cl><span class=n>st</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Housing Price Prediction&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>st</span><span class=o>.</span><span class=n>form</span><span class=p>(</span><span class=s2>&#34;prediction_form&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>longitude</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>number_input</span><span class=p>(</span><span class=s2>&#34;Longitude&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=-</span><span class=mf>122.23</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>latitude</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>number_input</span><span class=p>(</span><span class=s2>&#34;Latitude&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=mf>37.88</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... more inputs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>submitted</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>form_submit_button</span><span class=p>(</span><span class=s2>&#34;Predict&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>submitted</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>features</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;longitude&#34;</span><span class=p>:</span> <span class=n>longitude</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;latitude&#34;</span><span class=p>:</span> <span class=n>latitude</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=n>make_prediction</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>result</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>prediction</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s2>&#34;predictions&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;median_house_value&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>st</span><span class=o>.</span><span class=n>success</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Predicted Price: $</span><span class=si>{</span><span class=n>prediction</span><span class=si>:</span><span class=s2>,.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>API URL configuration:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Docker Compose: Use service name</span>
</span></span><span class=line><span class=cl>docker-compose up
</span></span><span class=line><span class=cl><span class=c1># Streamlit automatically receives API_URL=http://api:8080</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Local development: Use localhost</span>
</span></span><span class=line><span class=cl><span class=nv>API_URL</span><span class=o>=</span>http://localhost:8080 streamlit run app.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Production: Use Cloud Run URL</span>
</span></span><span class=line><span class=cl><span class=nv>API_URL</span><span class=o>=</span>https://housing-api-xyz.run.app streamlit run app.py
</span></span></code></pre></div><hr><h3 id=docker-compose-orchestrating-the-three-containers>Docker Compose: Orchestrating the Three Containers<a hidden class=anchor aria-hidden=true href=#docker-compose-orchestrating-the-three-containers>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># docker-compose.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>api</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>./api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dockerfile</span><span class=p>:</span><span class=w> </span><span class=l>Dockerfile</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>housing-price-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;8080:8080&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>PORT=8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>LOCAL_MODEL_PATH=/app/models/trained/housing_price_model.pkl</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>WANDB_API_KEY=${WANDB_API_KEY}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>./models:/app/models:ro</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>healthcheck</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>test</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;curl&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-f&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;http://localhost:8080/health&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>interval</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=l>10s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>retries</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlops-network</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>streamlit</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>./streamlit_app</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dockerfile</span><span class=p>:</span><span class=w> </span><span class=l>Dockerfile</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>housing-streamlit</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;8501:8501&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>API_URL=http://api:8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>healthcheck</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>test</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;curl&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-f&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;http://localhost:8501/_stcore/health&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>interval</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=l>10s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>retries</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlops-network</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mlops-network</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>bridge</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>housing-mlops-network</span><span class=w>
</span></span></span></code></pre></div><p><strong>Critical Decisions:</strong></p><p><strong>1. Network Isolation</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>mlops-network</span><span class=w>
</span></span></span></code></pre></div><p>Both containers are in the same Docker network, allowing Streamlit to call the API using <code>http://api:8080</code> (service name as hostname).</p><p><strong>Without this:</strong> You&rsquo;d have to use <code>http://host.docker.internal:8080</code> (only works on Docker Desktop) or the host IP.</p><p><strong>2. Read-Only Volume Mount</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>./models:/app/models:ro</span><span class=w>
</span></span></span></code></pre></div><p>The API mounts <code>models/</code> in <strong>read-only mode (<code>:ro</code>)</strong>. The container can read the model but not modify it.</p><p><strong>Why:</strong> Security. If the container is compromised, an attacker can&rsquo;t overwrite the model with a malicious one.</p><p><strong>3. Dependency Order</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>api</span><span class=w>
</span></span></span></code></pre></div><p>Docker Compose starts the API before Streamlit. This prevents Streamlit from failing when trying to connect to an API that&rsquo;s not yet running.</p><p><strong>Limitation:</strong> <code>depends_on</code> only waits for the container to <strong>start</strong>, not for the API to be <strong>ready</strong> (healthcheck pass). For that, you need an init container or retry logic in Streamlit.</p><hr><h3 id=complete-execution-commands>Complete Execution Commands<a hidden class=anchor aria-hidden=true href=#complete-execution-commands>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 1. Build all images</span>
</span></span><span class=line><span class=cl>docker-compose build
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. Train the model (pipeline container)</span>
</span></span><span class=line><span class=cl>docker run --env-file .env -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/models:/app/models housing-pipeline:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. Start API + Streamlit</span>
</span></span><span class=line><span class=cl>docker-compose up -d
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. Verify health</span>
</span></span><span class=line><span class=cl>curl http://localhost:8080/health
</span></span><span class=line><span class=cl>curl http://localhost:8501/_stcore/health
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. See logs</span>
</span></span><span class=line><span class=cl>docker-compose logs -f
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 6. Stop everything</span>
</span></span><span class=line><span class=cl>docker-compose down
</span></span></code></pre></div><hr><h3 id=what-this-architecture-solves>What This Architecture Solves<a hidden class=anchor aria-hidden=true href=#what-this-architecture-solves>#</a></h3><p><strong>Without containers:</strong></p><ul><li>&ldquo;Works on my machine&rdquo; syndrome</li><li>Conflicting dependencies (Python 3.9 vs 3.12)</li><li>Manual setup in every environment (dev, staging, prod)</li></ul><p><strong>With this architecture:</strong></p><ul><li><strong>Reproducibility:</strong> Same container runs on laptop, CI/CD, and production</li><li><strong>Isolation:</strong> API doesn&rsquo;t interfere with Streamlit, pipeline doesn&rsquo;t interfere with API</li><li><strong>Deployment:</strong> <code>docker push</code> → <code>gcloud run deploy</code> in &lt;5 minutes</li><li><strong>Rollback:</strong> <code>docker pull previous-image</code> → restart</li><li><strong>Observability:</strong> Automatic health checks, centralized logs</li></ul><p><strong>The real value:</strong> A data scientist without DevOps experience can deploy to production without knowing how to configure nginx, systemd, or virtual environments. Docker abstracts all that complexity.</p><hr><p><a name=api-architecture></a></p><h2 id=105-api-architecture-fastapi-in-production>10.5. API Architecture: FastAPI in Production<a hidden class=anchor aria-hidden=true href=#105-api-architecture-fastapi-in-production>#</a></h2><h3 id=why-this-section-matters>Why This Section Matters<a hidden class=anchor aria-hidden=true href=#why-this-section-matters>#</a></h3><p>You&rsquo;ve seen training pipelines, hyperparameter sweeps, and model registry. But <strong>90% of the time, your model isn&rsquo;t training—it&rsquo;s serving predictions in production.</strong></p><p>A poorly designed API is the bottleneck between an excellent model and a useful product. This section breaks down how this project builds a production-ready API, not a tutorial prototype.</p><hr><h3 id=the-general-architecture>The General Architecture<a hidden class=anchor aria-hidden=true href=#the-general-architecture>#</a></h3><pre tabindex=0><code>api/
├── app/
│   ├── main.py                    # FastAPI app + lifespan management
│   ├── core/
│   │   ├── config.py              # Pydantic Settings (env vars)
│   │   ├── model_loader.py        # Multi-source model loading
│   │   ├── wandb_logger.py        # Prediction logging
│   │   └── preprocessor.py        # Feature engineering
│   ├── routers/
│   │   └── predict.py             # Prediction endpoints
│   └── models/
│       └── schemas.py             # Pydantic request/response models
├── requirements.txt
├── Dockerfile
└── tests/
</code></pre><p><strong>Architectural decision:</strong> Separation of concerns by layers:</p><ol><li><strong>Core</strong>: Business logic (load model, logging, config)</li><li><strong>Routers</strong>: HTTP endpoints (routes, request validation)</li><li><strong>Models</strong>: Data schemas (Pydantic)</li></ol><p><strong>Why not everything in <code>main.py</code>?</strong> Because when the API grows (adding authentication, rate limiting, multiple models), each layer extends independently without touching the rest.</p><hr><h3 id=1-lifespan-management-the-pattern-that-avoids-first-request-latency>1. Lifespan Management: The Pattern That Avoids First-Request Latency<a hidden class=anchor aria-hidden=true href=#1-lifespan-management-the-pattern-that-avoids-first-request-latency>#</a></h3><h4 id=the-problem-it-solves>The Problem It Solves<a hidden class=anchor aria-hidden=true href=#the-problem-it-solves>#</a></h4><p><strong>Common anti-pattern:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># BAD: Load model on every request</span>
</span></span><span class=line><span class=cl><span class=nd>@app.post</span><span class=p>(</span><span class=s2>&#34;/predict&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>features</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=nb>open</span><span class=p>(</span><span class=s2>&#34;model.pkl&#34;</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>))</span>  <span class=c1># 5 seconds every request</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Problems:</strong></p><ul><li>First request takes 5 seconds (load model)</li><li>Every subsequent request also (no caching)</li><li>If 10 concurrent requests → 10 model loads (50 seconds total)</li></ul><h4 id=the-solution-asynccontextmanager>The Solution: asynccontextmanager<a hidden class=anchor aria-hidden=true href=#the-solution-asynccontextmanager>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@asynccontextmanager</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>lifespan</span><span class=p>(</span><span class=n>app</span><span class=p>:</span> <span class=n>FastAPI</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Lifecycle manager for the FastAPI application.
</span></span></span><span class=line><span class=cl><span class=s2>    Loads the model on startup and cleans up on shutdown.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Starting up API...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># STARTUP: Load model ONCE</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb_logger</span> <span class=o>=</span> <span class=n>WandBLogger</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>project</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>WANDB_PROJECT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>enabled</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_loader</span> <span class=o>=</span> <span class=n>ModelLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>local_model_path</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>LOCAL_MODEL_PATH</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_bucket</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>GCS_BUCKET</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_model_path</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>GCS_MODEL_PATH</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_name</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_NAME</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_stage</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_STAGE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_tracking_uri</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_TRACKING_URI</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Loading model...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>model_loader</span><span class=o>.</span><span class=n>load_model</span><span class=p>()</span>  <span class=c1># Takes 5 seconds, but ONLY once</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Model loaded: </span><span class=si>{</span><span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Save in app state (available to all endpoints)</span>
</span></span><span class=line><span class=cl>        <span class=n>app</span><span class=o>.</span><span class=n>state</span><span class=o>.</span><span class=n>model_loader</span> <span class=o>=</span> <span class=n>model_loader</span>
</span></span><span class=line><span class=cl>        <span class=n>app</span><span class=o>.</span><span class=n>state</span><span class=o>.</span><span class=n>wandb_logger</span> <span class=o>=</span> <span class=n>wandb_logger</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Failed to load model: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&#34;API will start but predictions will fail&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>yield</span>  <span class=c1># API runs here</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># SHUTDOWN: Cleanup</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Shutting down API...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb_logger</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Use lifespan in FastAPI</span>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>FastAPI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>title</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction API&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=o>=</span><span class=s2>&#34;1.0.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lifespan</span><span class=o>=</span><span class=n>lifespan</span>  <span class=c1># CRITICAL</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>What it does:</strong></p><ol><li><p><strong>Startup (before <code>yield</code>):</strong></p><ul><li>Load model into memory (5 seconds, <strong>only once</strong>)</li><li>Initialize W&amp;B logger</li><li>Save both in <code>app.state</code> (singleton pattern)</li></ul></li><li><p><strong>Running (after <code>yield</code>):</strong></p><ul><li>All requests use the cached model in <code>app.state.model_loader</code></li><li>Latency per request: &lt;50ms (only inference, no I/O)</li></ul></li><li><p><strong>Shutdown (after context manager):</strong></p><ul><li>Close W&amp;B run (flush pending logs)</li><li>Free resources</li></ul></li></ol><p><strong>Result:</strong></p><ul><li>First request: &lt;50ms (model already loaded)</li><li>Subsequent requests: &lt;50ms</li><li>10 concurrent requests: &lt;100ms average (parallelizable)</li></ul><p><strong>Trade-off:</strong> 5-10 second startup time. Acceptable for production—better than 5 seconds per request.</p><hr><h3 id=2-configuration-management-pydantic-settings-with-priorities>2. Configuration Management: Pydantic Settings with Priorities<a hidden class=anchor aria-hidden=true href=#2-configuration-management-pydantic-settings-with-priorities>#</a></h3><h4 id=the-pattern-settings-as-code>The Pattern: Settings-as-Code<a hidden class=anchor aria-hidden=true href=#the-pattern-settings-as-code>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/config.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pydantic_settings</span> <span class=kn>import</span> <span class=n>BaseSettings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Settings</span><span class=p>(</span><span class=n>BaseSettings</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>PROJECT_NAME</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;Housing Price Prediction API&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>VERSION</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;1.0.0&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>API_V1_STR</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;/api/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Model - MLflow (priority 1)</span>
</span></span><span class=line><span class=cl>    <span class=n>MLFLOW_MODEL_NAME</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>MLFLOW_MODEL_STAGE</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;Production&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>MLFLOW_TRACKING_URI</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Model - GCS (priority 2)</span>
</span></span><span class=line><span class=cl>    <span class=n>GCS_BUCKET</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>GCS_MODEL_PATH</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Model - Local (priority 3, fallback)</span>
</span></span><span class=line><span class=cl>    <span class=n>LOCAL_MODEL_PATH</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Weights &amp; Biases</span>
</span></span><span class=line><span class=cl>    <span class=n>WANDB_API_KEY</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>WANDB_PROJECT</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;housing-mlops-api&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Config</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>env_file</span> <span class=o>=</span> <span class=s2>&#34;.env&#34;</span>  <span class=c1># Read from .env automatically</span>
</span></span><span class=line><span class=cl>        <span class=n>case_sensitive</span> <span class=o>=</span> <span class=kc>True</span>  <span class=c1># MLFLOW_MODEL_NAME != mlflow_model_name</span>
</span></span></code></pre></div><p><strong>Why Pydantic Settings:</strong></p><ol><li><strong>Type Safety</strong>: <code>settings.VERSION</code> is <code>str</code>, not <code>Optional[Any]</code></li><li><strong>Validation</strong>: If <code>MLFLOW_MODEL_STAGE</code> is not a string, fails at startup (not on first request)</li><li><strong>Auto .env loading</strong>: You don&rsquo;t need <code>python-dotenv</code> manually</li><li><strong>Default values</strong>: <code>LOCAL_MODEL_PATH</code> has a default, <code>MLFLOW_MODEL_NAME</code> doesn&rsquo;t</li></ol><p><strong>Usage in code:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>app.core.config</span> <span class=kn>import</span> <span class=n>Settings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>settings</span> <span class=o>=</span> <span class=n>Settings</span><span class=p>()</span>  <span class=c1># Read env vars + .env</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_NAME</span><span class=p>:</span>  <span class=c1># Type-safe check</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>load_from_mlflow</span><span class=p>(</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_NAME</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=the-priority-strategy-cascade-fallback>The Priority Strategy (Cascade Fallback)<a hidden class=anchor aria-hidden=true href=#the-priority-strategy-cascade-fallback>#</a></h4><pre tabindex=0><code>Try to load from:
1. MLflow Registry (if MLFLOW_MODEL_NAME is configured)
   ↓ If fails
2. GCS (if GCS_BUCKET is configured)
   ↓ If fails
3. Local filesystem (always available as last resort)
   ↓ If fails
4. API starts but `/predict` returns 500
</code></pre><p><strong>Configuration by environment:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Production (.env.production)</span>
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Production
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_TRACKING_URI</span><span class=o>=</span>https://mlflow.company.com
</span></span><span class=line><span class=cl><span class=c1># GCS and Local stay empty → not used</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Staging (.env.staging)</span>
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Staging
</span></span><span class=line><span class=cl><span class=c1># Same setup, different stage</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Local development (.env.local)</span>
</span></span><span class=line><span class=cl><span class=nv>LOCAL_MODEL_PATH</span><span class=o>=</span>models/trained/housing_price_model.pkl
</span></span><span class=line><span class=cl><span class=c1># Without MLflow or GCS → load from local directly</span>
</span></span></code></pre></div><p><strong>Value:</strong> One codebase, multiple environments. No <code>if ENVIRONMENT == "production"</code> in the code.</p><hr><h3 id=3-model-loader-multi-source-with-intelligent-fallback>3. Model Loader: Multi-Source with Intelligent Fallback<a hidden class=anchor aria-hidden=true href=#3-model-loader-multi-source-with-intelligent-fallback>#</a></h3><h4 id=the-loader-architecture>The Loader Architecture<a hidden class=anchor aria-hidden=true href=#the-loader-architecture>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/model_loader.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ModelLoader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Handles loading ML models from various sources.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>local_model_path</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_bucket</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_model_path</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_name</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_stage</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_tracking_uri</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=o>=</span> <span class=n>local_model_path</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span> <span class=o>=</span> <span class=n>gcs_bucket</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span> <span class=o>=</span> <span class=n>gcs_model_path</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span> <span class=o>=</span> <span class=n>mlflow_model_name</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_stage</span> <span class=o>=</span> <span class=n>mlflow_model_stage</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_tracking_uri</span> <span class=o>=</span> <span class=n>mlflow_tracking_uri</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Any</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>  <span class=c1># Cached in memory</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_model_version</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;unknown&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span> <span class=o>=</span> <span class=n>HousingPreprocessor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>load_model</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Any</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Load model with cascade fallback strategy.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Priority 1: MLflow Registry</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attempting MLflow load: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_stage</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_mlflow</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_stage</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_tracking_uri</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MLflow load failed: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>, trying GCS...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Priority 2: GCS</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attempting GCS load: gs://</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_gcs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GCS load failed: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>, trying local...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Priority 3: Local filesystem</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=ow>and</span> <span class=n>Path</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span><span class=o>.</span><span class=n>exists</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attempting local load: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_local</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># All strategies failed</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Could not load model from any source. &#34;</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Check MLflow/GCS/local configuration.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Make predictions with preprocessing.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_loaded</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;Model not loaded&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Apply same preprocessing as the training pipeline</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Predict</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>predictions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_loaded</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Check if model is loaded.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
</span></span></code></pre></div><h4 id=critical-technical-decisions-3>Critical Technical Decisions<a hidden class=anchor aria-hidden=true href=#critical-technical-decisions-3>#</a></h4><p><strong>1. Why MLflow Is Priority 1</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># MLflow load</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Advantages over GCS/Local:</strong></p><ul><li><strong>Model URI abstracts storage</strong>: The model can be in S3, GCS, HDFS—MLflow resolves it</li><li><strong>Stage resolution</strong>: <code>Production</code> automatically resolves to the correct version (v1, v2, etc.)</li><li><strong>Metadata included</strong>: MLflow also loads <code>conda.yaml</code>, <code>requirements.txt</code>, feature metadata</li><li><strong>Trivial rollback</strong>: Change stage in MLflow UI, API automatically reloads on next restart</li></ul><p><strong>2. GCS As Fallback (Not Primary)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># GCS load</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>google.cloud</span> <span class=kn>import</span> <span class=n>storage</span>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bucket</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>bucket</span><span class=p>(</span><span class=s2>&#34;my-bucket&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>blob</span> <span class=o>=</span> <span class=n>bucket</span><span class=o>.</span><span class=n>blob</span><span class=p>(</span><span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_bytes</span> <span class=o>=</span> <span class=n>blob</span><span class=o>.</span><span class=n>download_as_bytes</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>model_bytes</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Why not primary:</strong></p><ul><li><strong>No versioning:</strong> <code>models/trained/housing_price_model.pkl</code> is always the &ldquo;latest&rdquo;—you can&rsquo;t load v1 vs v2 without changing the path</li><li><strong>No metadata:</strong> You only get the pickle, you don&rsquo;t know what hyperparameters/features it expects</li><li><strong>No stages:</strong> No concept of Staging vs Production</li></ul><p><strong>When to use GCS as primary:</strong></p><ul><li>MLflow is unavailable (outage)</li><li>Simple setup (only one model, don&rsquo;t need registry)</li><li>Budget constraint (avoid hosting MLflow)</li></ul><p><strong>3. Local As Last Resort</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Local load</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Only for:</strong></p><ul><li>Local development (don&rsquo;t want to depend on GCS/MLflow)</li><li>Debugging (broken model in GCS, test with local copy)</li><li>CI/CD tests (GitHub Actions doesn&rsquo;t have access to GCS)</li></ul><p><strong>Never for real production</strong>—if GCS and MLflow are down, you have bigger problems than the model.</p><p><strong>4. Embedded Preprocessing Pipeline</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span> <span class=o>=</span> <span class=n>HousingPreprocessor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>processed_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>predictions</span>
</span></span></code></pre></div><p><strong>Why critical:</strong> The model expects processed features (one-hot encoding of <code>ocean_proximity</code>, cluster feature engineering). If the client sends raw features, the model fails.</p><p><strong>Implementation options:</strong></p><p><strong>A) Preprocessing in the API (this project):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Client sends raw features</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;ocean_proximity&#34;</span><span class=p>:</span> <span class=s2>&#34;NEAR BAY&#34;</span><span class=p>,</span> <span class=s2>&#34;longitude&#34;</span><span class=p>:</span> <span class=o>-</span><span class=mf>122.23</span><span class=p>,</span> <span class=o>...</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API applies preprocessing</span>
</span></span><span class=line><span class=cl><span class=n>processed</span> <span class=o>=</span> <span class=n>preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>raw_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Model receives processed features</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>B) Preprocessing in the client (bad for public APIs):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Client must know exact preprocessing</span>
</span></span><span class=line><span class=cl><span class=n>processed</span> <span class=o>=</span> <span class=n>client_side_preprocessing</span><span class=p>(</span><span class=n>raw_features</span><span class=p>)</span>  <span class=c1># What does this do?</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API only does inference</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Trade-offs:</strong></p><table><thead><tr><th>Approach</th><th>Advantage</th><th>Disadvantage</th></tr></thead><tbody><tr><td><strong>Preprocessing in API</strong></td><td>Client doesn&rsquo;t need to know preprocessing</td><td>More complex API, +5ms latency</td></tr><tr><td><strong>Preprocessing in client</strong></td><td>Simple API, low latency</td><td>Client must replicate exact preprocessing</td></tr></tbody></table><p><strong>For public APIs:</strong> Always preprocessing in the API. Clients shouldn&rsquo;t know internal model details.</p><p><strong>For internal APIs:</strong> Depends. If the client is another service you control, you can do preprocessing there to reduce latency.</p><hr><h3 id=4-requestresponse-validation-pydantic-schemas>4. Request/Response Validation: Pydantic Schemas<a hidden class=anchor aria-hidden=true href=#4-requestresponse-validation-pydantic-schemas>#</a></h3><h4 id=the-anti-pattern-manual-validation>The Anti-Pattern: Manual Validation<a hidden class=anchor aria-hidden=true href=#the-anti-pattern-manual-validation>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># BAD: Manual validation prone to errors</span>
</span></span><span class=line><span class=cl><span class=nd>@app.post</span><span class=p>(</span><span class=s2>&#34;/predict&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>request</span><span class=p>:</span> <span class=nb>dict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=s2>&#34;longitude&#34;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>request</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;missing longitude&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>request</span><span class=p>[</span><span class=s2>&#34;longitude&#34;</span><span class=p>],</span> <span class=p>(</span><span class=nb>int</span><span class=p>,</span> <span class=nb>float</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;longitude must be number&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>request</span><span class=p>[</span><span class=s2>&#34;longitude&#34;</span><span class=p>]</span> <span class=o>&lt;</span> <span class=o>-</span><span class=mi>180</span> <span class=ow>or</span> <span class=n>request</span><span class=p>[</span><span class=s2>&#34;longitude&#34;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>180</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;longitude out of range&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... 50 more lines of manual validation</span>
</span></span></code></pre></div><p><strong>Problems:</strong></p><ul><li>Repetitive and fragile code</li><li>Inconsistent errors (<code>"missing longitude"</code> vs <code>"longitude is required"</code>)</li><li>No automatic documentation (OpenAPI)</li><li>Hard to test</li></ul><h4 id=the-solution-pydantic-schemas>The Solution: Pydantic Schemas<a hidden class=anchor aria-hidden=true href=#the-solution-pydantic-schemas>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/models/schemas.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pydantic</span> <span class=kn>import</span> <span class=n>BaseModel</span><span class=p>,</span> <span class=n>Field</span><span class=p>,</span> <span class=n>field_validator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>HousingFeatures</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Input features for housing price prediction.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>longitude</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span><span class=p>,</span>  <span class=c1># Required</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Longitude coordinate&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ge</span><span class=o>=-</span><span class=mi>180</span><span class=p>,</span>  <span class=c1># greater or equal</span>
</span></span><span class=line><span class=cl>        <span class=n>le</span><span class=o>=</span><span class=mi>180</span>    <span class=c1># less or equal</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>latitude</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Latitude coordinate&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=-</span><span class=mi>90</span><span class=p>,</span> <span class=n>le</span><span class=o>=</span><span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>housing_median_age</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Median age of houses&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>total_rooms</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Total number of rooms&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>total_bedrooms</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Total number of bedrooms&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>population</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Block population&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>households</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Number of households&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>median_income</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Median income&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ocean_proximity</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Proximity to ocean&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@field_validator</span><span class=p>(</span><span class=s1>&#39;ocean_proximity&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nd>@classmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>validate_ocean_proximity</span><span class=p>(</span><span class=bp>cls</span><span class=p>,</span> <span class=n>v</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Validate ocean proximity values.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>valid_values</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;&lt;1H OCEAN&#39;</span><span class=p>,</span> <span class=s1>&#39;INLAND&#39;</span><span class=p>,</span> <span class=s1>&#39;ISLAND&#39;</span><span class=p>,</span> <span class=s1>&#39;NEAR BAY&#39;</span><span class=p>,</span> <span class=s1>&#39;NEAR OCEAN&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>v</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>valid_values</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;ocean_proximity must be one of: </span><span class=si>{</span><span class=s1>&#39;, &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>valid_values</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>v</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>  <span class=c1># Normalize to uppercase</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;json_schema_extra&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;examples&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;longitude&#34;</span><span class=p>:</span> <span class=o>-</span><span class=mf>122.23</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;latitude&#34;</span><span class=p>:</span> <span class=mf>37.88</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;housing_median_age&#34;</span><span class=p>:</span> <span class=mf>41.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;total_rooms&#34;</span><span class=p>:</span> <span class=mf>880.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;total_bedrooms&#34;</span><span class=p>:</span> <span class=mf>129.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;population&#34;</span><span class=p>:</span> <span class=mf>322.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;households&#34;</span><span class=p>:</span> <span class=mf>126.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;median_income&#34;</span><span class=p>:</span> <span class=mf>8.3252</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;ocean_proximity&#34;</span><span class=p>:</span> <span class=s2>&#34;NEAR BAY&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>}]</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p><strong>What this automatically provides:</strong></p><ol><li><p><strong>Type validation:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=s2>&#34;not a number&#34;</span><span class=p>}</span>  <span class=c1>// Rejected: ValidationError
</span></span></span></code></pre></div></li><li><p><strong>Range validation:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mi>-200</span><span class=p>}</span>  <span class=c1>// Rejected: must be &gt;= -180
</span></span></span></code></pre></div></li><li><p><strong>Custom validation:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;ocean_proximity&#34;</span><span class=p>:</span> <span class=s2>&#34;INVALID&#34;</span><span class=p>}</span>  <span class=c1>// Rejected: must be one of [...]
</span></span></span></code></pre></div></li><li><p><strong>Automatic documentation at <code>/docs</code>:</strong></p><ul><li>Swagger UI shows all fields</li><li>Descriptions, constraints, examples</li><li>Try-it-out works out-of-the-box</li></ul></li><li><p><strong>Type-safe serialization:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>features</span> <span class=o>=</span> <span class=n>HousingFeatures</span><span class=p>(</span><span class=o>**</span><span class=n>request_json</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>features</span><span class=o>.</span><span class=n>longitude</span>  <span class=c1># Type: float (not Optional[Any])</span>
</span></span></code></pre></div></li></ol><h4 id=batch-predictions-support>Batch Predictions Support<a hidden class=anchor aria-hidden=true href=#batch-predictions-support>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PredictionRequest</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Request model for single or batch predictions.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>instances</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>HousingFeatures</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;List of housing features for prediction&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>min_length</span><span class=o>=</span><span class=mi>1</span>  <span class=c1># At least one instance</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p><strong>Usage:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;instances&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mf>-122.23</span><span class=p>,</span> <span class=err>...</span><span class=p>},</span>  <span class=c1>// Predict house 1
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mf>-118.45</span><span class=p>,</span> <span class=err>...</span><span class=p>},</span>  <span class=c1>// Predict house 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mf>-121.89</span><span class=p>,</span> <span class=err>...</span><span class=p>}</span>   <span class=c1>// Predict house 3
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>Why support batch:</strong></p><ul><li><strong>Reduced latency:</strong> 3 individual requests = 150ms. 1 batch of 3 = 60ms.</li><li><strong>Reduced cost:</strong> Less HTTP overhead (headers, handshake, etc.)</li><li><strong>Efficient inference:</strong> The model can vectorize operations</li></ul><p><strong>Trade-off:</strong> Very large batch size (>1000) can cause timeouts. Implement limit:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>instances</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>HousingFeatures</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>min_length</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>100</span>  <span class=c1># Maximum 100 predictions per request</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h4 id=response-schema>Response Schema<a hidden class=anchor aria-hidden=true href=#response-schema>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PredictionResult</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Individual prediction result.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>predicted_price</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Predicted median house value&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>confidence_interval</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>dict</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Confidence interval (if available)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PredictionResponse</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Response model for predictions.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>PredictionResult</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;List of predictions&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_version</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Model version used&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;json_schema_extra&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;examples&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;predictions&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;predicted_price&#34;</span><span class=p>:</span> <span class=mf>452600.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;confidence_interval&#34;</span><span class=p>:</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>                <span class=p>}],</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;model_version&#34;</span><span class=p>:</span> <span class=s2>&#34;randomforest_v1&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>}]</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p><strong><code>model_version</code> in response:</strong> Crucial for debugging. If a client reports incorrect predictions, the <code>model_version</code> tells you which model was used (v1, v2, Production, etc.).</p><hr><h3 id=5-router-pattern-endpoints-and-error-handling>5. Router Pattern: Endpoints and Error Handling<a hidden class=anchor aria-hidden=true href=#5-router-pattern-endpoints-and-error-handling>#</a></h3><h4 id=the-router-structure>The Router Structure<a hidden class=anchor aria-hidden=true href=#the-router-structure>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/routers/predict.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fastapi</span> <span class=kn>import</span> <span class=n>APIRouter</span><span class=p>,</span> <span class=n>HTTPException</span><span class=p>,</span> <span class=n>status</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>router</span> <span class=o>=</span> <span class=n>APIRouter</span><span class=p>(</span><span class=n>prefix</span><span class=o>=</span><span class=s2>&#34;/api/v1&#34;</span><span class=p>,</span> <span class=n>tags</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;predictions&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Global instances (set by main.py)</span>
</span></span><span class=line><span class=cl><span class=n>model_loader</span><span class=p>:</span> <span class=n>ModelLoader</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=n>wandb_logger</span><span class=p>:</span> <span class=n>WandBLogger</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>set_model_loader</span><span class=p>(</span><span class=n>loader</span><span class=p>:</span> <span class=n>ModelLoader</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Dependency injection pattern.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>global</span> <span class=n>model_loader</span>
</span></span><span class=line><span class=cl>    <span class=n>model_loader</span> <span class=o>=</span> <span class=n>loader</span>
</span></span></code></pre></div><p><strong>Why <code>prefix="/api/v1"</code>:</strong></p><pre tabindex=0><code>/api/v1/predict  ← Version 1 of the API
/api/v2/predict  ← Version 2 (breaking changes)
</code></pre><p>You can run both versions simultaneously during migration:</p><ul><li>Legacy clients use <code>/api/v1/</code></li><li>New clients use <code>/api/v2/</code></li><li>Deprecate v1 after 6 months</li></ul><p><strong>Without versioning:</strong> Breaking change → all clients break at the same time.</p><h4 id=the-main-endpoint-post-apiv1predict>The Main Endpoint: POST /api/v1/predict<a hidden class=anchor aria-hidden=true href=#the-main-endpoint-post-apiv1predict>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@router.post</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;/predict&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>response_model</span><span class=o>=</span><span class=n>PredictionResponse</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_200_OK</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>responses</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=mi>400</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>ErrorResponse</span><span class=p>,</span> <span class=s2>&#34;description&#34;</span><span class=p>:</span> <span class=s2>&#34;Invalid input data&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=mi>500</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>ErrorResponse</span><span class=p>,</span> <span class=s2>&#34;description&#34;</span><span class=p>:</span> <span class=s2>&#34;Prediction failed&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>summary</span><span class=o>=</span><span class=s2>&#34;Predict housing prices&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Make predictions for housing prices based on input features&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>request</span><span class=p>:</span> <span class=n>PredictionRequest</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>PredictionResponse</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Predict housing prices for given features.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. Check model loaded</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>model_loader</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=ow>not</span> <span class=n>model_loader</span><span class=o>.</span><span class=n>is_loaded</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=n>HTTPException</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_500_INTERNAL_SERVER_ERROR</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>detail</span><span class=o>=</span><span class=s2>&#34;Model not loaded&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 2. Convert Pydantic models to DataFrame</span>
</span></span><span class=line><span class=cl>        <span class=n>features_list</span> <span class=o>=</span> <span class=p>[</span><span class=n>instance</span><span class=o>.</span><span class=n>model_dump</span><span class=p>()</span> <span class=k>for</span> <span class=n>instance</span> <span class=ow>in</span> <span class=n>request</span><span class=o>.</span><span class=n>instances</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>features_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. Make predictions</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span> <span class=o>=</span> <span class=n>model_loader</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 4. Calculate metrics</span>
</span></span><span class=line><span class=cl>        <span class=n>response_time_ms</span> <span class=o>=</span> <span class=p>(</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start_time</span><span class=p>)</span> <span class=o>*</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 5. Format response</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>PredictionResult</span><span class=p>(</span><span class=n>predicted_price</span><span class=o>=</span><span class=nb>float</span><span class=p>(</span><span class=n>pred</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>pred</span> <span class=ow>in</span> <span class=n>predictions</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 6. Log to W&amp;B (async, non-blocking)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>wandb_logger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb_logger</span><span class=o>.</span><span class=n>log_prediction</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>features</span><span class=o>=</span><span class=n>features_list</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>predictions</span><span class=o>=</span><span class=p>[</span><span class=nb>float</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>predictions</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>model_version</span><span class=o>=</span><span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>response_time_ms</span><span class=o>=</span><span class=n>response_time_ms</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>PredictionResponse</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>predictions</span><span class=o>=</span><span class=n>results</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>model_version</span><span class=o>=</span><span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>ValueError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Validation error (e.g., feature out of expected range)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>wandb_logger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb_logger</span><span class=o>.</span><span class=n>log_error</span><span class=p>(</span><span class=s2>&#34;validation_error&#34;</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>),</span> <span class=n>features_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=n>HTTPException</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_400_BAD_REQUEST</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>detail</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;Invalid input data: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Unexpected error (e.g., corrupted model, OOM)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>wandb_logger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb_logger</span><span class=o>.</span><span class=n>log_error</span><span class=p>(</span><span class=s2>&#34;prediction_error&#34;</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=n>HTTPException</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_500_INTERNAL_SERVER_ERROR</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>detail</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;Prediction failed: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><p><strong>Error handling decisions:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Inference</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>ValueError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Client sent invalid data → 400 Bad Request</span>
</span></span><span class=line><span class=cl>    <span class=c1># Log to W&amp;B for analysis</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>400</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>Exception</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Unexpected error (bug in code/model) → 500 Internal Server Error</span>
</span></span><span class=line><span class=cl>    <span class=c1># Log to W&amp;B for alerting</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>500</span>
</span></span></code></pre></div><p><strong>Why distinguish 400 vs 500:</strong></p><ul><li><strong>400:</strong> Client&rsquo;s fault. No automatic retries.</li><li><strong>500:</strong> Server&rsquo;s fault. Client can retry.</li></ul><p><strong>Error logging to W&amp;B:</strong> Allows detecting patterns. If you see 1000 <code>validation_error</code> for <code>ocean_proximity="INVALID"</code>, add a clearer error message.</p><hr><h3 id=6-wb-logging-production-observability>6. W&amp;B Logging: Production Observability<a hidden class=anchor aria-hidden=true href=#6-wb-logging-production-observability>#</a></h3><h4 id=why-log-predictions>Why Log Predictions<a hidden class=anchor aria-hidden=true href=#why-log-predictions>#</a></h4><p><strong>Question:</strong> &ldquo;Why log every prediction if I already have uvicorn logs?&rdquo;</p><p><strong>Answer:</strong> Uvicorn logs tell you:</p><ul><li>What endpoint was called</li><li>HTTP status code</li><li>How long it took</li></ul><p>W&amp;B logs tell you:</p><ul><li><strong>What features</strong> were used</li><li><strong>What prediction</strong> was made</li><li><strong>Distribution of predictions</strong> (are they all in $200k-$500k? are there outliers?)</li><li><strong>Average latency</strong> per request</li><li><strong>Error rate</strong> (how many requests fail?)</li></ul><p><strong>Real use case:</strong> Stakeholder reports &ldquo;predictions are too high lately&rdquo;. You open W&amp;B dashboard:</p><pre tabindex=0><code>prediction/mean: $450k (before: $380k)
features/median_income: 9.2 (before: 7.5)
</code></pre><p><strong>Conclusion:</strong> No bug—clients are simply querying houses in more expensive areas (<code>median_income</code> higher). Without W&amp;B, you&rsquo;d be debugging code for hours.</p><h4 id=the-implementation>The Implementation<a hidden class=anchor aria-hidden=true href=#the-implementation>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/wandb_logger.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>WandBLogger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>project</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;housing-mlops-api&#34;</span><span class=p>,</span> <span class=n>enabled</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>enabled</span> <span class=o>=</span> <span class=n>enabled</span> <span class=ow>and</span> <span class=nb>bool</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;WANDB_API_KEY&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>enabled</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_run</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>init</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>project</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>project</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>job_type</span><span class=o>=</span><span class=s2>&#34;api-inference&#34;</span><span class=p>,</span>  <span class=c1># Distinguish from training runs</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;environment&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;ENVIRONMENT&#34;</span><span class=p>,</span> <span class=s2>&#34;production&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;model_version&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;MODEL_VERSION&#34;</span><span class=p>,</span> <span class=s2>&#34;unknown&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=n>reinit</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># Allow multiple init() in same process</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>log_prediction</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>features</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Dict</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>model_version</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>response_time_ms</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>enabled</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Aggregated metrics</span>
</span></span><span class=line><span class=cl>        <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/count&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/mean&#34;</span><span class=p>:</span> <span class=nb>sum</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/min&#34;</span><span class=p>:</span> <span class=nb>min</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/max&#34;</span><span class=p>:</span> <span class=nb>max</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;performance/response_time_ms&#34;</span><span class=p>:</span> <span class=n>response_time_ms</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;model/version&#34;</span><span class=p>:</span> <span class=n>model_version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;timestamp&#34;</span><span class=p>:</span> <span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>isoformat</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Feature distributions (sample first 100)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>features</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>100</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>feat</span><span class=p>,</span> <span class=n>pred</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>features</span><span class=p>,</span> <span class=n>predictions</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>                <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>                    <span class=sa>f</span><span class=s2>&#34;features/instance_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>/median_income&#34;</span><span class=p>:</span> <span class=n>feat</span><span class=p>[</span><span class=s2>&#34;median_income&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=sa>f</span><span class=s2>&#34;predictions/instance_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>:</span> <span class=n>pred</span>
</span></span><span class=line><span class=cl>                <span class=p>})</span>
</span></span></code></pre></div><p><strong>Why <code>job_type="api-inference"</code>:</strong></p><p>In W&amp;B dashboard, you can filter by job type:</p><ul><li><code>training</code>: Training pipeline runs</li><li><code>sweep</code>: Hyperparameter sweep runs</li><li><code>api-inference</code>: Production predictions</li></ul><p><strong>Why <code>reinit=True</code>:</strong> A uvicorn process can live for days. <code>reinit=True</code> allows creating multiple W&amp;B runs within the same process (one per startup/restart).</p><p><strong>Why sample first 100:</strong> Logging 10,000 individual features per request would be too much overhead. Sampling 100 gives a representative distribution without killing performance.</p><h4 id=wb-dashboard-in-production>W&amp;B Dashboard in Production<a hidden class=anchor aria-hidden=true href=#wb-dashboard-in-production>#</a></h4><pre tabindex=0><code># Metrics to monitor:

prediction/count: Requests per minute (RPM)
  - Expected: 100-500 RPM
  - Alert: &lt;10 RPM (is it down?) or &gt;2000 RPM (DDoS?)

prediction/mean: Average predicted price
  - Expected: $300k-$450k (depending on market)
  - Alert: &gt;$1M (broken model) or &lt;$50k (data drift)

performance/response_time_ms: Latency
  - Expected: 30-60ms
  - Alert: &gt;200ms (slow model or CPU throttling)

error/count: Errors per minute
  - Expected: 0-5 errors/min
  - Alert: &gt;50 errors/min (investigate immediately)
</code></pre><hr><h3 id=7-cors-and-security>7. CORS and Security<a hidden class=anchor aria-hidden=true href=#7-cors-and-security>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>app</span><span class=o>.</span><span class=n>add_middleware</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>CORSMiddleware</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_origins</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;http://localhost:3000&#34;</span><span class=p>,</span>     <span class=c1># Local frontend (React/Streamlit)</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;http://localhost:8080&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;https://app.company.com&#34;</span><span class=p>,</span>   <span class=c1># Production frontend</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_credentials</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1># No cookies (API is stateless)</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_methods</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;GET&#34;</span><span class=p>,</span> <span class=s2>&#34;POST&#34;</span><span class=p>],</span>  <span class=c1># Only necessary methods</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_headers</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;Content-Type&#34;</span><span class=p>,</span> <span class=s2>&#34;Authorization&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>max_age</span><span class=o>=</span><span class=mi>3600</span><span class=p>,</span>  <span class=c1># Cache preflight requests for 1 hour</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>Why restricted origins:</strong></p><p><strong>Anti-pattern (permissive):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>allow_origins</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;*&#34;</span><span class=p>]</span>  <span class=c1># BAD: Any site can call your API</span>
</span></span></code></pre></div><p><strong>Problem:</strong> A malicious site <code>evil.com</code> can make requests to your API from the user&rsquo;s browser, potentially:</p><ul><li>Consuming your GCP quota (if no auth)</li><li>Making spam predictions</li><li>DoS attack</li></ul><p><strong>Correct pattern (restrictive):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>allow_origins</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;https://app.company.com&#34;</span><span class=p>]</span>  <span class=c1># Only your frontend</span>
</span></span></code></pre></div><p><strong>For local development:</strong> Add <code>http://localhost:3000</code> temporarily, remove in production.</p><p><strong>Why <code>allow_credentials=False</code>:</strong> This API is stateless—doesn&rsquo;t use cookies or sessions. <code>allow_credentials=True</code> would be unnecessary and an additional attack surface.</p><hr><h3 id=8-the-complete-flow-of-a-request>8. The Complete Flow of A Request<a hidden class=anchor aria-hidden=true href=#8-the-complete-flow-of-a-request>#</a></h3><p><strong>Request:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl -X POST http://localhost:8080/api/v1/predict <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;instances&#34;: [{
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;longitude&#34;: -122.23,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;latitude&#34;: 37.88,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;housing_median_age&#34;: 41,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;total_rooms&#34;: 880,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;total_bedrooms&#34;: 129,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;population&#34;: 322,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;households&#34;: 126,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;median_income&#34;: 8.3252,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;ocean_proximity&#34;: &#34;NEAR BAY&#34;
</span></span></span><span class=line><span class=cl><span class=s1>    }]
</span></span></span><span class=line><span class=cl><span class=s1>  }&#39;</span>
</span></span></code></pre></div><p><strong>The internal journey (&lt; 50ms):</strong></p><pre tabindex=0><code>1. FastAPI receives request (1ms)
   ├─ CORS middleware validates origin
   └─ Router match: POST /api/v1/predict

2. Pydantic validation (2ms)
   ├─ Parse JSON → PredictionRequest object
   ├─ Validate types (longitude: float ✓)
   ├─ Validate ranges (longitude: -122.23, within [-180, 180] ✓)
   └─ Custom validator (ocean_proximity: &#34;NEAR BAY&#34; → valid ✓)

3. Endpoint handler: predict() (40ms)
   ├─ Check model_loader.is_loaded (0.1ms)
   ├─ Convert Pydantic → DataFrame (1ms)
   ├─ Preprocessing (5ms)
   │   ├─ One-hot encode ocean_proximity
   │   ├─ Compute cluster similarity features
   │   └─ Scale numerical features
   ├─ Model inference (30ms)
   │   └─ RandomForest.predict(processed_features)
   ├─ Format response (1ms)
   └─ Log to W&amp;B (async, &lt;1ms non-blocking)

4. FastAPI serializes response (2ms)
   └─ PredictionResponse → JSON

5. HTTP response sent (1ms)

Total: ~50ms
</code></pre><p><strong>Response:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;predictions&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;predicted_price&#34;</span><span class=p>:</span> <span class=mf>452600.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;confidence_interval&#34;</span><span class=p>:</span> <span class=kc>null</span>
</span></span><span class=line><span class=cl>  <span class=p>}],</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;model_version&#34;</span><span class=p>:</span> <span class=s2>&#34;models:/housing_price_model/Production&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><h3 id=9-what-this-architecture-achieves>9. What This Architecture Achieves<a hidden class=anchor aria-hidden=true href=#9-what-this-architecture-achieves>#</a></h3><p><strong>Without this architecture (naive API):</strong></p><ul><li>Load model on every request (5 seconds/request)</li><li>Manual validation prone to errors</li><li>No observability (debugging is guessing)</li><li>No API versioning (breaking changes break clients)</li><li>Open CORS (vulnerability)</li></ul><p><strong>With this architecture:</strong></p><ul><li><strong>Latency:</strong> &lt;50ms per prediction (cached model)</li><li><strong>Reliability:</strong> Pydantic guarantees valid requests before reaching the model</li><li><strong>Observability:</strong> W&amp;B dashboard shows prediction distribution, latency, errors</li><li><strong>Maintainability:</strong> Separation of concerns (core/routers/models)</li><li><strong>Security:</strong> Restrictive CORS, robust error handling</li><li><strong>Versioning:</strong> <code>/api/v1/</code> allows evolving the API without breaking clients</li></ul><p><strong>The real value:</strong> This API can scale from 10 requests/min to 10,000 requests/min without code changes—just add more containers with a load balancer. The architecture is already ready.</p><hr><p><a name=model-strategies></a></p><h2 id=11-model-selection-and-parameter-strategies>11. Model Selection and Parameter Strategies<a hidden class=anchor aria-hidden=true href=#11-model-selection-and-parameter-strategies>#</a></h2><hr><h2 id=navigation>Navigation<a hidden class=anchor aria-hidden=true href=#navigation>#</a></h2><p><strong><a href=/mlops/anatomia-pipeline-mlops-parte-1/>← Part 1: Pipeline and Orchestration</a></strong> | <strong><a href=/mlops/anatomia-pipeline-mlops-parte-3/>Part 3: Production and Best Practices →</a></strong></p><p>In Part 3 we will cover:</p><ul><li>Model selection and parameter strategies</li><li>Testing: Fixtures, mocking, and real coverage</li><li>Production patterns (Transform Pattern, Data Drift, Feature Stores)</li><li>Production Readiness Checklist</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://carlosdanieljimenez.com/tags/mlops/>Mlops</a></li><li><a href=https://carlosdanieljimenez.com/tags/ci-cd/>Ci-Cd</a></li><li><a href=https://carlosdanieljimenez.com/tags/docker/>Docker</a></li><li><a href=https://carlosdanieljimenez.com/tags/fastapi/>Fastapi</a></li><li><a href=https://carlosdanieljimenez.com/tags/github-actions/>Github-Actions</a></li><li><a href=https://carlosdanieljimenez.com/tags/wandb/>Wandb</a></li><li><a href=https://carlosdanieljimenez.com/tags/mlflow/>Mlflow</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://carlosdanieljimenez.com/>The Probability Engine</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div class=newsletter-cta><div class=newsletter-content><h3>📬 Did this help?</h3><p>I write about MLOps, Edge AI, and making models work outside the lab.
One email per month, max. No spam, no course pitches, just technical content.</p><form action=https://buttondown.com/api/emails/embed-subscribe/carlosjimenez88m method=post class=newsletter-form target=popupwindow onsubmit='window.open("https://buttondown.com/carlosjimenez88m","popupwindow")'><div class=form-group><input type=email name=email id=bd-email placeholder=your@email.com required aria-label="Email address">
<input type=submit value=Subscribe></div></form><p class=newsletter-stats>Join engineers building ML systems and Edge Computing infrastructure.</p></div></div><style>.newsletter-cta{margin:3rem auto 2rem;padding:2rem;max-width:650px;background:var(--entry);border:1px solid var(--border);border-radius:8px;text-align:center}.newsletter-content h3{margin:0 0 1rem;font-size:1.5rem;color:var(--primary)}.newsletter-content p{margin:0 0 1.5rem;color:var(--secondary);line-height:1.6}.newsletter-form{margin:1.5rem 0}.form-group{display:flex;gap:.5rem;max-width:500px;margin:0 auto;flex-wrap:wrap;justify-content:center}.newsletter-form input[type=email]{flex:1;min-width:250px;padding:.75rem 1rem;font-size:1rem;border:1px solid var(--border);border-radius:6px;background:var(--theme);color:var(--content);transition:border-color .2s ease}.newsletter-form input[type=email]:focus{outline:none;border-color:var(--primary);box-shadow:0 0 0 3px rgba(var(--primary-rgb),.1)}.newsletter-form input[type=submit]{padding:.75rem 2rem;font-size:1rem;font-weight:600;color:#fff;background:var(--primary);border:none;border-radius:6px;cursor:pointer;transition:opacity .2s ease}.newsletter-form input[type=submit]:hover{opacity:.9}.newsletter-stats{font-size:.875rem;color:var(--secondary);margin-top:1rem;font-style:italic}@media screen and (max-width:600px){.newsletter-cta{padding:1.5rem 1rem;margin:2rem .5rem 1rem}.newsletter-content h3{font-size:1.25rem}.form-group{flex-direction:column;width:100%}.newsletter-form input[type=email],.newsletter-form input[type=submit]{width:100%;min-width:100%}}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>