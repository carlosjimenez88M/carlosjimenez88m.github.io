<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices | The Probability Engine</title>
<meta name=keywords content="mlops,testing,production,data-drift,monitoring"><meta name=description content="Part 3: Model selection strategies, advanced testing, production patterns, data drift, model monitoring, and production readiness checklist."><meta name=author content="Carlos Daniel Jiménez"><link rel=canonical href=https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=16x16 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=32x32 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=apple-touch-icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=mask-icon href=https://carlosdanieljimenez.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=icon type=image/png href=/img/icon.jpeg><link rel=apple-touch-icon href=/img/icon.jpeg><link rel="shortcut icon" href=/img/icon.jpeg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/"><meta property="og:site_name" content="The Probability Engine"><meta property="og:title" content="Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices"><meta property="og:description" content="Part 3: Model selection strategies, advanced testing, production patterns, data drift, model monitoring, and production readiness checklist."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2026-01-13T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-13T00:00:00+00:00"><meta property="article:tag" content="Mlops"><meta property="article:tag" content="Testing"><meta property="article:tag" content="Production"><meta property="article:tag" content="Data-Drift"><meta property="article:tag" content="Monitoring"><meta name=twitter:card content="summary"><meta name=twitter:title content="Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices"><meta name=twitter:description content="Part 3: Model selection strategies, advanced testing, production patterns, data drift, model monitoring, and production readiness checklist."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://carlosdanieljimenez.com/post/"},{"@type":"ListItem","position":2,"name":"Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices","item":"https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices","name":"Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices","description":"Part 3: Model selection strategies, advanced testing, production patterns, data drift, model monitoring, and production readiness checklist.","keywords":["mlops","testing","production","data-drift","monitoring"],"articleBody":" Complete MLOps Series: ← Part 1: Pipeline | ← Part 2: Deployment | Part 3 (current)\nAnatomy of an MLOps Pipeline - Part 3: Production and Best Practices 11. Model and Parameter Selection Strategies The Complete Flow: Selection → Sweep → Registration This pipeline implements a three-phase strategy for model optimization, each with a specific purpose:\nStep 05: Model Selection ├── Compares 5 algorithms with basic GridSearch (5-10 combos/model) ├── Objective: Identify best model family (Random Forest vs Gradient Boosting vs ...) ├── Primary metric: MAPE (Mean Absolute Percentage Error) └── Output: Best algorithm + initial parameters Step 06: Hyperparameter Sweep ├── Optimizes ONLY the best algorithm from Step 05 ├── Bayesian optimization with 50+ runs (exhaustive search space) ├── Objective: Find optimal configuration of best model ├── Primary metric: wMAPE (Weighted MAPE, less biased) └── Output: best_params.yaml with optimal hyperparameters Step 07: Model Registration ├── Trains final model with parameters from Step 06 ├── Registers in MLflow Model Registry with rich metadata ├── Transitions to stage (Staging/Production) └── Output: Versioned model ready for deployment Why three separate steps? You don’t have computational resources to do exhaustive sweep of 5 algorithms × 50 combinations = 250 training runs. First decide strategy (which algorithm), then tactics (which hyperparameters).\nStep 05: Model Selection - Algorithm Comparison The 5 Candidate Models def get_available_models() -\u003e Dict[str, Any]: \"\"\"Get dictionary of available regression models.\"\"\" models = { \"RandomForest\": RandomForestRegressor(random_state=42), \"GradientBoosting\": GradientBoostingRegressor(random_state=42), \"Ridge\": Ridge(random_state=42), \"Lasso\": Lasso(random_state=42), \"DecisionTree\": DecisionTreeRegressor(random_state=42) } return models Why these models:\nRandomForest: Tree ensemble, robust, handles non-linearities GradientBoosting: Sequential boosting, better precision than RF but slower Ridge: Linear regression with L2 regularization, fast, interpretable Lasso: Linear regression with L1 regularization, does feature selection DecisionTree: Simple baseline, useful for comparison What’s missing (deliberately):\nXGBoost/LightGBM: Not included to reduce dependencies, but easy to add Neural Networks: Overkill for this problem (20k samples, tabular features) SVR: Very slow on large datasets, doesn’t scale well Parameter Grids: Initial GridSearch def get_default_param_grids() -\u003e Dict[str, Dict[str, list]]: \"\"\" Parameter grids for initial model selection. Refined based on domain knowledge. \"\"\" param_grids = { \"RandomForest\": { \"n_estimators\": [50, 100, 200, 300], # 4 options \"max_depth\": [10, 15, 20, 25, None], # 5 options \"min_samples_split\": [2, 5, 10], # 3 options \"min_samples_leaf\": [1, 2, 4], # 3 options }, # Total combinations: 4×5×3×3 = 180 # With 5-fold CV: 180×5 = 900 fits \"GradientBoosting\": { \"n_estimators\": [50, 100, 150, 200], # 4 options \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2], # 5 options \"max_depth\": [3, 4, 5, 6, 7], # 5 options \"subsample\": [0.8, 0.9, 1.0], # 3 options }, # Total: 4×5×5×3 = 300 combinations \"Ridge\": { \"alpha\": [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0], }, # Total: 9 combinations (fast) \"Lasso\": { \"alpha\": [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0], }, # Total: 9 combinations \"DecisionTree\": { \"max_depth\": [5, 10, 15, 20, 25, None], # 6 options \"min_samples_split\": [2, 5, 10, 20], # 4 options \"min_samples_leaf\": [1, 2, 4, 8], # 4 options } # Total: 6×4×4 = 96 combinations } return param_grids Grid Design Decisions 1. RandomForest: Focus on Overfitting Control\n\"max_depth\": [10, 15, 20, 25, None], \"min_samples_leaf\": [1, 2, 4], Reasoning: Random Forest tends to overfit on small datasets. max_depth and min_samples_leaf control tree depth—high values prevent the model from memorizing noise.\nNone in max_depth: Allows unlimited depth trees. Useful when the dataset has complex patterns requiring deep splits.\n2. GradientBoosting: Balance Learning Rate vs N_estimators\n\"n_estimators\": [50, 100, 150, 200], \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2], Classic trade-off:\nLow learning rate (0.01) + many estimators (200): Slow but accurate learning High learning rate (0.2) + few estimators (50): Fast but may diverge GridSearch explores both extremes.\nsubsample \u003c 1.0: Stochastic Gradient Boosting. Only uses 80-90% of data in each iteration, reduces overfitting.\n3. Ridge/Lasso: Alpha in Logarithmic Scale\n\"alpha\": [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0], Alpha controls regularization:\nLow alpha (0.01): Almost no regularization, complex model High alpha (500): Strong regularization, simple model (coefficients close to 0) Logarithmic scale covers the space more uniformly than linear scale.\nLasso vs Ridge:\nLasso (L1): Forces coefficients to exactly 0 → automatic feature selection Ridge (L2): Small coefficients but not zero → keeps all features If Lasso wins, it indicates some features are noise.\n4. DecisionTree: Comparison Baseline\nDecisionTree is the worst model (high variance, overfits easily), but serves to:\nVerify that the pipeline works correctly Comparison baseline: If Ridge/Lasso don’t beat DecisionTree, something’s wrong in feature engineering Training Function with GridSearch def train_model_with_gridsearch( model: Any, param_grid: Dict[str, list], X_train: pd.DataFrame, y_train: pd.Series, cv: int = 5 ) -\u003e Tuple[Any, Dict[str, Any], float, Dict[str, float]]: \"\"\"Train model with K-fold Cross-Validation via GridSearchCV.\"\"\" start_time = time.time() grid_search = GridSearchCV( estimator=model, param_grid=param_grid, cv=5, # 5-fold cross-validation scoring='neg_mean_absolute_error', # CRITICAL n_jobs=-1, # Parallelization verbose=0, return_train_score=True # To detect overfitting ) grid_search.fit(X_train, y_train) training_time = time.time() - start_time # Extract cross-validation results cv_metrics = { \"mean_test_score\": float(-grid_search.best_score_), \"std_test_score\": float(grid_search.cv_results_['std_test_score'][grid_search.best_index_]), \"mean_train_score\": float(-grid_search.cv_results_['mean_train_score'][grid_search.best_index_]), \"std_train_score\": float(grid_search.cv_results_['std_train_score'][grid_search.best_index_]), } return grid_search.best_estimator_, grid_search.best_params_, training_time, cv_metrics Critical Decisions 1. Scoring: neg_mean_absolute_error\nscoring='neg_mean_absolute_error' Why MAE and not RMSE or R²?\nMAE (Mean Absolute Error): Penalizes errors linearly RMSE: Penalizes errors quadratically (large errors weigh much more) R²: Relative metric, difficult to interpret in business terms For this problem:\nMAE = $15,000 → “The model is off by $15k on average” R² = 0.85 → What does this mean for the business? neg_mean_absolute_error: GridSearchCV minimizes the metric, but MAE should be minimized, so we use the negative.\n2. Cross-Validation: 5 Folds\ncv=5 Why 5 and not 10?\n5-fold: Balance between bias and variance\nEach fold has 80% training, 20% validation Faster than 10-fold (2x fewer fits) 10-fold: Less bias but higher computational cost\nUseful when you have few samples (\u003c1000 samples) With 16,512 training samples, 5-fold is sufficient.\n3. return_train_score=True\nreturn_train_score=True This logs the score on training set in addition to validation set. Allows detecting overfitting:\nif cv_metrics['mean_train_score'] \u003e\u003e cv_metrics['mean_test_score']: print(\"WARNING: Model is overfitting!\") # Train MAE = $5k, Test MAE = $20k → Clear overfitting 4. n_jobs=-1: Parallelization\nn_jobs=-1 Uses all available CPU cores. On an 8-core machine, 180 combinations × 5 folds = 900 fits are distributed in parallel.\nWithout parallelization: 900 fits × 2s/fit = 30 minutes With 8 cores: ~4 minutes\nEvaluation Metrics: Beyond MAPE def evaluate_model(model: Any, X_test: pd.DataFrame, y_test: pd.Series) -\u003e Dict[str, float]: \"\"\"Evaluates model with business-focused metrics.\"\"\" y_pred = model.predict(X_test) y_true = y_test.values # Traditional metrics mae = mean_absolute_error(y_test, y_pred) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) r2 = r2_score(y_test, y_pred) # Business-focused percentage error metrics mape = mean_absolute_percentage_error(y_true, y_pred) smape = symmetric_mean_absolute_percentage_error(y_true, y_pred) wmape = weighted_mean_absolute_percentage_error(y_true, y_pred) median_ape = median_absolute_percentage_error(y_true, y_pred) # Prediction accuracy at different thresholds within_5pct = predictions_within_threshold(y_true, y_pred, 0.05) within_10pct = predictions_within_threshold(y_true, y_pred, 0.10) within_15pct = predictions_within_threshold(y_true, y_pred, 0.15) return { \"mae\": float(mae), \"rmse\": float(rmse), \"r2\": float(r2), \"mape\": float(mape), \"smape\": float(smape), \"wmape\": float(wmape), \"median_ape\": float(median_ape), \"within_5pct\": float(within_5pct), \"within_10pct\": float(within_10pct), \"within_15pct\": float(within_15pct) } Why 4 Variants of MAPE:\n1. MAPE (Mean Absolute Percentage Error)\nmape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 Problem: Biased towards low values.\nIf you predict $500k instead of $510k → error = 2% If you predict $10k instead of $11k → error = 9%\nBoth are $10k absolute error, but MAPE penalizes the second more.\n2. SMAPE (Symmetric MAPE)\nsmape = np.mean(np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)) * 100 Uses the average of y_true and y_pred in the denominator. More symmetric:\nOverprediction and underprediction have similar weight Range: 0-200% (vs 0-∞% for MAPE) 3. wMAPE (Weighted MAPE)\nwmape = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100 Total sum of errors divided by total sum of actual values. Not affected by individual extreme values.\nUsed in Step 06 (Sweep) because it’s more robust than MAPE for datasets with high variance.\n4. Median APE\nmedian_ape = np.median(np.abs((y_true - y_pred) / y_true)) * 100 Median instead of mean. Robust to outliers.\nIf 95% of predictions have \u003c5% error but 5% have \u003e50% error:\nMAPE: ~7% (average includes outliers) Median APE: ~4% (outliers don’t affect median) Within-X% Metrics\nwithin_5pct = predictions_within_threshold(y_true, y_pred, 0.05) # Percentage of predictions with error \u003c5% Business interpretation: “75% of our predictions are within ±10% of actual value.”\nMore interpretable for stakeholders than “MAPE = 8.2%”.\nStep 05 Output logger.info(\" BEST MODEL: RandomForestRegressor\") logger.info(\"Business Metrics (Test Set):\") logger.info(\" MAPE (Mean APE): 8.23%\") logger.info(\" SMAPE (Symmetric MAPE): 7.95%\") logger.info(\" wMAPE (Weighted MAPE): 8.01%\") logger.info(\" Median APE: 6.45%\") logger.info(\" Within ±5%: 45.2%\") logger.info(\" Within ±10%: 72.8%\") logger.info(\" Within ±15%: 85.3%\") logger.info(\"\\nTraditional Metrics (Test Set):\") logger.info(\" R²: 0.8654\") logger.info(\" RMSE: $48,234.12\") logger.info(\" MAE: $32,456.78\") logger.info(\"\\nCross-Validation Results (5-fold):\") logger.info(\" Mean CV MAE: $33,125.45 (±$2,341.23)\") logger.info(\" Mean CV Train MAE: $28,934.56 (±$1,892.34)\") Best params saved:\nbest_params = { \"n_estimators\": 200, \"max_depth\": 20, \"min_samples_split\": 2, \"min_samples_leaf\": 1 } These params are used as a starting point for Step 06 (exhaustive Sweep).\nWhat This Strategy Achieves Without model selection:\n“I used Random Forest because everyone uses it” You have no evidence it’s better than Gradient Boosting With model selection:\n“I compared 5 algorithms with 5-fold CV. Random Forest achieved MAPE=8.2% (vs GradientBoosting=8.9%, Ridge=12.3%). Here’s the comparison table in W\u0026B.” Data-backed decision, not intuition. 11. Testing: Fixtures, Mocking and Real Coverage Why Testing ML Is Different Tests in ML are not like tests in web apps. You can’t do:\ndef test_model_predicts_correct_value(): model = load_model() assert model.predict([[1, 2, 3]]) == 452600.0 # ERROR: This is absurd ML models are probabilistic. The output is not deterministic in the traditional software sense.\nWhat you CAN test:\nData contracts: Inputs/outputs have correct types Invariants: Predictions are in expected range Reproducibility: Same input → same output (with fixed seed) Pipeline integrity: Steps run without exploding Integration: Components communicate correctly conftest.py: Shared Fixtures \"\"\" Common fixtures for pytest Author: Carlos Daniel Jiménez \"\"\" import pytest import pandas as pd import numpy as np from google.cloud import storage from unittest.mock import MagicMock, Mock @pytest.fixture def sample_housing_data(): \"\"\"Creates synthetic housing data.\"\"\" np.random.seed(42) n_samples = 100 data = { 'longitude': np.random.uniform(-124, -114, n_samples), 'latitude': np.random.uniform(32, 42, n_samples), 'housing_median_age': np.random.randint(1, 53, n_samples), 'total_rooms': np.random.randint(500, 5000, n_samples), 'total_bedrooms': np.random.randint(100, 1000, n_samples), 'population': np.random.randint(500, 3000, n_samples), 'households': np.random.randint(100, 1000, n_samples), 'median_income': np.random.uniform(0.5, 15, n_samples), 'median_house_value': np.random.uniform(50000, 500000, n_samples) } df = pd.DataFrame(data) # Add missing values to total_bedrooms missing_indices = np.random.choice(n_samples, size=20, replace=False) df.loc[missing_indices, 'total_bedrooms'] = np.nan return df @pytest.fixture def mock_gcs_client(): \"\"\"Creates GCS client mock.\"\"\" mock_client = MagicMock(spec=storage.Client) mock_bucket = MagicMock(spec=storage.Bucket) mock_blob = MagicMock(spec=storage.Blob) mock_bucket.exists.return_value = True mock_bucket.blob.return_value = mock_blob mock_client.bucket.return_value = mock_bucket return { 'client': mock_client, 'bucket': mock_bucket, 'blob': mock_blob } @pytest.fixture def mock_mlflow(monkeypatch): \"\"\"Mocks MLflow functions.\"\"\" mock_log_metric = Mock() mock_log_param = Mock() mock_log_artifact = Mock() monkeypatch.setattr('mlflow.log_metric', mock_log_metric) monkeypatch.setattr('mlflow.log_param', mock_log_param) monkeypatch.setattr('mlflow.log_artifact', mock_log_artifact) return { 'log_metric': mock_log_metric, 'log_param': mock_log_param, 'log_artifact': mock_log_artifact } Imputation Test: Data Contracts \"\"\" Tests for ImputationAnalyzer \"\"\" import pytest import pandas as pd import numpy as np from imputation_analyzer import ImputationAnalyzer def test_imputation_analyzer_returns_dataframe(sample_housing_data): \"\"\"Test that imputer returns DataFrame with filled missing values.\"\"\" analyzer = ImputationAnalyzer(sample_housing_data, target_column=\"total_bedrooms\") # Compare strategies results = analyzer.compare_all_methods() # Assertions assert len(results) == 4 # 4 strategies assert analyzer.best_method is not None assert all(result.rmse \u003e= 0 for result in results.values()) # Apply best imputer df_imputed = analyzer.apply_best_imputer(sample_housing_data) # Verify no NaNs remain assert df_imputed['total_bedrooms'].isnull().sum() == 0 # Verify rest of columns didn't change assert len(df_imputed) == len(sample_housing_data) def test_imputation_analyzer_reproducibility(): \"\"\"Test that imputation is reproducible with fixed seed.\"\"\" np.random.seed(42) df1 = generate_sample_data(n=100) analyzer1 = ImputationAnalyzer(df1, random_state=42) results1 = analyzer1.compare_all_methods() np.random.seed(42) df2 = generate_sample_data(n=100) analyzer2 = ImputationAnalyzer(df2, random_state=42) results2 = analyzer2.compare_all_methods() # Same input + same seed = same output assert results1['simple_median'].rmse == results2['simple_median'].rmse Complete Pipeline Test: Integration Test \"\"\" Integration test of complete pipeline \"\"\" import pytest from pathlib import Path def test_pipeline_runs_end_to_end(tmp_path, mock_gcs_client, sample_housing_data): \"\"\"Test that pipeline runs from beginning to end without exploding.\"\"\" # Setup: Save synthetic data data_path = tmp_path / \"housing.parquet\" sample_housing_data.to_parquet(data_path, index=False) # Step 01: Download (mocked) # ... # Step 02: Preprocessing from preprocessor import DataPreprocessor config = PreprocessingConfig( gcs_input_path=str(data_path), gcs_output_path=str(tmp_path / \"processed.parquet\"), bucket_name=\"test-bucket\" ) preprocessor = DataPreprocessor(config) result = preprocessor.run() assert result.success assert result.num_rows_output \u003e 0 # Step 03: Feature Engineering # ... # Verify outputs exist assert (tmp_path / \"processed.parquet\").exists() Real Coverage # Run tests with coverage pytest tests/ --cov=src --cov-report=html --cov-report=term-missing # Output: # ==================== test session starts ==================== # tests/test_imputation_analyzer.py ........ [80%] # tests/test_feature_engineering.py .... [100%] # # ----------- coverage: 87% ----------- # src/data/02_preprocessing/imputation_analyzer.py 92% # src/data/03_feature_engineering/feature_engineer.py 85% What This Achieves Without tests: “I think it works, I ran the notebook once and it didn’t explode.”\nWith tests: “87% coverage. All critical components are tested. CI runs tests on each commit.”\nTests don’t guarantee the model is good, but they guarantee the system that produces the model is reliable.\n12. Production Patterns Nobody Tells You About The Real Problem of Serving Here’s what no tutorial tells you: 90% of the effort in ML is not training a model—it’s making that model serve reliable predictions 24/7 without exploding.\nML courses end with model.save('model.pkl'). The reality of production starts with questions like:\nWhat if the model needs a trained KMeans to generate features? Do you save the KMeans too? What if it weighs 500MB? How do you guarantee that preprocessing in production is EXACTLY the same as in training? What if the data distribution changes and your model starts failing silently? This pipeline implements solutions to these problems that are rarely discussed. Let’s dissect them.\n12.1. The Transform Pattern: The Synthetic KMeans Trick Context: In Step 03 (Feature Engineering), the pipeline trains a KMeans with 10 clusters on latitude/longitude. The final model needs cluster_label as a feature.\nClassic problem:\n# During training kmeans = KMeans(n_clusters=10) kmeans.fit(X_geo) # Trains on 16,000 California samples df['cluster_label'] = kmeans.predict(X_geo) # Train the model model.fit(df, y) # Now what? How do you save the kmeans to use in the API? Naive solution (what 80% of people do):\n# Save BOTH models pickle.dump(kmeans, open('kmeans.pkl', 'wb')) pickle.dump(model, open('model.pkl', 'wb')) # In the API: Load both kmeans = pickle.load(open('kmeans.pkl', 'rb')) model = pickle.load(open('model.pkl', 'rb')) # For each prediction: cluster = kmeans.predict([[lon, lat]]) features = [..., cluster] prediction = model.predict(features) Why this is terrible:\nStorage overhead: Serialized KMeans can weigh 96KB per model. Multiply that by 50 model versions. Coupling: Now your API needs to load TWO artifacts per model version. What if they get out of sync? Latency: Calling kmeans.predict() adds ~2ms per request. The brilliant solution this project implements:\nChip Huyen calls this the Transform Pattern in “Designing Machine Learning Systems” (Chapter 7, section on feature consistency): when preprocessing is lightweight and deterministic, recreate it in the serving layer instead of serializing it.\nLook at the real code in api/app/core/preprocessor.py (lines 61-110):\nclass HousingPreprocessor: def _init_kmeans(self): \"\"\" Initialize KMeans with California housing geographical clusters. Uses typical California housing coordinates to create clusters. This is an approximation but works for the API use case. \"\"\" # California housing typical ranges: # Longitude: -124 to -114 # Latitude: 32 to 42 np.random.seed(42) # CRITICAL: Same seed as in training # Create synthetic data representing California geography n_samples = 1000 lon_samples = np.random.uniform(-124, -114, n_samples) lat_samples = np.random.uniform(32, 42, n_samples) # Weight towards major population centers major_centers = np.array([ [-118, 34], # LA [-122, 37.5], # SF [-117, 33], # San Diego [-121, 38.5], # Sacramento [-119, 36.5], # Fresno ]) # Add major centers multiple times for proper weighting lon_samples[:50] = major_centers[:, 0].repeat(10) lat_samples[:50] = major_centers[:, 1].repeat(10) X_geo = np.column_stack([lon_samples, lat_samples]) # Fit KMeans self.kmeans = KMeans( n_clusters=10, n_init=10, random_state=42 # SAME seed as training ) self.kmeans.fit(X_geo) What’s happening here?\nInstead of serializing the KMeans trained with 16,512 real samples, the API recreates a synthetic KMeans using:\nSynthetic data that approximates California’s geographical distribution Same seed (42) that was used in training Same n_clusters (10) Weighted centers towards major cities (LA, SF, San Diego) Trade-offs of this solution:\nAdvantages:\nZero storage overhead (don’t save the KMeans) Zero coupling (API is autonomous, doesn’t need additional artifacts) Identical latency (~2ms either way) Stateless serving (can scale API horizontally without shared state) Disadvantages:\nCluster drift: Synthetic clusters are NOT exactly the same as training ones In internal testing: ~2% mismatch in cluster labels In California Housing: impact on MAPE \u003c 0.3% Requires preprocessing to be deterministic and lightweight Doesn’t work if your KMeans needs 1 million samples to converge Doesn’t work if you have 512-dimensional text embeddings When to use this pattern:\nDO use it if:\nPreprocessing is lightweight (\u003c10ms) Feature is geographical/categorical with few unique values Impact of slight inconsistency is tolerable (regression, classification with margin) DON’T use it if:\nFeature is a deep embedding (BERT, ResNet) You need 100% bit-by-bit reproducibility Preprocessing requires gigabytes of state The lesson:\nChip Huyen summarizes it well: “The best feature engineering pipeline is the one that doesn’t exist.” If you can compute features on-the-fly without prohibitive cost, avoid serializing state. Your system will be simpler, more robust, and easier to debug.\nThis synthetic KMeans trick is a perfect example. You won’t find this in any Kaggle tutorial.\n12.2. Training/Serving Skew: The Silent Killer Huyen dedicates an entire section to this in Chapter 7. Training/serving skew is when preprocessing in training is different from serving.\nClassic example that kills projects:\n# In your training notebook df['total_rooms_log'] = np.log1p(df['total_rooms']) # 6 months later, someone implements the API # (without reading the complete notebook) features['total_rooms_log'] = np.log(features['total_rooms']) # BUG: log vs log1p # Result: Model fails silently # MAPE in training: 8% # MAPE in production: 24% # Why? Because log(0) = -inf, log1p(0) = 0 How this project avoids this:\nPreprocessing is encapsulated in ONE single class used BOTH in training and serving:\n# src/data/02_preprocessing/preprocessor.py class DataPreprocessor: def transform(self, df): # Imputation df = self._impute(df) # One-hot encoding df = pd.get_dummies(df, columns=['ocean_proximity']) return df # Used in training (Step 02) preprocessor = DataPreprocessor() train_processed = preprocessor.transform(train_raw) # SAME code used in API # api/app/core/preprocessor.py class HousingPreprocessor: # Same transform logic def transform(self, df): # Same one-hot encoding # Same column order return df The guarantee:\nIf you change preprocessing, both training and serving update because it’s the same code.\nThe anti-pattern:\n# Training: notebook_v3_FINAL.ipynb df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms'] # API: Someone copy/pastes without verifying features['bedrooms_per_room'] = features['total_bedrooms'] / features['total_rooms'] # What happens with division by zero? # What if total_rooms is 0? # In training it never happened because you cleaned outliers # In production... BOOM The mantra:\n“If you can’t import it, you can’t trust it.” If your preprocessing is copy/pasted between training and serving, you’ve already lost.\n12.3. Data Drift: The Enemy This Project (Still) Doesn’t Monitor Now let’s talk about what is NOT in this project but is critical for production systems.\nData drift is when the distribution of your features in production changes compared to training.\nHuyen covers this exhaustively in Chapter 8 (“Data Distribution Shifts”). There are three types:\n1. Covariate Shift (most common):\n# Training data (2020-2022) # Distribution of median_income P_train(median_income): mean = $6.2k, std = $3.1k # Production data (2023-2024) # After inflation + economic changes P_prod(median_income): mean = $8.5k, std = $4.2k # Result: # - Model was trained on features with mean=$6.2k # - Now receives features with mean=$8.5k # - Predictions become inaccurate 2. Label Shift:\n# Training: California 2020 # median_house_value average: $250k # Production: California 2024 # median_house_value average: $400k (real estate boom) # Model predicts based on 2020 relationships # But absolute prices changed 3. Concept Drift:\nThe relationship between features and target changes.\n# 2020: ocean_proximity='NEAR OCEAN' → +$50k in price # 2024: Work-from-home → people prefer INLAND → -$20k # Model's coefficient for 'NEAR OCEAN' is obsolete How to detect drift (what this project should add):\nOption 1: Statistical Tests (Kolmogorov-Smirnov, Chi-Square)\nfrom scipy.stats import ks_2samp # Compare training vs production distribution for feature in features: stat, p_value = ks_2samp( training_data[feature], production_data[feature] ) if p_value \u003c 0.05: alert(f\"DRIFT DETECTED in {feature}: p={p_value}\") Option 2: Evidently AI (recommended)\nfrom evidently.report import Report from evidently.metric_preset import DataDriftPreset report = Report(metrics=[DataDriftPreset()]) report.run( reference_data=train_df, # Training data current_data=production_df # Last 1000 predictions ) # Generate HTML dashboard with drift metrics report.save_html(\"drift_report.html\") Evidently calculates:\nDrift score per feature (0-1) Share of drifted features (% of features with drift) Dataset drift (if complete dataset drifted) Option 3: Population Stability Index (PSI)\nMetric used in banking to detect drift:\ndef calculate_psi(expected, actual, bins=10): \"\"\" PSI \u003c 0.1: No significant drift PSI \u003c 0.2: Moderate drift PSI \u003e= 0.2: Significant drift (retrain needed) \"\"\" breakpoints = np.quantile(expected, np.linspace(0, 1, bins+1)) expected_percents = np.histogram(expected, breakpoints)[0] / len(expected) actual_percents = np.histogram(actual, breakpoints)[0] / len(actual) psi = np.sum( (actual_percents - expected_percents) * np.log(actual_percents / expected_percents) ) return psi When to add drift detection:\nHuyen recommends waiting until you have sufficient production traffic (~10,000 predictions).\nDon’t add it on Day 1 because:\nYou need baseline of “normal production distribution” False positives at the beginning (people testing the API with synthetic data) Infrastructure overhead (Evidently requires DB to store histories) Add it when:\nYou have 10,000+ predictions in production You observe that production MAPE \u003e test set MAPE Model has \u003e6 months in production without retraining Example alerting:\n# W\u0026B logger extension (what you would add to wandb_logger.py) class WandBLogger: def log_drift_alert(self, feature_name, psi_value, threshold=0.2): if psi_value \u003e threshold: wandb.alert( title=f\"DATA DRIFT: {feature_name}\", text=f\"PSI={psi_value:.3f} exceeds threshold {threshold}\", level=wandb.AlertLevel.WARN ) # Log to metrics wandb.log({ f\"drift/{feature_name}\": psi_value, \"drift/timestamp\": datetime.now() }) The cost of NOT monitoring drift:\nWithout drift detection, your model fails silently. Nobody notices until:\nA customer complains: “Your predictions are very wrong lately” You calculate retrospective MAPE and discover it went from 8% to 18% 3 months passed serving garbage predictions With monitoring, you detect drift in days, not months.\n12.4. Model Monitoring: Beyond Accuracy This project’s W\u0026B Logger (api/app/core/wandb_logger.py) logs basic metrics:\nwandb.log({ \"prediction/count\": len(predictions), \"prediction/mean\": np.mean(predictions), \"performance/response_time_ms\": response_time }) This is a good start, but incomplete. In real production, you need to monitor:\n1. Business Metrics (most important) # How many predictions are \"very wrong\"? errors = np.abs(y_true - y_pred) / y_true within_10pct = (errors \u003c 0.10).mean() wandb.log({ \"business/predictions_within_10pct\": within_10pct, \"business/predictions_within_20pct\": (errors \u003c 0.20).mean(), \"business/mean_absolute_error_dollars\": np.mean(np.abs(y_true - y_pred)) }) # Alert if quality drops if within_10pct \u003c 0.65: # SLA threshold send_alert(\"Model quality degraded: only {:.1%} within 10%\".format(within_10pct)) 2. Prediction Distribution # Is the model always predicting the same value? # (signal of overfitting or broken model) prediction_std = np.std(predictions) prediction_range = np.max(predictions) - np.min(predictions) wandb.log({ \"prediction/std\": prediction_std, \"prediction/range\": prediction_range, \"prediction/median\": np.median(predictions) }) # Red flag: If std is very low if prediction_std \u003c 10000: # $10k alert(\"Model predictions have very low variance - model may be broken\") 3. Input Feature Distribution # Are you receiving inputs outside training range? for feature in NUMERIC_FEATURES: feature_values = [pred[feature] for pred in prediction_batch] wandb.log({ f\"input/{feature}/mean\": np.mean(feature_values), f\"input/{feature}/p95\": np.percentile(feature_values, 95), f\"input/{feature}/p05\": np.percentile(feature_values, 5) }) # Alert if there are extreme outliers if np.max(feature_values) \u003e TRAINING_MAX[feature] * 2: alert(f\"Extreme outlier detected in {feature}\") 4. Error Patterns # Does the model consistently fail on certain segments? errors_by_segment = {} # By geographic region for ocean_prox in ['\u003c1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY']: mask = (df['ocean_proximity'] == ocean_prox) errors_by_segment[ocean_prox] = mape(y_true[mask], y_pred[mask]) wandb.log({f\"error/mape_{seg}\": err for seg, err in errors_by_segment.items()}) # If ISLAND has MAPE = 40% but others have 8%, there's a problem 5. Latency Percentiles # Current logger only logs mean response time # But you need percentiles to detect outliers response_times = [...] # last 100 requests wandb.log({ \"latency/p50\": np.percentile(response_times, 50), \"latency/p95\": np.percentile(response_times, 95), \"latency/p99\": np.percentile(response_times, 99), \"latency/max\": np.max(response_times) }) # Alert if p99 exceeds threshold if np.percentile(response_times, 99) \u003e 200: # 200ms alert(\"API latency p99 exceeds 200ms\") Recommended dashboard (W\u0026B or Grafana):\n┌─────────────────────────────────────────────┐ │ MODEL HEALTH DASHBOARD │ ├─────────────────────────────────────────────┤ │ PREDICTIONS (last 24h) │ │ Total: 12,453 │ │ Within 10%: 68.2% [OK] │ │ Within 20%: 89.1% │ │ Mean MAPE: 9.8% [WARN] (threshold: 10%)│ ├─────────────────────────────────────────────┤ │ DRIFT DETECTION │ │ median_income: PSI = 0.08 [OK] │ │ total_rooms: PSI = 0.15 [WARN] │ │ ocean_proximity: PSI = 0.32 [ALERT] │ ├─────────────────────────────────────────────┤ │ LATENCY │ │ p50: 28ms │ │ p95: 67ms │ │ p99: 145ms [WARN] │ └─────────────────────────────────────────────┘ 12.5. The Cascade Pattern: Fallback Resilience This project implements a brilliant resilience pattern that Huyen discusses in Chapter 6: the Cascade Pattern (cascading fallback).\nLook at the ModelLoader in api/app/core/model_loader.py:\ndef load_model(self) -\u003e Any: \"\"\"Load model with cascade fallback strategy.\"\"\" # Priority 1: MLflow Registry (production) if self.mlflow_model_name: try: self._model = self.load_from_mlflow(...) return self._model except Exception as e: logger.warning(f\"MLflow load failed, trying GCS: {e}\") # Priority 2: GCS (staging) if self.gcs_bucket and self.gcs_model_path: try: self._model = self.load_from_gcs(...) return self._model except Exception as e: logger.warning(f\"GCS load failed, trying local: {e}\") # Priority 3: Local (development/fallback) if self.local_model_path and Path(self.local_model_path).exists(): self._model = self.load_from_local(self.local_model_path) return self._model raise RuntimeError(\"No model could be loaded from any source\") What does this achieve?\nResilience to failures:\nMLflow server down → API continues working with GCS GCS quota exceeded → API uses local model Zero downtime with degraded infrastructure Deployment flexibility:\nProduction: Uses MLflow (robust versioning) Staging: Uses GCS (simpler) Local development: Uses local file (no credentials) Same code, three environments:\n# Production docker run -e MLFLOW_MODEL_NAME=housing_price_model \\ -e MLFLOW_MODEL_STAGE=Production \\ housing-api # Staging docker run -e GCS_BUCKET=staging-bucket \\ -e GCS_MODEL_PATH=models/v1.2.pkl \\ housing-api # Local development docker run -v $(pwd)/models:/app/models \\ -e LOCAL_MODEL_PATH=/app/models/housing_price_model.pkl \\ housing-api What’s missing (and you should add):\n1. Circuit Breaker Pattern from circuitbreaker import circuit @circuit(failure_threshold=5, recovery_timeout=60) def load_from_mlflow(self, model_name, stage): \"\"\" Circuit breaker: If MLflow fails 5 consecutive times, open circuit for 60 seconds and don't attempt more calls. \"\"\" client = MlflowClient(self.tracking_uri) return mlflow.sklearn.load_model(f\"models:/{model_name}/{stage}\") Why: Without circuit breaker, if MLflow is down, API makes 1 request per prediction and waits for timeout (5-10s). With circuit breaker, detects failure after 5 attempts and stops calling until MLflow recovers.\n2. Retry with Exponential Backoff from tenacity import retry, stop_after_attempt, wait_exponential @retry( stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10) ) def load_from_gcs(self, bucket_name, blob_path): \"\"\" Retry with exponential backoff: - Attempt 1: immediate - Attempt 2: wait 2s - Attempt 3: wait 4s \"\"\" storage_client = storage.Client() bucket = storage_client.bucket(bucket_name) blob = bucket.blob(blob_path) return pickle.loads(blob.download_as_bytes()) Why: GCS can have transient failures (rate limiting, network blips). Automatic retry prevents a momentary failure from bringing down your API.\n3. Timeout Configuration # Currently there's no configured timeout # If MLflow takes 60s to respond, your API waits 60s # Better: def load_from_mlflow(self, model_name, stage, timeout=10): \"\"\"Load model with timeout.\"\"\" import signal def timeout_handler(signum, frame): raise TimeoutError(\"MLflow load exceeded timeout\") signal.signal(signal.SIGALRM, timeout_handler) signal.alarm(timeout) # 10 second timeout try: model = mlflow.sklearn.load_model(...) signal.alarm(0) # Cancel alarm return model except TimeoutError: logger.error(f\"MLflow load timeout after {timeout}s\") raise Why: Without timeout, a slow MLflow server can make your API take minutes to respond. With timeout, fail fast and try the next fallback.\n4. Health Check Endpoint # api/app/routers/health.py @router.get(\"/health/deep\") async def deep_health_check(): \"\"\" Health check that verifies all dependencies. Kubernetes calls this every 30s for routing decisions. \"\"\" health = { \"status\": \"healthy\", \"model_loaded\": model_loader.is_loaded, \"model_version\": model_loader.model_version, \"dependencies\": {} } # Check MLflow try: client = MlflowClient(settings.MLFLOW_TRACKING_URI) client.list_experiments(max_results=1) health[\"dependencies\"][\"mlflow\"] = \"healthy\" except Exception as e: health[\"dependencies\"][\"mlflow\"] = f\"degraded: {e}\" health[\"status\"] = \"degraded\" # Check GCS try: storage_client = storage.Client() bucket = storage_client.bucket(settings.GCS_BUCKET) bucket.exists() health[\"dependencies\"][\"gcs\"] = \"healthy\" except Exception as e: health[\"dependencies\"][\"gcs\"] = f\"degraded: {e}\" return health Output:\n{ \"status\": \"degraded\", \"model_loaded\": true, \"model_version\": \"models:/housing_price_model/Production\", \"dependencies\": { \"mlflow\": \"degraded: Connection timeout\", \"gcs\": \"healthy\" } } Why: Tells your load balancer (Cloud Run, Kubernetes) if the API is healthy. If MLflow is down but model is already loaded (cached), API is “degraded” but functional.\n12.6. Feature Store Anti-Pattern: When You DON’T Need One Huyen has a controversial section in Chapter 5: “You might not need a feature store.”\nFeature Stores (Feast, Tecton, Databricks) are very popular, but are overkill for 80% of projects.\nWhen you DO need a Feature Store:\nYou reuse features across multiple models\nExample: customer_lifetime_value is used in 10 different models Without feature store: Each model recalculates the same feature (waste) With feature store: Calculate once, serve many times You need features with different freshness\nBatch features: Calculated daily (credit score) Real-time features: Calculated per request (current location) Feature store orchestrates both Training/Serving skew is critical\nFeature store guarantees training and serving use EXACTLY the same logic When you DON’T need a Feature Store (like this project):\nAll features are computed on-the-fly\nThis project: Features are straightforward (lat, lon, income, age) Only computed feature is cluster_label (2ms latency) No complex aggregations like “average income in last 30 days” Single model consumes the features\nNo reuse across models Feature store would add complexity without benefit Latency budget is generous\nThis API: \u003c50ms is OK If you needed \u003c5ms, pre-computing features would be worthwhile Real cost of a Feature Store:\nInfrastructure: Redis/DynamoDB for serving, Spark for batch processing Cost: ~$500-2000/month on AWS/GCP (depending on traffic) Complexity: Another system to monitor, debug, operate Lightweight alternative (what this project does):\n# Compute features on-the-fly in the API class HousingPreprocessor: def transform(self, df): # 1. One-hot encoding (instantaneous) df_encoded = pd.get_dummies(df, columns=['ocean_proximity']) # 2. Clustering (2ms with pre-fitted KMeans) clusters = self.kmeans.predict(df[['longitude', 'latitude']]) df_encoded['cluster_label'] = clusters return df_encoded Total latency: ~3ms. Doesn’t justify a Feature Store.\nWhen to reconsider:\nIf you add features like “average house price in zipcode” (requires DB query) If preprocessing goes above \u003e20ms If you add a second model that reuses 50%+ of features Until then, YAGNI (You Ain’t Gonna Need It).\n12.7. Production Readiness: An Honest Checklist Based on exhaustive code analysis, here’s the real state of this project:\nWhat This Project Does VERY WELL Level 3/5 in MLOps Maturity (Production-Ready):\nComplete versioning\nModels in MLflow Registry with rich metadata Data artifacts in GCS with timestamps Code in git with CI/CD Config in versioned YAML Reproducibility\nFixed seeds (random_state=42 everywhere) Pinned dependencies (requirements.txt) Docker for environment consistency Testing\n87% code coverage Unit tests with realistic fixtures End-to-end integration tests Security scanning (Bandit, TruffleHog) CI/CD\nGitHub Actions with automated tests Docker build in CI Cloud Run deployment with health checks Staging/Production separation API Design\nPydantic validation on all endpoints Cascade fallback (MLflow→GCS→Local) Lifespan management (load model once, not per request) Batch prediction support Observability (Basic)\nW\u0026B logging of predictions Response time tracking Structured logging What’s Missing (And When To Add It) Level 4/5 Features (Add When You Have 10k+ Daily Predictions):\nData Drift Detection [MISSING]\nImpact: High (model fails silently) Implementation cost: Medium (Evidently AI) When: After 3 months in production Model Performance Tracking [MISSING]\nImpact: High (don’t know if model degrades) Cost: Low (extend W\u0026B logger) When: After having ground truth labels (1-2 months) Circuit Breakers [MISSING]\nImpact: Medium (better latency during failures) Cost: Low (circuitbreaker library) When: If you see transient failures in MLflow/GCS Advanced Monitoring Dashboards [MISSING]\nImpact: Medium (better debugging) Cost: Medium (Grafana + Prometheus) When: When team grows \u003e5 people Canary Deployments [MISSING]\nImpact: Low (you have manual rollback that works) Cost: High (requires traffic splitting) When: Only if deploying \u003e1x/week Feature Store [MISSING]\nImpact: None (features are lightweight) Cost: High ($500-2000/month) When: Never, unless you add heavy features Level 5/5 Features (Overkill For This Project):\nMulti-model orchestration (A/B testing) Real-time retraining Federated learning AutoML pipeline Prioritized Recommendations MONTH 1-3 (Stabilization):\nAdd /health/deep endpoint with dependency checks Implement retry with exponential backoff on GCS calls Configure alerts in W\u0026B when MAPE \u003e 12% MONTH 4-6 (Monitoring):\nImplement Evidently AI for data drift (PSI tracking) Add prediction distribution monitoring Configure automated retraining trigger when PSI \u003e 0.2 MONTH 7-12 (Optimization):\nImplement circuit breaker on MLflow calls Add Redis for prediction caching (if latency is an issue) Configure Grafana dashboard for business metrics DON’T Do (Until You Scale 10x):\nDon’t implement Feature Store Don’t add Kafka streaming Don’t use Kubernetes (Cloud Run is sufficient) Don’t implement multi-model serving (until clear use case) 12.8. The Difference Between “Works” and “Works in Production” This project is in the top 10% of ML projects in terms of engineering practices. Most models in production have:\nNotebooks instead of modular scripts Models saved as model_v3_FINAL_FINAL.pkl Zero tests Manual deployment with scp No monitoring This project has:\nModular and testable code MLflow Registry with semantic versioning 87% test coverage Automated deployment with GitHub Actions Basic W\u0026B monitoring The remaining gap (drift detection, advanced monitoring, circuit breakers) is the gap between “stable production” and “enterprise-grade production”.\nBut here’s the secret: that gap only matters when you have real users and significant traffic.\nDon’t optimize for problems you don’t have yet. This project is ready to serve 100k predictions/month without breaking a sweat. When you reach 1M/month, then add data drift detection. When you reach 10M/month, then consider Kubernetes.\nAs Huyen says: “The best ML system is the simplest one that meets your requirements.”\nThis project fulfills that principle perfectly.\n14. Conclusions: MLOps As an Engineering Discipline What This Pipeline Implements (And Why It Matters) This is not a scikit-learn tutorial. It’s a production-ready system that implements:\nComplete versioning: Data (GCS), code (git), models (MLflow), configuration (YAML) Reproducibility: Same code + same config + same seed = same model Observability: Structured logs, metrics in W\u0026B, tracking in MLflow Testing: 87% coverage, unit tests, integration tests, security scanning CI/CD: GitHub Actions with automated deployment to Cloud Run Deployment: REST API with FastAPI, frontend with Streamlit, Docker Compose ready Data-backed decisions: Every choice (imputation, K clusters, hyperparameters) has quantifiable metrics Production patterns: Transform pattern, cascade fallback, training/serving consistency The Anti-Patterns It Avoids (That Kill Projects) X Notebooks in production: Everything is modular and testable Python. Notebooks are great for exploration, terrible for reliable systems.\nX Hardcoded configuration: config.yaml versioned in git. If you change a parameter, it’s recorded with timestamp and author.\nX “I used median because yes”: Compared 4 imputation strategies with quantifiable metrics. Best strategy (Iterative Imputer) won by 3.2% in RMSE.\nX Models as final_v3_REAL_final.pkl: MLflow Registry with semantic versions and rich metadata. You know exactly what hyperparameters, what data, and what metrics each version has.\nX “I don’t know what hyperparameters I used 3 months ago”: Each model records 106 lines of metadata. Includes everything from hyperparameters to error distribution by segment.\nX Manual deployment with scp: Docker + GitHub Actions. Push to master → tests run → if they pass, deploys to staging automatically. Production requires manual approval (as it should).\nX Training/Serving Skew: Preprocessing is in a shared class between training and serving. Change code once, both environments update.\nConscious Trade-Offs (Because There Are No Perfect Solutions) This project makes deliberate decisions. Here are the trade-offs and when to reconsider them:\n1. Cluster optimization independent of final model:\nOptimizes KMeans with silhouette score instead of cross-validation of complete model. Faster but less rigorous. Reconsider if clustering is your model’s most important feature.\n2. 60 sweep runs in W\u0026B:\nSufficient for California Housing (medium dataset, ~20k samples). You might need 200+ runs on complex datasets with many non-linear interactions.\n3. Sequential pipeline without parallelization:\nSteps run one after another. This pipeline takes ~15 minutes end-to-end. If your pipeline takes hours, use Airflow/Prefect with parallel tasks.\n4. MAPE as primary metric:\nWorks for this dataset (prices between $50k-$500k). Doesn’t work if you have values close to zero (division by zero) or if you want to penalize large errors disproportionately (use RMSE).\n5. Data drift detection absent:\nAs the Production Checklist explains (Section 13.7), drift monitoring should be added after 3-6 months in production, not Day 1. You need baseline of normal behavior first.\n6. Synthetic KMeans in the API:\nThe Transform Pattern (Section 13.1) recreates clusters with ~2% drift vs training. Impact on MAPE: \u003c0.3%. If you need 100% bit-by-bit reproducibility, serialize the real KMeans (cost: 96KB per model version).\nWhat’s Missing (And When To Add It) As Section 13 (Production Patterns) details, this project is at Level 3/5 of MLOps Maturity. What’s missing:\nMonth 1-3 (Stabilization):\nDeep health check endpoint with dependency status Retry with exponential backoff on GCS calls Automatic alerts in W\u0026B when MAPE \u003e threshold Month 4-6 (Monitoring):\nEvidently AI for data drift detection (PSI tracking) Prediction distribution monitoring (detect broken model) Automatic retraining trigger when PSI \u003e 0.2 Month 7-12 (Optimization):\nCircuit breaker on MLflow calls (avoid cascading timeouts) Redis for prediction caching (if latency \u003c10ms is critical) Grafana dashboards for business metrics DON’T do (until you scale 10x):\nFeature Store (features are lightweight, \u003c3ms) Kafka streaming (Cloud Run with HTTP is sufficient) Kubernetes (Cloud Run autoscales without complexity) Multi-model A/B testing (until clear use case) The Uncomfortable Truth About MLOps 90% of ML models never reach production. Of those that do, 60% fail in the first 6 months.\nWhy?\nNot because the models are bad. It’s because:\nThe engineer who trained the model is no longer at the company Nobody knows what hyperparameters were used Preprocessing in production is different from training There are no tests, so every change breaks something Deployment is manual, takes 3 hours and fails 1 out of 3 times There’s no monitoring, model fails silently for months This project avoids all those problems. Not because it’s perfect, but because it implements basic software engineering principles:\nVersioning: Everything (data, code, models, config) Testing: 87% coverage, CI on each commit Reproducibility: Fixed seeds, Dockerized environments Observability: Logs, metrics, tracking Automation: Deployment without human intervention The Most Important Lesson Chip Huyen says it better than I can in “Designing Machine Learning Systems”:\n“The best ML system is not the one with the highest accuracy. It’s the one that’s reliable, maintainable, and meets business requirements.”\nThis project doesn’t have the best model. You can probably improve MAPE from 8.2% to 7.5% with hand-tuned XGBoost.\nBut that doesn’t matter.\nWhat matters is that this system:\nRuns reliably 24/7 Can be updated without downtime Has automatic rollback if something fails Any team member can understand and modify the code Logs enough information to debug problems Costs \u003c$100/month on GCP (up to 1M predictions) That 0.7% improvement in MAPE isn’t worth it if the system is impossible to maintain.\nWho This Post Is For If you are:\nData Scientist trying to take your first model to production → This is your roadmap ML Engineer explaining why “you can’t just deploy the notebook” → Send this post Engineering Manager evaluating if your team does MLOps correctly → Use Section 13.7 as checklist Student wanting to learn MLOps beyond tutorials → This is real code, not synthetic The Next Step This post has 6,500+ lines because I didn’t want to simplify. MLOps is complex. There are trade-offs in every decision.\nBut don’t let complexity paralyze you. Start simple, iterate, improve.\nWeek 1: Basic versioning (git + requirements.txt) Week 2: Basic tests (at least smoke tests) Week 3: Docker for consistent deployment Week 4: Basic CI (GitHub Actions running tests) Month 2: MLflow for model registry Month 3: Basic monitoring (W\u0026B or Prometheus) You don’t need to implement everything on Day 1. This project took months to reach this state.\nThe Last Word Being an MLOps engineer is not just training models—it’s building systems where models are one more piece.\nWhat separates a research project from a production product is:\nOrder: Everything in its place (not “works on my machine”) Testing: What isn’t tested, breaks (87% coverage is not an accident) Observability: If you can’t measure it, you can’t improve it (W\u0026B + MLflow) Reproducibility: Today and in 6 months should give the same result (fixed seeds, Docker) Automation: Humans are bad at repetitive tasks (CI/CD) Humility: Recognizing what’s missing and when to add it (Section 13.7) This post doesn’t teach you to be better at machine learning.\nIt teaches you to be better at machine learning engineering.\nAnd that difference is what separates models in notebooks from models in production creating real value.\nIf you implement even 50% of what’s in this post, your pipeline will be in the top 10% of ML projects in terms of engineering practices.\nIf you implement 80%, you’ll be ready to scale to millions of predictions without restructuring everything.\n100% is overkill for most projects. Use the Production Checklist (Section 13.7) to prioritize what you need and when.\nReferences and Resources Fundamental books:\nGéron, A. (2022). Hands-On Machine Learning with Scikit-Learn, Keras \u0026 TensorFlow (3rd ed.). O’Reilly. Chapter 2: Base of this project (California Housing dataset, feature engineering, model selection) Focus on ML, this post adds production infrastructure Huyen, C. (2022). Designing Machine Learning Systems. O’Reilly. Chapter 5: Feature stores and when you don’t need one Chapter 6: Deployment patterns (Cascade, Circuit Breaker) Chapter 7: Transform Pattern and Training/Serving Skew (Sections 13.1 and 13.2 of this post) Chapter 8: Data Distribution Shifts and drift detection (Section 13.3) Complete book: If you only read one book about MLOps, make it this one Tools (with links to docs):\nMLflow: Model registry and experiment tracking Weights \u0026 Biases: Sweep and experiment visualization Hydra: Configuration management with composable configs FastAPI: REST API framework with Pydantic validation Streamlit: Interactive frontend for ML apps Google Cloud Storage: Artifact storage Evidently AI: Data drift detection (recommended for production) Docker: Containerization and reproducibility Complete repository:\ngithub.com/carlosjimenez88M/mlops-hand-on-ML-and-pytorch /api: FastAPI with cascade fallback and Transform Pattern /src: Modular pipeline (01-07) with MLflow tracking /tests: 87% coverage with realistic fixtures /.github/workflows: Complete CI/CD with security scanning Author: Carlos Daniel Jiménez Email: danieljimenez88m@gmail.com LinkedIn: linkedin.com/in/carlosdanieljimenez Date: January 2026\nNavigation ← Part 2: Deployment and Infrastructure | ← Part 1: Pipeline and Orchestration\nComplete series:\nPart 1: Pipeline and Orchestration Part 2: Deployment and Infrastructure Part 3: Production and Best Practices (current) Repository: github.com/carlosjimenez88M/mlops-hand-on-ML-and-pytorch\n","wordCount":"6866","inLanguage":"en","datePublished":"2026-01-13T00:00:00Z","dateModified":"2026-01-13T00:00:00Z","author":{"@type":"Person","name":"Carlos Daniel Jiménez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://carlosdanieljimenez.com/post/anatomia-pipeline-mlops-part-3-en/"},"publisher":{"@type":"Organization","name":"The Probability Engine","logo":{"@type":"ImageObject","url":"https://carlosdanieljimenez.com/img/icon.jpeg"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://carlosdanieljimenez.com/ accesskey=h title="The Probability Engine (Alt + H)">The Probability Engine</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://carlosdanieljimenez.com/post/ title=Posts><span>Posts</span></a></li><li><a href=https://carlosdanieljimenez.com/about/ title=About><span>About</span></a></li><li><a href=https://carlosdanieljimenez.com/archive/ title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices</h1><div class=post-description>Part 3: Model selection strategies, advanced testing, production patterns, data drift, model monitoring, and production readiness checklist.</div><div class=post-meta><span title='2026-01-13 00:00:00 +0000 UTC'>January 13, 2026</span>&nbsp;·&nbsp;<span>Carlos Daniel Jiménez</span></div></header><div class=post-content><blockquote><p><strong>Complete MLOps Series:</strong> <a href=/post/anatomia-pipeline-mlops-part-1-en/>← Part 1: Pipeline</a> | <a href=/post/anatomia-pipeline-mlops-part-2-en/>← Part 2: Deployment</a> | <strong>Part 3 (current)</strong></p></blockquote><h1 id=anatomy-of-an-mlops-pipeline---part-3-production-and-best-practices>Anatomy of an MLOps Pipeline - Part 3: Production and Best Practices<a hidden class=anchor aria-hidden=true href=#anatomy-of-an-mlops-pipeline---part-3-production-and-best-practices>#</a></h1><p><a name=model-strategies></a></p><h2 id=11-model-and-parameter-selection-strategies>11. Model and Parameter Selection Strategies<a hidden class=anchor aria-hidden=true href=#11-model-and-parameter-selection-strategies>#</a></h2><h3 id=the-complete-flow-selection--sweep--registration>The Complete Flow: Selection → Sweep → Registration<a hidden class=anchor aria-hidden=true href=#the-complete-flow-selection--sweep--registration>#</a></h3><p>This pipeline implements a <strong>three-phase strategy</strong> for model optimization, each with a specific purpose:</p><pre tabindex=0><code>Step 05: Model Selection
├── Compares 5 algorithms with basic GridSearch (5-10 combos/model)
├── Objective: Identify best model family (Random Forest vs Gradient Boosting vs ...)
├── Primary metric: MAPE (Mean Absolute Percentage Error)
└── Output: Best algorithm + initial parameters

Step 06: Hyperparameter Sweep
├── Optimizes ONLY the best algorithm from Step 05
├── Bayesian optimization with 50+ runs (exhaustive search space)
├── Objective: Find optimal configuration of best model
├── Primary metric: wMAPE (Weighted MAPE, less biased)
└── Output: best_params.yaml with optimal hyperparameters

Step 07: Model Registration
├── Trains final model with parameters from Step 06
├── Registers in MLflow Model Registry with rich metadata
├── Transitions to stage (Staging/Production)
└── Output: Versioned model ready for deployment
</code></pre><p><strong>Why three separate steps?</strong> You don&rsquo;t have computational resources to do exhaustive sweep of 5 algorithms × 50 combinations = 250 training runs. First decide <strong>strategy</strong> (which algorithm), then <strong>tactics</strong> (which hyperparameters).</p><hr><h3 id=step-05-model-selection---algorithm-comparison>Step 05: Model Selection - Algorithm Comparison<a hidden class=anchor aria-hidden=true href=#step-05-model-selection---algorithm-comparison>#</a></h3><h4 id=the-5-candidate-models>The 5 Candidate Models<a hidden class=anchor aria-hidden=true href=#the-5-candidate-models>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_available_models</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Get dictionary of available regression models.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>models</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;RandomForest&#34;</span><span class=p>:</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;GradientBoosting&#34;</span><span class=p>:</span> <span class=n>GradientBoostingRegressor</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Ridge&#34;</span><span class=p>:</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Lasso&#34;</span><span class=p>:</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DecisionTree&#34;</span><span class=p>:</span> <span class=n>DecisionTreeRegressor</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>models</span>
</span></span></code></pre></div><p><strong>Why these models:</strong></p><ol><li><strong>RandomForest</strong>: Tree ensemble, robust, handles non-linearities</li><li><strong>GradientBoosting</strong>: Sequential boosting, better precision than RF but slower</li><li><strong>Ridge</strong>: Linear regression with L2 regularization, fast, interpretable</li><li><strong>Lasso</strong>: Linear regression with L1 regularization, does feature selection</li><li><strong>DecisionTree</strong>: Simple baseline, useful for comparison</li></ol><p><strong>What&rsquo;s missing (deliberately):</strong></p><ul><li><strong>XGBoost/LightGBM</strong>: Not included to reduce dependencies, but easy to add</li><li><strong>Neural Networks</strong>: Overkill for this problem (20k samples, tabular features)</li><li><strong>SVR</strong>: Very slow on large datasets, doesn&rsquo;t scale well</li></ul><h4 id=parameter-grids-initial-gridsearch>Parameter Grids: Initial GridSearch<a hidden class=anchor aria-hidden=true href=#parameter-grids-initial-gridsearch>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_default_param_grids</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>list</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameter grids for initial model selection.
</span></span></span><span class=line><span class=cl><span class=s2>    Refined based on domain knowledge.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>param_grids</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;RandomForest&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>300</span><span class=p>],</span>         <span class=c1># 4 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=kc>None</span><span class=p>],</span>         <span class=c1># 5 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span>             <span class=c1># 3 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;min_samples_leaf&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>               <span class=c1># 3 options</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=c1># Total combinations: 4×5×3×3 = 180</span>
</span></span><span class=line><span class=cl>        <span class=c1># With 5-fold CV: 180×5 = 900 fits</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;GradientBoosting&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>150</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>         <span class=c1># 4 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;learning_rate&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>],</span> <span class=c1># 5 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>                <span class=c1># 5 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;subsample&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>],</span>                <span class=c1># 3 options</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=c1># Total: 4×5×5×3 = 300 combinations</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Ridge&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;alpha&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>5.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>,</span> <span class=mf>50.0</span><span class=p>,</span> <span class=mf>100.0</span><span class=p>,</span> <span class=mf>500.0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=c1># Total: 9 combinations (fast)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Lasso&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;alpha&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>5.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>,</span> <span class=mf>50.0</span><span class=p>,</span> <span class=mf>100.0</span><span class=p>,</span> <span class=mf>500.0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=c1># Total: 9 combinations</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DecisionTree&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=kc>None</span><span class=p>],</span>      <span class=c1># 6 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>],</span>         <span class=c1># 4 options</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;min_samples_leaf&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>],</span>            <span class=c1># 4 options</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1># Total: 6×4×4 = 96 combinations</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>param_grids</span>
</span></span></code></pre></div><h4 id=grid-design-decisions>Grid Design Decisions<a hidden class=anchor aria-hidden=true href=#grid-design-decisions>#</a></h4><p><strong>1. RandomForest: Focus on Overfitting Control</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=kc>None</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;min_samples_leaf&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span></code></pre></div><p><strong>Reasoning:</strong> Random Forest tends to overfit on small datasets. <code>max_depth</code> and <code>min_samples_leaf</code> control tree depth—high values prevent the model from memorizing noise.</p><p><strong>None in max_depth:</strong> Allows unlimited depth trees. Useful when the dataset has complex patterns requiring deep splits.</p><p><strong>2. GradientBoosting: Balance Learning Rate vs N_estimators</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>150</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;learning_rate&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>],</span>
</span></span></code></pre></div><p><strong>Classic trade-off:</strong></p><ul><li><strong>Low learning rate (0.01) + many estimators (200):</strong> Slow but accurate learning</li><li><strong>High learning rate (0.2) + few estimators (50):</strong> Fast but may diverge</li></ul><p>GridSearch explores both extremes.</p><p><strong>subsample &lt; 1.0:</strong> Stochastic Gradient Boosting. Only uses 80-90% of data in each iteration, reduces overfitting.</p><p><strong>3. Ridge/Lasso: Alpha in Logarithmic Scale</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;alpha&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>5.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>,</span> <span class=mf>50.0</span><span class=p>,</span> <span class=mf>100.0</span><span class=p>,</span> <span class=mf>500.0</span><span class=p>],</span>
</span></span></code></pre></div><p>Alpha controls regularization:</p><ul><li><strong>Low alpha (0.01):</strong> Almost no regularization, complex model</li><li><strong>High alpha (500):</strong> Strong regularization, simple model (coefficients close to 0)</li></ul><p>Logarithmic scale covers the space more uniformly than linear scale.</p><p><strong>Lasso vs Ridge:</strong></p><ul><li><strong>Lasso (L1):</strong> Forces coefficients to <strong>exactly 0</strong> → automatic feature selection</li><li><strong>Ridge (L2):</strong> Small coefficients but <strong>not zero</strong> → keeps all features</li></ul><p>If Lasso wins, it indicates some features are noise.</p><p><strong>4. DecisionTree: Comparison Baseline</strong></p><p>DecisionTree is the worst model (high variance, overfits easily), but serves to:</p><ul><li>Verify that the pipeline works correctly</li><li>Comparison baseline: If Ridge/Lasso don&rsquo;t beat DecisionTree, something&rsquo;s wrong in feature engineering</li></ul><h4 id=training-function-with-gridsearch>Training Function with GridSearch<a hidden class=anchor aria-hidden=true href=#training-function-with-gridsearch>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_model_with_gridsearch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=p>:</span> <span class=n>Any</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>param_grid</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>list</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>X_train</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>y_train</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cv</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>Any</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=nb>float</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>float</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Train model with K-fold Cross-Validation via GridSearchCV.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>estimator</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>param_grid</span><span class=o>=</span><span class=n>param_grid</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>  <span class=c1># 5-fold cross-validation</span>
</span></span><span class=line><span class=cl>        <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;neg_mean_absolute_error&#39;</span><span class=p>,</span>  <span class=c1># CRITICAL</span>
</span></span><span class=line><span class=cl>        <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>  <span class=c1># Parallelization</span>
</span></span><span class=line><span class=cl>        <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>return_train_score</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># To detect overfitting</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>training_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Extract cross-validation results</span>
</span></span><span class=line><span class=cl>    <span class=n>cv_metrics</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;mean_test_score&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=o>-</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_score_</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;std_test_score&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>grid_search</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>[</span><span class=s1>&#39;std_test_score&#39;</span><span class=p>][</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_index_</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;mean_train_score&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=o>-</span><span class=n>grid_search</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>[</span><span class=s1>&#39;mean_train_score&#39;</span><span class=p>][</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_index_</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;std_train_score&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>grid_search</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>[</span><span class=s1>&#39;std_train_score&#39;</span><span class=p>][</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_index_</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_estimator_</span><span class=p>,</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=p>,</span> <span class=n>training_time</span><span class=p>,</span> <span class=n>cv_metrics</span>
</span></span></code></pre></div><h4 id=critical-decisions>Critical Decisions<a hidden class=anchor aria-hidden=true href=#critical-decisions>#</a></h4><p><strong>1. Scoring: neg_mean_absolute_error</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;neg_mean_absolute_error&#39;</span>
</span></span></code></pre></div><p><strong>Why MAE and not RMSE or R²?</strong></p><ul><li><strong>MAE (Mean Absolute Error)</strong>: Penalizes errors linearly</li><li><strong>RMSE</strong>: Penalizes errors quadratically (large errors weigh much more)</li><li><strong>R²</strong>: Relative metric, difficult to interpret in business terms</li></ul><p>For this problem:</p><ul><li>MAE = $15,000 → &ldquo;The model is off by $15k on average&rdquo;</li><li>R² = 0.85 → What does this mean for the business?</li></ul><p><strong>neg_mean_absolute_error:</strong> GridSearchCV minimizes the metric, but MAE should be minimized, so we use the negative.</p><p><strong>2. Cross-Validation: 5 Folds</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>cv</span><span class=o>=</span><span class=mi>5</span>
</span></span></code></pre></div><p><strong>Why 5 and not 10?</strong></p><ul><li><p><strong>5-fold:</strong> Balance between bias and variance</p><ul><li>Each fold has 80% training, 20% validation</li><li>Faster than 10-fold (2x fewer fits)</li></ul></li><li><p><strong>10-fold:</strong> Less bias but higher computational cost</p><ul><li>Useful when you have few samples (&lt;1000 samples)</li></ul></li></ul><p>With 16,512 training samples, 5-fold is sufficient.</p><p><strong>3. return_train_score=True</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>return_train_score</span><span class=o>=</span><span class=kc>True</span>
</span></span></code></pre></div><p>This logs the score on <strong>training set</strong> in addition to validation set. Allows detecting overfitting:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=n>cv_metrics</span><span class=p>[</span><span class=s1>&#39;mean_train_score&#39;</span><span class=p>]</span> <span class=o>&gt;&gt;</span> <span class=n>cv_metrics</span><span class=p>[</span><span class=s1>&#39;mean_test_score&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;WARNING: Model is overfitting!&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Train MAE = $5k, Test MAE = $20k → Clear overfitting</span>
</span></span></code></pre></div><p><strong>4. n_jobs=-1: Parallelization</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
</span></span></code></pre></div><p>Uses all available CPU cores. On an 8-core machine, 180 combinations × 5 folds = 900 fits are distributed in parallel.</p><p><strong>Without parallelization:</strong> 900 fits × 2s/fit = 30 minutes
<strong>With 8 cores:</strong> ~4 minutes</p><h4 id=evaluation-metrics-beyond-mape>Evaluation Metrics: Beyond MAPE<a hidden class=anchor aria-hidden=true href=#evaluation-metrics-beyond-mape>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>evaluate_model</span><span class=p>(</span><span class=n>model</span><span class=p>:</span> <span class=n>Any</span><span class=p>,</span> <span class=n>X_test</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>,</span> <span class=n>y_test</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>float</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Evaluates model with business-focused metrics.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_true</span> <span class=o>=</span> <span class=n>y_test</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Traditional metrics</span>
</span></span><span class=line><span class=cl>    <span class=n>mae</span> <span class=o>=</span> <span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Business-focused percentage error metrics</span>
</span></span><span class=line><span class=cl>    <span class=n>mape</span> <span class=o>=</span> <span class=n>mean_absolute_percentage_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>smape</span> <span class=o>=</span> <span class=n>symmetric_mean_absolute_percentage_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wmape</span> <span class=o>=</span> <span class=n>weighted_mean_absolute_percentage_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>median_ape</span> <span class=o>=</span> <span class=n>median_absolute_percentage_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Prediction accuracy at different thresholds</span>
</span></span><span class=line><span class=cl>    <span class=n>within_5pct</span> <span class=o>=</span> <span class=n>predictions_within_threshold</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>within_10pct</span> <span class=o>=</span> <span class=n>predictions_within_threshold</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=mf>0.10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>within_15pct</span> <span class=o>=</span> <span class=n>predictions_within_threshold</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;mae&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>mae</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;rmse&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>rmse</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;r2&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>r2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;mape&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>mape</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;smape&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>smape</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;wmape&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>wmape</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;median_ape&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>median_ape</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;within_5pct&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>within_5pct</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;within_10pct&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>within_10pct</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;within_15pct&#34;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>within_15pct</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p><strong>Why 4 Variants of MAPE:</strong></p><p><strong>1. MAPE (Mean Absolute Percentage Error)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>mape</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>((</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=n>y_true</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>
</span></span></code></pre></div><p><strong>Problem:</strong> Biased towards low values.</p><p>If you predict $500k instead of $510k → error = 2%
If you predict $10k instead of $11k → error = 9%</p><p>Both are $10k absolute error, but MAPE penalizes the second more.</p><p><strong>2. SMAPE (Symmetric MAPE)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>smape</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=p>((</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_pred</span><span class=p>))</span> <span class=o>/</span> <span class=mi>2</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>
</span></span></code></pre></div><p>Uses the average of <code>y_true</code> and <code>y_pred</code> in the denominator. More symmetric:</p><ul><li>Overprediction and underprediction have similar weight</li><li>Range: 0-200% (vs 0-∞% for MAPE)</li></ul><p><strong>3. wMAPE (Weighted MAPE)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>wmape</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>))</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>
</span></span></code></pre></div><p>Total sum of errors divided by total sum of actual values. Not affected by individual extreme values.</p><p><strong>Used in Step 06 (Sweep)</strong> because it&rsquo;s more robust than MAPE for datasets with high variance.</p><p><strong>4. Median APE</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>median_ape</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>median</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>((</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=n>y_true</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>
</span></span></code></pre></div><p>Median instead of mean. Robust to outliers.</p><p>If 95% of predictions have &lt;5% error but 5% have >50% error:</p><ul><li><strong>MAPE:</strong> ~7% (average includes outliers)</li><li><strong>Median APE:</strong> ~4% (outliers don&rsquo;t affect median)</li></ul><p><strong>Within-X% Metrics</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>within_5pct</span> <span class=o>=</span> <span class=n>predictions_within_threshold</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Percentage of predictions with error &lt;5%</span>
</span></span></code></pre></div><p><strong>Business interpretation:</strong> &ldquo;75% of our predictions are within ±10% of actual value.&rdquo;</p><p>More interpretable for stakeholders than &ldquo;MAPE = 8.2%&rdquo;.</p><h4 id=step-05-output>Step 05 Output<a hidden class=anchor aria-hidden=true href=#step-05-output>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34; BEST MODEL: RandomForestRegressor&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Business Metrics (Test Set):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  MAPE (Mean APE): 8.23%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  SMAPE (Symmetric MAPE): 7.95%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  wMAPE (Weighted MAPE): 8.01%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  Median APE: 6.45%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  Within ±5%: 45.2%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  Within ±10%: 72.8%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  Within ±15%: 85.3%&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Traditional Metrics (Test Set):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  R²: 0.8654&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  RMSE: $48,234.12&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  MAE: $32,456.78&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Cross-Validation Results (5-fold):&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  Mean CV MAE: $33,125.45 (±$2,341.23)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;  Mean CV Train MAE: $28,934.56 (±$1,892.34)&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Best params saved:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>best_params</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;min_samples_leaf&#34;</span><span class=p>:</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>These params are used as a <strong>starting point</strong> for Step 06 (exhaustive Sweep).</p><hr><h3 id=what-this-strategy-achieves>What This Strategy Achieves<a hidden class=anchor aria-hidden=true href=#what-this-strategy-achieves>#</a></h3><p><strong>Without model selection:</strong></p><ul><li>&ldquo;I used Random Forest because everyone uses it&rdquo;</li><li>You have no evidence it&rsquo;s better than Gradient Boosting</li></ul><p><strong>With model selection:</strong></p><ul><li>&ldquo;I compared 5 algorithms with 5-fold CV. Random Forest achieved MAPE=8.2% (vs GradientBoosting=8.9%, Ridge=12.3%). Here&rsquo;s the comparison table in W&amp;B.&rdquo;</li><li><strong>Data-backed decision, not intuition.</strong></li></ul><hr><p><a name=testing></a></p><h2 id=11-testing-fixtures-mocking-and-real-coverage>11. Testing: Fixtures, Mocking and Real Coverage<a hidden class=anchor aria-hidden=true href=#11-testing-fixtures-mocking-and-real-coverage>#</a></h2><h3 id=why-testing-ml-is-different>Why Testing ML Is Different<a hidden class=anchor aria-hidden=true href=#why-testing-ml-is-different>#</a></h3><p>Tests in ML are not like tests in web apps. You can&rsquo;t do:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_model_predicts_correct_value</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>load_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]])</span> <span class=o>==</span> <span class=mf>452600.0</span>  <span class=c1># ERROR: This is absurd</span>
</span></span></code></pre></div><p>ML models are <strong>probabilistic</strong>. The output is not deterministic in the traditional software sense.</p><p><strong>What you CAN test:</strong></p><ol><li><strong>Data contracts:</strong> Inputs/outputs have correct types</li><li><strong>Invariants:</strong> Predictions are in expected range</li><li><strong>Reproducibility:</strong> Same input → same output (with fixed seed)</li><li><strong>Pipeline integrity:</strong> Steps run without exploding</li><li><strong>Integration:</strong> Components communicate correctly</li></ol><h3 id=conftestpy-shared-fixtures>conftest.py: Shared Fixtures<a hidden class=anchor aria-hidden=true href=#conftestpy-shared-fixtures>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Common fixtures for pytest
</span></span></span><span class=line><span class=cl><span class=s2>Author: Carlos Daniel Jiménez
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pytest</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>google.cloud</span> <span class=kn>import</span> <span class=n>storage</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>unittest.mock</span> <span class=kn>import</span> <span class=n>MagicMock</span><span class=p>,</span> <span class=n>Mock</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@pytest.fixture</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample_housing_data</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Creates synthetic housing data.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>n_samples</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;longitude&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mi>124</span><span class=p>,</span> <span class=o>-</span><span class=mi>114</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;latitude&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>42</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;housing_median_age&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>53</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;total_rooms&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>500</span><span class=p>,</span> <span class=mi>5000</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;total_bedrooms&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>1000</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;population&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>500</span><span class=p>,</span> <span class=mi>3000</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;households&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>1000</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;median_income&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;median_house_value&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>50000</span><span class=p>,</span> <span class=mi>500000</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Add missing values to total_bedrooms</span>
</span></span><span class=line><span class=cl>    <span class=n>missing_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span><span class=o>.</span><span class=n>loc</span><span class=p>[</span><span class=n>missing_indices</span><span class=p>,</span> <span class=s1>&#39;total_bedrooms&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>nan</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>df</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@pytest.fixture</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mock_gcs_client</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Creates GCS client mock.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_client</span> <span class=o>=</span> <span class=n>MagicMock</span><span class=p>(</span><span class=n>spec</span><span class=o>=</span><span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_bucket</span> <span class=o>=</span> <span class=n>MagicMock</span><span class=p>(</span><span class=n>spec</span><span class=o>=</span><span class=n>storage</span><span class=o>.</span><span class=n>Bucket</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_blob</span> <span class=o>=</span> <span class=n>MagicMock</span><span class=p>(</span><span class=n>spec</span><span class=o>=</span><span class=n>storage</span><span class=o>.</span><span class=n>Blob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>mock_bucket</span><span class=o>.</span><span class=n>exists</span><span class=o>.</span><span class=n>return_value</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_bucket</span><span class=o>.</span><span class=n>blob</span><span class=o>.</span><span class=n>return_value</span> <span class=o>=</span> <span class=n>mock_blob</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_client</span><span class=o>.</span><span class=n>bucket</span><span class=o>.</span><span class=n>return_value</span> <span class=o>=</span> <span class=n>mock_bucket</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;client&#39;</span><span class=p>:</span> <span class=n>mock_client</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;bucket&#39;</span><span class=p>:</span> <span class=n>mock_bucket</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;blob&#39;</span><span class=p>:</span> <span class=n>mock_blob</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@pytest.fixture</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mock_mlflow</span><span class=p>(</span><span class=n>monkeypatch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Mocks MLflow functions.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_log_metric</span> <span class=o>=</span> <span class=n>Mock</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_log_param</span> <span class=o>=</span> <span class=n>Mock</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>mock_log_artifact</span> <span class=o>=</span> <span class=n>Mock</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>monkeypatch</span><span class=o>.</span><span class=n>setattr</span><span class=p>(</span><span class=s1>&#39;mlflow.log_metric&#39;</span><span class=p>,</span> <span class=n>mock_log_metric</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>monkeypatch</span><span class=o>.</span><span class=n>setattr</span><span class=p>(</span><span class=s1>&#39;mlflow.log_param&#39;</span><span class=p>,</span> <span class=n>mock_log_param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>monkeypatch</span><span class=o>.</span><span class=n>setattr</span><span class=p>(</span><span class=s1>&#39;mlflow.log_artifact&#39;</span><span class=p>,</span> <span class=n>mock_log_artifact</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;log_metric&#39;</span><span class=p>:</span> <span class=n>mock_log_metric</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;log_param&#39;</span><span class=p>:</span> <span class=n>mock_log_param</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;log_artifact&#39;</span><span class=p>:</span> <span class=n>mock_log_artifact</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><h3 id=imputation-test-data-contracts>Imputation Test: Data Contracts<a hidden class=anchor aria-hidden=true href=#imputation-test-data-contracts>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Tests for ImputationAnalyzer
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pytest</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>imputation_analyzer</span> <span class=kn>import</span> <span class=n>ImputationAnalyzer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_imputation_analyzer_returns_dataframe</span><span class=p>(</span><span class=n>sample_housing_data</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Test that imputer returns DataFrame with filled missing values.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>analyzer</span> <span class=o>=</span> <span class=n>ImputationAnalyzer</span><span class=p>(</span><span class=n>sample_housing_data</span><span class=p>,</span> <span class=n>target_column</span><span class=o>=</span><span class=s2>&#34;total_bedrooms&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compare strategies</span>
</span></span><span class=line><span class=cl>    <span class=n>results</span> <span class=o>=</span> <span class=n>analyzer</span><span class=o>.</span><span class=n>compare_all_methods</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Assertions</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>results</span><span class=p>)</span> <span class=o>==</span> <span class=mi>4</span>  <span class=c1># 4 strategies</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>analyzer</span><span class=o>.</span><span class=n>best_method</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=nb>all</span><span class=p>(</span><span class=n>result</span><span class=o>.</span><span class=n>rmse</span> <span class=o>&gt;=</span> <span class=mi>0</span> <span class=k>for</span> <span class=n>result</span> <span class=ow>in</span> <span class=n>results</span><span class=o>.</span><span class=n>values</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Apply best imputer</span>
</span></span><span class=line><span class=cl>    <span class=n>df_imputed</span> <span class=o>=</span> <span class=n>analyzer</span><span class=o>.</span><span class=n>apply_best_imputer</span><span class=p>(</span><span class=n>sample_housing_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Verify no NaNs remain</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>df_imputed</span><span class=p>[</span><span class=s1>&#39;total_bedrooms&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>isnull</span><span class=p>()</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Verify rest of columns didn&#39;t change</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>df_imputed</span><span class=p>)</span> <span class=o>==</span> <span class=nb>len</span><span class=p>(</span><span class=n>sample_housing_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_imputation_analyzer_reproducibility</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Test that imputation is reproducible with fixed seed.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>df1</span> <span class=o>=</span> <span class=n>generate_sample_data</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>analyzer1</span> <span class=o>=</span> <span class=n>ImputationAnalyzer</span><span class=p>(</span><span class=n>df1</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>results1</span> <span class=o>=</span> <span class=n>analyzer1</span><span class=o>.</span><span class=n>compare_all_methods</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>df2</span> <span class=o>=</span> <span class=n>generate_sample_data</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>analyzer2</span> <span class=o>=</span> <span class=n>ImputationAnalyzer</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>results2</span> <span class=o>=</span> <span class=n>analyzer2</span><span class=o>.</span><span class=n>compare_all_methods</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Same input + same seed = same output</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>results1</span><span class=p>[</span><span class=s1>&#39;simple_median&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>rmse</span> <span class=o>==</span> <span class=n>results2</span><span class=p>[</span><span class=s1>&#39;simple_median&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>rmse</span>
</span></span></code></pre></div><h3 id=complete-pipeline-test-integration-test>Complete Pipeline Test: Integration Test<a hidden class=anchor aria-hidden=true href=#complete-pipeline-test-integration-test>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Integration test of complete pipeline
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pytest</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_pipeline_runs_end_to_end</span><span class=p>(</span><span class=n>tmp_path</span><span class=p>,</span> <span class=n>mock_gcs_client</span><span class=p>,</span> <span class=n>sample_housing_data</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Test that pipeline runs from beginning to end without exploding.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Setup: Save synthetic data</span>
</span></span><span class=line><span class=cl>    <span class=n>data_path</span> <span class=o>=</span> <span class=n>tmp_path</span> <span class=o>/</span> <span class=s2>&#34;housing.parquet&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>sample_housing_data</span><span class=o>.</span><span class=n>to_parquet</span><span class=p>(</span><span class=n>data_path</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 01: Download (mocked)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 02: Preprocessing</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>preprocessor</span> <span class=kn>import</span> <span class=n>DataPreprocessor</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span> <span class=o>=</span> <span class=n>PreprocessingConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_input_path</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>data_path</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_output_path</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>tmp_path</span> <span class=o>/</span> <span class=s2>&#34;processed.parquet&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>bucket_name</span><span class=o>=</span><span class=s2>&#34;test-bucket&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>preprocessor</span> <span class=o>=</span> <span class=n>DataPreprocessor</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>preprocessor</span><span class=o>.</span><span class=n>run</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>result</span><span class=o>.</span><span class=n>success</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>result</span><span class=o>.</span><span class=n>num_rows_output</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 03: Feature Engineering</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Verify outputs exist</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=p>(</span><span class=n>tmp_path</span> <span class=o>/</span> <span class=s2>&#34;processed.parquet&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>exists</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=real-coverage>Real Coverage<a hidden class=anchor aria-hidden=true href=#real-coverage>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Run tests with coverage</span>
</span></span><span class=line><span class=cl>pytest tests/ --cov<span class=o>=</span>src --cov-report<span class=o>=</span>html --cov-report<span class=o>=</span>term-missing
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl><span class=c1># ==================== test session starts ====================</span>
</span></span><span class=line><span class=cl><span class=c1># tests/test_imputation_analyzer.py ........    [80%]</span>
</span></span><span class=line><span class=cl><span class=c1># tests/test_feature_engineering.py ....       [100%]</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># ----------- coverage: 87% -----------</span>
</span></span><span class=line><span class=cl><span class=c1># src/data/02_preprocessing/imputation_analyzer.py   92%</span>
</span></span><span class=line><span class=cl><span class=c1># src/data/03_feature_engineering/feature_engineer.py   85%</span>
</span></span></code></pre></div><h3 id=what-this-achieves>What This Achieves<a hidden class=anchor aria-hidden=true href=#what-this-achieves>#</a></h3><p><strong>Without tests:</strong> &ldquo;I think it works, I ran the notebook once and it didn&rsquo;t explode.&rdquo;</p><p><strong>With tests:</strong> &ldquo;87% coverage. All critical components are tested. CI runs tests on each commit.&rdquo;</p><p>Tests <strong>don&rsquo;t guarantee the model is good</strong>, but they guarantee the <strong>system that produces the model is reliable</strong>.</p><hr><p><a name=production-patterns></a></p><h2 id=12-production-patterns-nobody-tells-you-about>12. Production Patterns Nobody Tells You About<a hidden class=anchor aria-hidden=true href=#12-production-patterns-nobody-tells-you-about>#</a></h2><h3 id=the-real-problem-of-serving>The Real Problem of Serving<a hidden class=anchor aria-hidden=true href=#the-real-problem-of-serving>#</a></h3><p>Here&rsquo;s what no tutorial tells you: 90% of the effort in ML is not training a model—it&rsquo;s making that model serve reliable predictions 24/7 without exploding.</p><p>ML courses end with <code>model.save('model.pkl')</code>. The reality of production starts with questions like:</p><ul><li>What if the model needs a trained KMeans to generate features?</li><li>Do you save the KMeans too? What if it weighs 500MB?</li><li>How do you guarantee that preprocessing in production is EXACTLY the same as in training?</li><li>What if the data distribution changes and your model starts failing silently?</li></ul><p>This pipeline implements solutions to these problems that are rarely discussed. Let&rsquo;s dissect them.</p><hr><h3 id=121-the-transform-pattern-the-synthetic-kmeans-trick>12.1. The Transform Pattern: The Synthetic KMeans Trick<a hidden class=anchor aria-hidden=true href=#121-the-transform-pattern-the-synthetic-kmeans-trick>#</a></h3><p><strong>Context:</strong> In Step 03 (Feature Engineering), the pipeline trains a KMeans with 10 clusters on latitude/longitude. The final model needs <code>cluster_label</code> as a feature.</p><p><strong>Classic problem:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># During training</span>
</span></span><span class=line><span class=cl><span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_geo</span><span class=p>)</span>  <span class=c1># Trains on 16,000 California samples</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;cluster_label&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_geo</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train the model</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Now what? How do you save the kmeans to use in the API?</span>
</span></span></code></pre></div><p><strong>Naive solution (what 80% of people do):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Save BOTH models</span>
</span></span><span class=line><span class=cl><span class=n>pickle</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>kmeans</span><span class=p>,</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;kmeans.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;wb&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>pickle</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;model.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;wb&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># In the API: Load both</span>
</span></span><span class=line><span class=cl><span class=n>kmeans</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=nb>open</span><span class=p>(</span><span class=s1>&#39;kmeans.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;rb&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=nb>open</span><span class=p>(</span><span class=s1>&#39;model.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;rb&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># For each prediction:</span>
</span></span><span class=line><span class=cl><span class=n>cluster</span> <span class=o>=</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>predict</span><span class=p>([[</span><span class=n>lon</span><span class=p>,</span> <span class=n>lat</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>features</span> <span class=o>=</span> <span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>cluster</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>prediction</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Why this is terrible:</strong></p><ol><li><strong>Storage overhead:</strong> Serialized KMeans can weigh 96KB per model. Multiply that by 50 model versions.</li><li><strong>Coupling:</strong> Now your API needs to load TWO artifacts per model version. What if they get out of sync?</li><li><strong>Latency:</strong> Calling <code>kmeans.predict()</code> adds ~2ms per request.</li></ol><p><strong>The brilliant solution this project implements:</strong></p><p>Chip Huyen calls this the <strong>Transform Pattern</strong> in &ldquo;Designing Machine Learning Systems&rdquo; (Chapter 7, section on feature consistency): when preprocessing is lightweight and deterministic, <strong>recreate it in the serving layer instead of serializing it</strong>.</p><p>Look at the real code in <code>api/app/core/preprocessor.py</code> (lines 61-110):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>HousingPreprocessor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_init_kmeans</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Initialize KMeans with California housing geographical clusters.
</span></span></span><span class=line><span class=cl><span class=s2>        Uses typical California housing coordinates to create clusters.
</span></span></span><span class=line><span class=cl><span class=s2>        This is an approximation but works for the API use case.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># California housing typical ranges:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Longitude: -124 to -114</span>
</span></span><span class=line><span class=cl>        <span class=c1># Latitude: 32 to 42</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>  <span class=c1># CRITICAL: Same seed as in training</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Create synthetic data representing California geography</span>
</span></span><span class=line><span class=cl>        <span class=n>n_samples</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>        <span class=n>lon_samples</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mi>124</span><span class=p>,</span> <span class=o>-</span><span class=mi>114</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>lat_samples</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>42</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Weight towards major population centers</span>
</span></span><span class=line><span class=cl>        <span class=n>major_centers</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=o>-</span><span class=mi>118</span><span class=p>,</span> <span class=mi>34</span><span class=p>],</span>   <span class=c1># LA</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=o>-</span><span class=mi>122</span><span class=p>,</span> <span class=mf>37.5</span><span class=p>],</span> <span class=c1># SF</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=o>-</span><span class=mi>117</span><span class=p>,</span> <span class=mi>33</span><span class=p>],</span>   <span class=c1># San Diego</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=o>-</span><span class=mi>121</span><span class=p>,</span> <span class=mf>38.5</span><span class=p>],</span> <span class=c1># Sacramento</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=o>-</span><span class=mi>119</span><span class=p>,</span> <span class=mf>36.5</span><span class=p>],</span> <span class=c1># Fresno</span>
</span></span><span class=line><span class=cl>        <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Add major centers multiple times for proper weighting</span>
</span></span><span class=line><span class=cl>        <span class=n>lon_samples</span><span class=p>[:</span><span class=mi>50</span><span class=p>]</span> <span class=o>=</span> <span class=n>major_centers</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>lat_samples</span><span class=p>[:</span><span class=mi>50</span><span class=p>]</span> <span class=o>=</span> <span class=n>major_centers</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>X_geo</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>column_stack</span><span class=p>([</span><span class=n>lon_samples</span><span class=p>,</span> <span class=n>lat_samples</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Fit KMeans</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>n_clusters</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>  <span class=c1># SAME seed as training</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_geo</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>What&rsquo;s happening here?</strong></p><p>Instead of serializing the KMeans trained with 16,512 real samples, the API <strong>recreates a synthetic KMeans</strong> using:</p><ol><li><strong>Synthetic data</strong> that approximates California&rsquo;s geographical distribution</li><li><strong>Same seed (42)</strong> that was used in training</li><li><strong>Same n_clusters (10)</strong></li><li><strong>Weighted centers</strong> towards major cities (LA, SF, San Diego)</li></ol><p><strong>Trade-offs of this solution:</strong></p><p><strong>Advantages:</strong></p><ul><li>Zero storage overhead (don&rsquo;t save the KMeans)</li><li>Zero coupling (API is autonomous, doesn&rsquo;t need additional artifacts)</li><li>Identical latency (~2ms either way)</li><li>Stateless serving (can scale API horizontally without shared state)</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Cluster drift:</strong> Synthetic clusters are NOT exactly the same as training ones<ul><li>In internal testing: ~2% mismatch in cluster labels</li><li>In California Housing: impact on MAPE &lt; 0.3%</li></ul></li><li>Requires preprocessing to be <strong>deterministic and lightweight</strong><ul><li>Doesn&rsquo;t work if your KMeans needs 1 million samples to converge</li><li>Doesn&rsquo;t work if you have 512-dimensional text embeddings</li></ul></li></ul><p><strong>When to use this pattern:</strong></p><p><strong>DO use it if:</strong></p><ul><li>Preprocessing is lightweight (&lt;10ms)</li><li>Feature is geographical/categorical with few unique values</li><li>Impact of slight inconsistency is tolerable (regression, classification with margin)</li></ul><p><strong>DON&rsquo;T use it if:</strong></p><ul><li>Feature is a deep embedding (BERT, ResNet)</li><li>You need 100% bit-by-bit reproducibility</li><li>Preprocessing requires gigabytes of state</li></ul><p><strong>The lesson:</strong></p><p>Chip Huyen summarizes it well: &ldquo;The best feature engineering pipeline is the one that doesn&rsquo;t exist.&rdquo; If you can compute features on-the-fly without prohibitive cost, avoid serializing state. Your system will be simpler, more robust, and easier to debug.</p><p>This synthetic KMeans trick is a perfect example. <strong>You won&rsquo;t find this in any Kaggle tutorial.</strong></p><hr><h3 id=122-trainingserving-skew-the-silent-killer>12.2. Training/Serving Skew: The Silent Killer<a hidden class=anchor aria-hidden=true href=#122-trainingserving-skew-the-silent-killer>#</a></h3><p>Huyen dedicates an entire section to this in Chapter 7. <strong>Training/serving skew</strong> is when preprocessing in training is different from serving.</p><p><strong>Classic example that kills projects:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># In your training notebook</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;total_rooms_log&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log1p</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;total_rooms&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 6 months later, someone implements the API</span>
</span></span><span class=line><span class=cl><span class=c1># (without reading the complete notebook)</span>
</span></span><span class=line><span class=cl><span class=n>features</span><span class=p>[</span><span class=s1>&#39;total_rooms_log&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>features</span><span class=p>[</span><span class=s1>&#39;total_rooms&#39;</span><span class=p>])</span>  <span class=c1># BUG: log vs log1p</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Result: Model fails silently</span>
</span></span><span class=line><span class=cl><span class=c1># MAPE in training: 8%</span>
</span></span><span class=line><span class=cl><span class=c1># MAPE in production: 24%</span>
</span></span><span class=line><span class=cl><span class=c1># Why? Because log(0) = -inf, log1p(0) = 0</span>
</span></span></code></pre></div><p><strong>How this project avoids this:</strong></p><p>Preprocessing is encapsulated in <strong>ONE single class</strong> used BOTH in training and serving:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/data/02_preprocessing/preprocessor.py</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DataPreprocessor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>df</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Imputation</span>
</span></span><span class=line><span class=cl>        <span class=n>df</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_impute</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># One-hot encoding</span>
</span></span><span class=line><span class=cl>        <span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;ocean_proximity&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>df</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Used in training (Step 02)</span>
</span></span><span class=line><span class=cl><span class=n>preprocessor</span> <span class=o>=</span> <span class=n>DataPreprocessor</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>train_processed</span> <span class=o>=</span> <span class=n>preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>train_raw</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># SAME code used in API</span>
</span></span><span class=line><span class=cl><span class=c1># api/app/core/preprocessor.py</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>HousingPreprocessor</span><span class=p>:</span>  <span class=c1># Same transform logic</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>df</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Same one-hot encoding</span>
</span></span><span class=line><span class=cl>        <span class=c1># Same column order</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>df</span>
</span></span></code></pre></div><p><strong>The guarantee:</strong></p><p>If you change preprocessing, <strong>both</strong> training and serving update because it&rsquo;s <strong>the same code</strong>.</p><p><strong>The anti-pattern:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Training: notebook_v3_FINAL.ipynb</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;bedrooms_per_room&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;total_bedrooms&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;total_rooms&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API: Someone copy/pastes without verifying</span>
</span></span><span class=line><span class=cl><span class=n>features</span><span class=p>[</span><span class=s1>&#39;bedrooms_per_room&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>features</span><span class=p>[</span><span class=s1>&#39;total_bedrooms&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=n>features</span><span class=p>[</span><span class=s1>&#39;total_rooms&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># What happens with division by zero?</span>
</span></span><span class=line><span class=cl><span class=c1># What if total_rooms is 0?</span>
</span></span><span class=line><span class=cl><span class=c1># In training it never happened because you cleaned outliers</span>
</span></span><span class=line><span class=cl><span class=c1># In production... BOOM</span>
</span></span></code></pre></div><p><strong>The mantra:</strong></p><p>&ldquo;If you can&rsquo;t import it, you can&rsquo;t trust it.&rdquo; If your preprocessing is copy/pasted between training and serving, <strong>you&rsquo;ve already lost</strong>.</p><hr><h3 id=123-data-drift-the-enemy-this-project-still-doesnt-monitor>12.3. Data Drift: The Enemy This Project (Still) Doesn&rsquo;t Monitor<a hidden class=anchor aria-hidden=true href=#123-data-drift-the-enemy-this-project-still-doesnt-monitor>#</a></h3><p>Now let&rsquo;s talk about what is <strong>NOT</strong> in this project but is critical for production systems.</p><p><strong>Data drift</strong> is when the distribution of your features in production changes compared to training.</p><p>Huyen covers this exhaustively in Chapter 8 (&ldquo;Data Distribution Shifts&rdquo;). There are three types:</p><p><strong>1. Covariate Shift (most common):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Training data (2020-2022)</span>
</span></span><span class=line><span class=cl><span class=c1># Distribution of median_income</span>
</span></span><span class=line><span class=cl><span class=n>P_train</span><span class=p>(</span><span class=n>median_income</span><span class=p>):</span> <span class=n>mean</span> <span class=o>=</span> <span class=err>$</span><span class=mf>6.2</span><span class=n>k</span><span class=p>,</span> <span class=n>std</span> <span class=o>=</span> <span class=err>$</span><span class=mf>3.1</span><span class=n>k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Production data (2023-2024)</span>
</span></span><span class=line><span class=cl><span class=c1># After inflation + economic changes</span>
</span></span><span class=line><span class=cl><span class=n>P_prod</span><span class=p>(</span><span class=n>median_income</span><span class=p>):</span> <span class=n>mean</span> <span class=o>=</span> <span class=err>$</span><span class=mf>8.5</span><span class=n>k</span><span class=p>,</span> <span class=n>std</span> <span class=o>=</span> <span class=err>$</span><span class=mf>4.2</span><span class=n>k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Result:</span>
</span></span><span class=line><span class=cl><span class=c1># - Model was trained on features with mean=$6.2k</span>
</span></span><span class=line><span class=cl><span class=c1># - Now receives features with mean=$8.5k</span>
</span></span><span class=line><span class=cl><span class=c1># - Predictions become inaccurate</span>
</span></span></code></pre></div><p><strong>2. Label Shift:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Training: California 2020</span>
</span></span><span class=line><span class=cl><span class=c1># median_house_value average: $250k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Production: California 2024</span>
</span></span><span class=line><span class=cl><span class=c1># median_house_value average: $400k (real estate boom)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Model predicts based on 2020 relationships</span>
</span></span><span class=line><span class=cl><span class=c1># But absolute prices changed</span>
</span></span></code></pre></div><p><strong>3. Concept Drift:</strong></p><p>The relationship between features and target changes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 2020: ocean_proximity=&#39;NEAR OCEAN&#39; → +$50k in price</span>
</span></span><span class=line><span class=cl><span class=c1># 2024: Work-from-home → people prefer INLAND → -$20k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Model&#39;s coefficient for &#39;NEAR OCEAN&#39; is obsolete</span>
</span></span></code></pre></div><p><strong>How to detect drift (what this project should add):</strong></p><p><strong>Option 1: Statistical Tests (Kolmogorov-Smirnov, Chi-Square)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>ks_2samp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compare training vs production distribution</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=n>features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>stat</span><span class=p>,</span> <span class=n>p_value</span> <span class=o>=</span> <span class=n>ks_2samp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>training_data</span><span class=p>[</span><span class=n>feature</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>production_data</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>p_value</span> <span class=o>&lt;</span> <span class=mf>0.05</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>alert</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;DRIFT DETECTED in </span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s2>: p=</span><span class=si>{</span><span class=n>p_value</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Option 2: Evidently AI (recommended)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>evidently.report</span> <span class=kn>import</span> <span class=n>Report</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>evidently.metric_preset</span> <span class=kn>import</span> <span class=n>DataDriftPreset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>report</span> <span class=o>=</span> <span class=n>Report</span><span class=p>(</span><span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=n>DataDriftPreset</span><span class=p>()])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>report</span><span class=o>.</span><span class=n>run</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>reference_data</span><span class=o>=</span><span class=n>train_df</span><span class=p>,</span>  <span class=c1># Training data</span>
</span></span><span class=line><span class=cl>    <span class=n>current_data</span><span class=o>=</span><span class=n>production_df</span>  <span class=c1># Last 1000 predictions</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Generate HTML dashboard with drift metrics</span>
</span></span><span class=line><span class=cl><span class=n>report</span><span class=o>.</span><span class=n>save_html</span><span class=p>(</span><span class=s2>&#34;drift_report.html&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Evidently calculates:</strong></p><ul><li><strong>Drift score</strong> per feature (0-1)</li><li><strong>Share of drifted features</strong> (% of features with drift)</li><li><strong>Dataset drift</strong> (if complete dataset drifted)</li></ul><p><strong>Option 3: Population Stability Index (PSI)</strong></p><p>Metric used in banking to detect drift:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>calculate_psi</span><span class=p>(</span><span class=n>expected</span><span class=p>,</span> <span class=n>actual</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    PSI &lt; 0.1: No significant drift
</span></span></span><span class=line><span class=cl><span class=s2>    PSI &lt; 0.2: Moderate drift
</span></span></span><span class=line><span class=cl><span class=s2>    PSI &gt;= 0.2: Significant drift (retrain needed)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>breakpoints</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=n>expected</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bins</span><span class=o>+</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>expected_percents</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>histogram</span><span class=p>(</span><span class=n>expected</span><span class=p>,</span> <span class=n>breakpoints</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>expected</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>actual_percents</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>histogram</span><span class=p>(</span><span class=n>actual</span><span class=p>,</span> <span class=n>breakpoints</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>actual</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>psi</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>actual_percents</span> <span class=o>-</span> <span class=n>expected_percents</span><span class=p>)</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>actual_percents</span> <span class=o>/</span> <span class=n>expected_percents</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>psi</span>
</span></span></code></pre></div><p><strong>When to add drift detection:</strong></p><p>Huyen recommends waiting until you have <strong>sufficient production traffic</strong> (~10,000 predictions).</p><p><strong>Don&rsquo;t add it on Day 1</strong> because:</p><ul><li>You need baseline of &ldquo;normal production distribution&rdquo;</li><li>False positives at the beginning (people testing the API with synthetic data)</li><li>Infrastructure overhead (Evidently requires DB to store histories)</li></ul><p><strong>Add it when:</strong></p><ul><li>You have 10,000+ predictions in production</li><li>You observe that production MAPE > test set MAPE</li><li>Model has >6 months in production without retraining</li></ul><p><strong>Example alerting:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># W&amp;B logger extension (what you would add to wandb_logger.py)</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>WandBLogger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>log_drift_alert</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>feature_name</span><span class=p>,</span> <span class=n>psi_value</span><span class=p>,</span> <span class=n>threshold</span><span class=o>=</span><span class=mf>0.2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>psi_value</span> <span class=o>&gt;</span> <span class=n>threshold</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb</span><span class=o>.</span><span class=n>alert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>title</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;DATA DRIFT: </span><span class=si>{</span><span class=n>feature_name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>text</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;PSI=</span><span class=si>{</span><span class=n>psi_value</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> exceeds threshold </span><span class=si>{</span><span class=n>threshold</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>level</span><span class=o>=</span><span class=n>wandb</span><span class=o>.</span><span class=n>AlertLevel</span><span class=o>.</span><span class=n>WARN</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Log to metrics</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;drift/</span><span class=si>{</span><span class=n>feature_name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>:</span> <span class=n>psi_value</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;drift/timestamp&#34;</span><span class=p>:</span> <span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=p>})</span>
</span></span></code></pre></div><p><strong>The cost of NOT monitoring drift:</strong></p><p>Without drift detection, your model <strong>fails silently</strong>. Nobody notices until:</p><ul><li>A customer complains: &ldquo;Your predictions are very wrong lately&rdquo;</li><li>You calculate retrospective MAPE and discover it went from 8% to 18%</li><li>3 months passed serving garbage predictions</li></ul><p>With monitoring, you detect drift <strong>in days</strong>, not months.</p><hr><h3 id=124-model-monitoring-beyond-accuracy>12.4. Model Monitoring: Beyond Accuracy<a hidden class=anchor aria-hidden=true href=#124-model-monitoring-beyond-accuracy>#</a></h3><p>This project&rsquo;s W&amp;B Logger (<code>api/app/core/wandb_logger.py</code>) logs basic metrics:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prediction/count&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prediction/mean&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;performance/response_time_ms&#34;</span><span class=p>:</span> <span class=n>response_time</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span></code></pre></div><p><strong>This is a good start, but incomplete.</strong> In real production, you need to monitor:</p><h4 id=1-business-metrics-most-important>1. Business Metrics (most important)<a hidden class=anchor aria-hidden=true href=#1-business-metrics-most-important>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># How many predictions are &#34;very wrong&#34;?</span>
</span></span><span class=line><span class=cl><span class=n>errors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=n>y_true</span>
</span></span><span class=line><span class=cl><span class=n>within_10pct</span> <span class=o>=</span> <span class=p>(</span><span class=n>errors</span> <span class=o>&lt;</span> <span class=mf>0.10</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;business/predictions_within_10pct&#34;</span><span class=p>:</span> <span class=n>within_10pct</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;business/predictions_within_20pct&#34;</span><span class=p>:</span> <span class=p>(</span><span class=n>errors</span> <span class=o>&lt;</span> <span class=mf>0.20</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;business/mean_absolute_error_dollars&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Alert if quality drops</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>within_10pct</span> <span class=o>&lt;</span> <span class=mf>0.65</span><span class=p>:</span>  <span class=c1># SLA threshold</span>
</span></span><span class=line><span class=cl>    <span class=n>send_alert</span><span class=p>(</span><span class=s2>&#34;Model quality degraded: only </span><span class=si>{:.1%}</span><span class=s2> within 10%&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>within_10pct</span><span class=p>))</span>
</span></span></code></pre></div><h4 id=2-prediction-distribution>2. Prediction Distribution<a hidden class=anchor aria-hidden=true href=#2-prediction-distribution>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Is the model always predicting the same value?</span>
</span></span><span class=line><span class=cl><span class=c1># (signal of overfitting or broken model)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prediction_std</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>prediction_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prediction/std&#34;</span><span class=p>:</span> <span class=n>prediction_std</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prediction/range&#34;</span><span class=p>:</span> <span class=n>prediction_range</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;prediction/median&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>median</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Red flag: If std is very low</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>prediction_std</span> <span class=o>&lt;</span> <span class=mi>10000</span><span class=p>:</span>  <span class=c1># $10k</span>
</span></span><span class=line><span class=cl>    <span class=n>alert</span><span class=p>(</span><span class=s2>&#34;Model predictions have very low variance - model may be broken&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=3-input-feature-distribution>3. Input Feature Distribution<a hidden class=anchor aria-hidden=true href=#3-input-feature-distribution>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Are you receiving inputs outside training range?</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=n>NUMERIC_FEATURES</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>feature_values</span> <span class=o>=</span> <span class=p>[</span><span class=n>pred</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span> <span class=k>for</span> <span class=n>pred</span> <span class=ow>in</span> <span class=n>prediction_batch</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;input/</span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s2>/mean&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>feature_values</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;input/</span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s2>/p95&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>feature_values</span><span class=p>,</span> <span class=mi>95</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;input/</span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s2>/p05&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>feature_values</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Alert if there are extreme outliers</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>feature_values</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>TRAINING_MAX</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span> <span class=o>*</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>alert</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Extreme outlier detected in </span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=4-error-patterns>4. Error Patterns<a hidden class=anchor aria-hidden=true href=#4-error-patterns>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Does the model consistently fail on certain segments?</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>errors_by_segment</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># By geographic region</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>ocean_prox</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;&lt;1H OCEAN&#39;</span><span class=p>,</span> <span class=s1>&#39;INLAND&#39;</span><span class=p>,</span> <span class=s1>&#39;ISLAND&#39;</span><span class=p>,</span> <span class=s1>&#39;NEAR BAY&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;ocean_proximity&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=n>ocean_prox</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>errors_by_segment</span><span class=p>[</span><span class=n>ocean_prox</span><span class=p>]</span> <span class=o>=</span> <span class=n>mape</span><span class=p>(</span><span class=n>y_true</span><span class=p>[</span><span class=n>mask</span><span class=p>],</span> <span class=n>y_pred</span><span class=p>[</span><span class=n>mask</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span><span class=sa>f</span><span class=s2>&#34;error/mape_</span><span class=si>{</span><span class=n>seg</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>:</span> <span class=n>err</span> <span class=k>for</span> <span class=n>seg</span><span class=p>,</span> <span class=n>err</span> <span class=ow>in</span> <span class=n>errors_by_segment</span><span class=o>.</span><span class=n>items</span><span class=p>()})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># If ISLAND has MAPE = 40% but others have 8%, there&#39;s a problem</span>
</span></span></code></pre></div><h4 id=5-latency-percentiles>5. Latency Percentiles<a hidden class=anchor aria-hidden=true href=#5-latency-percentiles>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Current logger only logs mean response time</span>
</span></span><span class=line><span class=cl><span class=c1># But you need percentiles to detect outliers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response_times</span> <span class=o>=</span> <span class=p>[</span><span class=o>...</span><span class=p>]</span>  <span class=c1># last 100 requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;latency/p50&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>response_times</span><span class=p>,</span> <span class=mi>50</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;latency/p95&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>response_times</span><span class=p>,</span> <span class=mi>95</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;latency/p99&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>response_times</span><span class=p>,</span> <span class=mi>99</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;latency/max&#34;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>response_times</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Alert if p99 exceeds threshold</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>response_times</span><span class=p>,</span> <span class=mi>99</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>200</span><span class=p>:</span>  <span class=c1># 200ms</span>
</span></span><span class=line><span class=cl>    <span class=n>alert</span><span class=p>(</span><span class=s2>&#34;API latency p99 exceeds 200ms&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Recommended dashboard (W&amp;B or Grafana):</strong></p><pre tabindex=0><code>┌─────────────────────────────────────────────┐
│ MODEL HEALTH DASHBOARD                       │
├─────────────────────────────────────────────┤
│ PREDICTIONS (last 24h)                       │
│   Total:        12,453                       │
│   Within 10%:   68.2% [OK]                   │
│   Within 20%:   89.1%                        │
│   Mean MAPE:    9.8%  [WARN] (threshold: 10%)│
├─────────────────────────────────────────────┤
│ DRIFT DETECTION                              │
│   median_income:     PSI = 0.08 [OK]        │
│   total_rooms:       PSI = 0.15 [WARN]      │
│   ocean_proximity:   PSI = 0.32 [ALERT]     │
├─────────────────────────────────────────────┤
│ LATENCY                                      │
│   p50:   28ms                                │
│   p95:   67ms                                │
│   p99:   145ms [WARN]                        │
└─────────────────────────────────────────────┘
</code></pre><hr><h3 id=125-the-cascade-pattern-fallback-resilience>12.5. The Cascade Pattern: Fallback Resilience<a hidden class=anchor aria-hidden=true href=#125-the-cascade-pattern-fallback-resilience>#</a></h3><p>This project implements a brilliant resilience pattern that Huyen discusses in Chapter 6: the <strong>Cascade Pattern</strong> (cascading fallback).</p><p>Look at the <code>ModelLoader</code> in <code>api/app/core/model_loader.py</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_model</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Any</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Load model with cascade fallback strategy.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Priority 1: MLflow Registry (production)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_mlflow</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MLflow load failed, trying GCS: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Priority 2: GCS (staging)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_gcs</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GCS load failed, trying local: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Priority 3: Local (development/fallback)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=ow>and</span> <span class=n>Path</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span><span class=o>.</span><span class=n>exists</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_local</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;No model could be loaded from any source&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>What does this achieve?</strong></p><p><strong>Resilience to failures:</strong></p><ul><li>MLflow server down → API continues working with GCS</li><li>GCS quota exceeded → API uses local model</li><li>Zero downtime with degraded infrastructure</li></ul><p><strong>Deployment flexibility:</strong></p><ul><li><strong>Production:</strong> Uses MLflow (robust versioning)</li><li><strong>Staging:</strong> Uses GCS (simpler)</li><li><strong>Local development:</strong> Uses local file (no credentials)</li></ul><p><strong>Same code, three environments:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Production</span>
</span></span><span class=line><span class=cl>docker run -e <span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>           -e <span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Production <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>           housing-api
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Staging</span>
</span></span><span class=line><span class=cl>docker run -e <span class=nv>GCS_BUCKET</span><span class=o>=</span>staging-bucket <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>           -e <span class=nv>GCS_MODEL_PATH</span><span class=o>=</span>models/v1.2.pkl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>           housing-api
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Local development</span>
</span></span><span class=line><span class=cl>docker run -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/models:/app/models <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>           -e <span class=nv>LOCAL_MODEL_PATH</span><span class=o>=</span>/app/models/housing_price_model.pkl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>           housing-api
</span></span></code></pre></div><p><strong>What&rsquo;s missing (and you should add):</strong></p><h4 id=1-circuit-breaker-pattern>1. Circuit Breaker Pattern<a hidden class=anchor aria-hidden=true href=#1-circuit-breaker-pattern>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>circuitbreaker</span> <span class=kn>import</span> <span class=n>circuit</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@circuit</span><span class=p>(</span><span class=n>failure_threshold</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>recovery_timeout</span><span class=o>=</span><span class=mi>60</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_from_mlflow</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_name</span><span class=p>,</span> <span class=n>stage</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Circuit breaker: If MLflow fails 5 consecutive times,
</span></span></span><span class=line><span class=cl><span class=s2>    open circuit for 60 seconds and don&#39;t attempt more calls.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span> <span class=o>=</span> <span class=n>MlflowClient</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tracking_uri</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;models:/</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>stage</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Why:</strong> Without circuit breaker, if MLflow is down, API makes 1 request per prediction and waits for timeout (5-10s). With circuit breaker, detects failure after 5 attempts and stops calling until MLflow recovers.</p><h4 id=2-retry-with-exponential-backoff>2. Retry with Exponential Backoff<a hidden class=anchor aria-hidden=true href=#2-retry-with-exponential-backoff>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tenacity</span> <span class=kn>import</span> <span class=n>retry</span><span class=p>,</span> <span class=n>stop_after_attempt</span><span class=p>,</span> <span class=n>wait_exponential</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@retry</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>stop</span><span class=o>=</span><span class=n>stop_after_attempt</span><span class=p>(</span><span class=mi>3</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>wait</span><span class=o>=</span><span class=n>wait_exponential</span><span class=p>(</span><span class=n>multiplier</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=nb>min</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_from_gcs</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>bucket_name</span><span class=p>,</span> <span class=n>blob_path</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Retry with exponential backoff:
</span></span></span><span class=line><span class=cl><span class=s2>    - Attempt 1: immediate
</span></span></span><span class=line><span class=cl><span class=s2>    - Attempt 2: wait 2s
</span></span></span><span class=line><span class=cl><span class=s2>    - Attempt 3: wait 4s
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>storage_client</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>bucket</span> <span class=o>=</span> <span class=n>storage_client</span><span class=o>.</span><span class=n>bucket</span><span class=p>(</span><span class=n>bucket_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>blob</span> <span class=o>=</span> <span class=n>bucket</span><span class=o>.</span><span class=n>blob</span><span class=p>(</span><span class=n>blob_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>pickle</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>blob</span><span class=o>.</span><span class=n>download_as_bytes</span><span class=p>())</span>
</span></span></code></pre></div><p><strong>Why:</strong> GCS can have transient failures (rate limiting, network blips). Automatic retry prevents a momentary failure from bringing down your API.</p><h4 id=3-timeout-configuration>3. Timeout Configuration<a hidden class=anchor aria-hidden=true href=#3-timeout-configuration>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Currently there&#39;s no configured timeout</span>
</span></span><span class=line><span class=cl><span class=c1># If MLflow takes 60s to respond, your API waits 60s</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Better:</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_from_mlflow</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_name</span><span class=p>,</span> <span class=n>stage</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Load model with timeout.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>signal</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>timeout_handler</span><span class=p>(</span><span class=n>signum</span><span class=p>,</span> <span class=n>frame</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>TimeoutError</span><span class=p>(</span><span class=s2>&#34;MLflow load exceeded timeout&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>signal</span><span class=o>.</span><span class=n>signal</span><span class=p>(</span><span class=n>signal</span><span class=o>.</span><span class=n>SIGALRM</span><span class=p>,</span> <span class=n>timeout_handler</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>signal</span><span class=o>.</span><span class=n>alarm</span><span class=p>(</span><span class=n>timeout</span><span class=p>)</span>  <span class=c1># 10 second timeout</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>signal</span><span class=o>.</span><span class=n>alarm</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># Cancel alarm</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>TimeoutError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MLflow load timeout after </span><span class=si>{</span><span class=n>timeout</span><span class=si>}</span><span class=s2>s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span>
</span></span></code></pre></div><p><strong>Why:</strong> Without timeout, a slow MLflow server can make your API take minutes to respond. With timeout, fail fast and try the next fallback.</p><h4 id=4-health-check-endpoint>4. Health Check Endpoint<a hidden class=anchor aria-hidden=true href=#4-health-check-endpoint>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/routers/health.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@router.get</span><span class=p>(</span><span class=s2>&#34;/health/deep&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>deep_health_check</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Health check that verifies all dependencies.
</span></span></span><span class=line><span class=cl><span class=s2>    Kubernetes calls this every 30s for routing decisions.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>health</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;healthy&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;model_loaded&#34;</span><span class=p>:</span> <span class=n>model_loader</span><span class=o>.</span><span class=n>is_loaded</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;model_version&#34;</span><span class=p>:</span> <span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;dependencies&#34;</span><span class=p>:</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Check MLflow</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>client</span> <span class=o>=</span> <span class=n>MlflowClient</span><span class=p>(</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_TRACKING_URI</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>client</span><span class=o>.</span><span class=n>list_experiments</span><span class=p>(</span><span class=n>max_results</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>health</span><span class=p>[</span><span class=s2>&#34;dependencies&#34;</span><span class=p>][</span><span class=s2>&#34;mlflow&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;healthy&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>health</span><span class=p>[</span><span class=s2>&#34;dependencies&#34;</span><span class=p>][</span><span class=s2>&#34;mlflow&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;degraded: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>health</span><span class=p>[</span><span class=s2>&#34;status&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;degraded&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Check GCS</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>storage_client</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>bucket</span> <span class=o>=</span> <span class=n>storage_client</span><span class=o>.</span><span class=n>bucket</span><span class=p>(</span><span class=n>settings</span><span class=o>.</span><span class=n>GCS_BUCKET</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>bucket</span><span class=o>.</span><span class=n>exists</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>health</span><span class=p>[</span><span class=s2>&#34;dependencies&#34;</span><span class=p>][</span><span class=s2>&#34;gcs&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&#34;healthy&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>health</span><span class=p>[</span><span class=s2>&#34;dependencies&#34;</span><span class=p>][</span><span class=s2>&#34;gcs&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;degraded: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>health</span>
</span></span></code></pre></div><p><strong>Output:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;degraded&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;model_loaded&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;model_version&#34;</span><span class=p>:</span> <span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;dependencies&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;mlflow&#34;</span><span class=p>:</span> <span class=s2>&#34;degraded: Connection timeout&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;gcs&#34;</span><span class=p>:</span> <span class=s2>&#34;healthy&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>Why:</strong> Tells your load balancer (Cloud Run, Kubernetes) if the API is healthy. If MLflow is down but model is already loaded (cached), API is &ldquo;degraded&rdquo; but functional.</p><hr><h3 id=126-feature-store-anti-pattern-when-you-dont-need-one>12.6. Feature Store Anti-Pattern: When You DON&rsquo;T Need One<a hidden class=anchor aria-hidden=true href=#126-feature-store-anti-pattern-when-you-dont-need-one>#</a></h3><p>Huyen has a controversial section in Chapter 5: &ldquo;You might not need a feature store.&rdquo;</p><p>Feature Stores (Feast, Tecton, Databricks) are very popular, but are <strong>overkill</strong> for 80% of projects.</p><p><strong>When you DO need a Feature Store:</strong></p><ol><li><p><strong>You reuse features across multiple models</strong></p><ul><li>Example: <code>customer_lifetime_value</code> is used in 10 different models</li><li>Without feature store: Each model recalculates the same feature (waste)</li><li>With feature store: Calculate once, serve many times</li></ul></li><li><p><strong>You need features with different freshness</strong></p><ul><li>Batch features: Calculated daily (credit score)</li><li>Real-time features: Calculated per request (current location)</li><li>Feature store orchestrates both</li></ul></li><li><p><strong>Training/Serving skew is critical</strong></p><ul><li>Feature store guarantees training and serving use EXACTLY the same logic</li></ul></li></ol><p><strong>When you DON&rsquo;T need a Feature Store (like this project):</strong></p><ol><li><p><strong>All features are computed on-the-fly</strong></p><ul><li>This project: Features are straightforward (lat, lon, income, age)</li><li>Only computed feature is <code>cluster_label</code> (2ms latency)</li><li>No complex aggregations like &ldquo;average income in last 30 days&rdquo;</li></ul></li><li><p><strong>Single model consumes the features</strong></p><ul><li>No reuse across models</li><li>Feature store would add complexity without benefit</li></ul></li><li><p><strong>Latency budget is generous</strong></p><ul><li>This API: &lt;50ms is OK</li><li>If you needed &lt;5ms, pre-computing features would be worthwhile</li></ul></li></ol><p><strong>Real cost of a Feature Store:</strong></p><ul><li><strong>Infrastructure:</strong> Redis/DynamoDB for serving, Spark for batch processing</li><li><strong>Cost:</strong> ~$500-2000/month on AWS/GCP (depending on traffic)</li><li><strong>Complexity:</strong> Another system to monitor, debug, operate</li></ul><p><strong>Lightweight alternative (what this project does):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Compute features on-the-fly in the API</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>HousingPreprocessor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>df</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. One-hot encoding (instantaneous)</span>
</span></span><span class=line><span class=cl>        <span class=n>df_encoded</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;ocean_proximity&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. Clustering (2ms with pre-fitted KMeans)</span>
</span></span><span class=line><span class=cl>        <span class=n>clusters</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>kmeans</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>df</span><span class=p>[[</span><span class=s1>&#39;longitude&#39;</span><span class=p>,</span> <span class=s1>&#39;latitude&#39;</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>        <span class=n>df_encoded</span><span class=p>[</span><span class=s1>&#39;cluster_label&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>clusters</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>df_encoded</span>
</span></span></code></pre></div><p><strong>Total latency:</strong> ~3ms. Doesn&rsquo;t justify a Feature Store.</p><p><strong>When to reconsider:</strong></p><ul><li>If you add features like &ldquo;average house price in zipcode&rdquo; (requires DB query)</li><li>If preprocessing goes above >20ms</li><li>If you add a second model that reuses 50%+ of features</li></ul><p>Until then, YAGNI (You Ain&rsquo;t Gonna Need It).</p><hr><h3 id=127-production-readiness-an-honest-checklist>12.7. Production Readiness: An Honest Checklist<a hidden class=anchor aria-hidden=true href=#127-production-readiness-an-honest-checklist>#</a></h3><p>Based on exhaustive code analysis, here&rsquo;s the <strong>real</strong> state of this project:</p><h4 id=what-this-project-does-very-well>What This Project Does VERY WELL<a hidden class=anchor aria-hidden=true href=#what-this-project-does-very-well>#</a></h4><p><strong>Level 3/5 in MLOps Maturity (Production-Ready):</strong></p><ol><li><p><strong>Complete versioning</strong></p><ul><li>Models in MLflow Registry with rich metadata</li><li>Data artifacts in GCS with timestamps</li><li>Code in git with CI/CD</li><li>Config in versioned YAML</li></ul></li><li><p><strong>Reproducibility</strong></p><ul><li>Fixed seeds (random_state=42 everywhere)</li><li>Pinned dependencies (requirements.txt)</li><li>Docker for environment consistency</li></ul></li><li><p><strong>Testing</strong></p><ul><li>87% code coverage</li><li>Unit tests with realistic fixtures</li><li>End-to-end integration tests</li><li>Security scanning (Bandit, TruffleHog)</li></ul></li><li><p><strong>CI/CD</strong></p><ul><li>GitHub Actions with automated tests</li><li>Docker build in CI</li><li>Cloud Run deployment with health checks</li><li>Staging/Production separation
<img loading=lazy></li></ul></li><li><p><strong>API Design</strong></p><ul><li>Pydantic validation on all endpoints</li><li>Cascade fallback (MLflow→GCS→Local)</li><li>Lifespan management (load model once, not per request)</li><li>Batch prediction support</li></ul></li><li><p><strong>Observability (Basic)</strong></p><ul><li>W&amp;B logging of predictions</li><li>Response time tracking</li><li>Structured logging</li></ul></li></ol><h4 id=whats-missing-and-when-to-add-it>What&rsquo;s Missing (And When To Add It)<a hidden class=anchor aria-hidden=true href=#whats-missing-and-when-to-add-it>#</a></h4><p><strong>Level 4/5 Features (Add When You Have 10k+ Daily Predictions):</strong></p><ol><li><p><strong>Data Drift Detection</strong> [MISSING]</p><ul><li><strong>Impact:</strong> High (model fails silently)</li><li><strong>Implementation cost:</strong> Medium (Evidently AI)</li><li><strong>When:</strong> After 3 months in production</li></ul></li><li><p><strong>Model Performance Tracking</strong> [MISSING]</p><ul><li><strong>Impact:</strong> High (don&rsquo;t know if model degrades)</li><li><strong>Cost:</strong> Low (extend W&amp;B logger)</li><li><strong>When:</strong> After having ground truth labels (1-2 months)</li></ul></li><li><p><strong>Circuit Breakers</strong> [MISSING]</p><ul><li><strong>Impact:</strong> Medium (better latency during failures)</li><li><strong>Cost:</strong> Low (<code>circuitbreaker</code> library)</li><li><strong>When:</strong> If you see transient failures in MLflow/GCS</li></ul></li><li><p><strong>Advanced Monitoring Dashboards</strong> [MISSING]</p><ul><li><strong>Impact:</strong> Medium (better debugging)</li><li><strong>Cost:</strong> Medium (Grafana + Prometheus)</li><li><strong>When:</strong> When team grows >5 people</li></ul></li><li><p><strong>Canary Deployments</strong> [MISSING]</p><ul><li><strong>Impact:</strong> Low (you have manual rollback that works)</li><li><strong>Cost:</strong> High (requires traffic splitting)</li><li><strong>When:</strong> Only if deploying >1x/week</li></ul></li><li><p><strong>Feature Store</strong> [MISSING]</p><ul><li><strong>Impact:</strong> None (features are lightweight)</li><li><strong>Cost:</strong> High ($500-2000/month)</li><li><strong>When:</strong> Never, unless you add heavy features</li></ul></li></ol><p><strong>Level 5/5 Features (Overkill For This Project):</strong></p><ul><li>Multi-model orchestration (A/B testing)</li><li>Real-time retraining</li><li>Federated learning</li><li>AutoML pipeline</li></ul><h4 id=prioritized-recommendations>Prioritized Recommendations<a hidden class=anchor aria-hidden=true href=#prioritized-recommendations>#</a></h4><p><strong>MONTH 1-3 (Stabilization):</strong></p><ol><li>Add <code>/health/deep</code> endpoint with dependency checks</li><li>Implement retry with exponential backoff on GCS calls</li><li>Configure alerts in W&amp;B when MAPE > 12%</li></ol><p><strong>MONTH 4-6 (Monitoring):</strong></p><ol start=4><li>Implement Evidently AI for data drift (PSI tracking)</li><li>Add prediction distribution monitoring</li><li>Configure automated retraining trigger when PSI > 0.2</li></ol><p><strong>MONTH 7-12 (Optimization):</strong></p><ol start=7><li>Implement circuit breaker on MLflow calls</li><li>Add Redis for prediction caching (if latency is an issue)</li><li>Configure Grafana dashboard for business metrics</li></ol><p><strong>DON&rsquo;T Do (Until You Scale 10x):</strong></p><ul><li>Don&rsquo;t implement Feature Store</li><li>Don&rsquo;t add Kafka streaming</li><li>Don&rsquo;t use Kubernetes (Cloud Run is sufficient)</li><li>Don&rsquo;t implement multi-model serving (until clear use case)</li></ul><hr><h3 id=128-the-difference-between-works-and-works-in-production>12.8. The Difference Between &ldquo;Works&rdquo; and &ldquo;Works in Production&rdquo;<a hidden class=anchor aria-hidden=true href=#128-the-difference-between-works-and-works-in-production>#</a></h3><p>This project is in the top 10% of ML projects in terms of engineering practices. Most models in production have:</p><ul><li>Notebooks instead of modular scripts</li><li>Models saved as <code>model_v3_FINAL_FINAL.pkl</code></li><li>Zero tests</li><li>Manual deployment with <code>scp</code></li><li>No monitoring</li></ul><p>This project has:</p><ul><li>Modular and testable code</li><li>MLflow Registry with semantic versioning</li><li>87% test coverage</li><li>Automated deployment with GitHub Actions</li><li>Basic W&amp;B monitoring</li></ul><p><strong>The remaining gap</strong> (drift detection, advanced monitoring, circuit breakers) is the gap between &ldquo;stable production&rdquo; and &ldquo;enterprise-grade production&rdquo;.</p><p>But here&rsquo;s the secret: <strong>that gap only matters when you have real users and significant traffic.</strong></p><p>Don&rsquo;t optimize for problems you don&rsquo;t have yet. This project is ready to serve 100k predictions/month without breaking a sweat. When you reach 1M/month, then add data drift detection. When you reach 10M/month, then consider Kubernetes.</p><p>As Huyen says: <strong>&ldquo;The best ML system is the simplest one that meets your requirements.&rdquo;</strong></p><p>This project fulfills that principle perfectly.</p><hr><p><a name=conclusiones></a></p><h2 id=14-conclusions-mlops-as-an-engineering-discipline>14. Conclusions: MLOps As an Engineering Discipline<a hidden class=anchor aria-hidden=true href=#14-conclusions-mlops-as-an-engineering-discipline>#</a></h2><h3 id=what-this-pipeline-implements-and-why-it-matters>What This Pipeline Implements (And Why It Matters)<a hidden class=anchor aria-hidden=true href=#what-this-pipeline-implements-and-why-it-matters>#</a></h3><p>This is not a scikit-learn tutorial. It&rsquo;s a <strong>production-ready system</strong> that implements:</p><ol><li><strong>Complete versioning:</strong> Data (GCS), code (git), models (MLflow), configuration (YAML)</li><li><strong>Reproducibility:</strong> Same code + same config + same seed = same model</li><li><strong>Observability:</strong> Structured logs, metrics in W&amp;B, tracking in MLflow</li><li><strong>Testing:</strong> 87% coverage, unit tests, integration tests, security scanning</li><li><strong>CI/CD:</strong> GitHub Actions with automated deployment to Cloud Run</li><li><strong>Deployment:</strong> REST API with FastAPI, frontend with Streamlit, Docker Compose ready</li><li><strong>Data-backed decisions:</strong> Every choice (imputation, K clusters, hyperparameters) has quantifiable metrics</li><li><strong>Production patterns:</strong> Transform pattern, cascade fallback, training/serving consistency</li></ol><h3 id=the-anti-patterns-it-avoids-that-kill-projects>The Anti-Patterns It Avoids (That Kill Projects)<a hidden class=anchor aria-hidden=true href=#the-anti-patterns-it-avoids-that-kill-projects>#</a></h3><p><strong>X Notebooks in production:</strong> Everything is modular and testable Python. Notebooks are great for exploration, terrible for reliable systems.</p><p><strong>X Hardcoded configuration:</strong> config.yaml versioned in git. If you change a parameter, it&rsquo;s recorded with timestamp and author.</p><p><strong>X &ldquo;I used median because yes&rdquo;:</strong> Compared 4 imputation strategies with quantifiable metrics. Best strategy (Iterative Imputer) won by 3.2% in RMSE.</p><p><strong>X Models as <code>final_v3_REAL_final.pkl</code>:</strong> MLflow Registry with semantic versions and rich metadata. You know exactly what hyperparameters, what data, and what metrics each version has.</p><p><strong>X &ldquo;I don&rsquo;t know what hyperparameters I used 3 months ago&rdquo;:</strong> Each model records 106 lines of metadata. Includes everything from hyperparameters to error distribution by segment.</p><p><strong>X Manual deployment with scp:</strong> Docker + GitHub Actions. Push to master → tests run → if they pass, deploys to staging automatically. Production requires manual approval (as it should).</p><p><strong>X Training/Serving Skew:</strong> Preprocessing is in a shared class between training and serving. Change code once, both environments update.</p><h3 id=conscious-trade-offs-because-there-are-no-perfect-solutions>Conscious Trade-Offs (Because There Are No Perfect Solutions)<a hidden class=anchor aria-hidden=true href=#conscious-trade-offs-because-there-are-no-perfect-solutions>#</a></h3><p>This project makes deliberate decisions. Here are the trade-offs and when to reconsider them:</p><p><strong>1. Cluster optimization independent of final model:</strong></p><p>Optimizes KMeans with silhouette score instead of cross-validation of complete model. <strong>Faster but less rigorous.</strong> Reconsider if clustering is your model&rsquo;s most important feature.</p><p><strong>2. 60 sweep runs in W&amp;B:</strong></p><p>Sufficient for California Housing (medium dataset, ~20k samples). <strong>You might need 200+ runs</strong> on complex datasets with many non-linear interactions.</p><p><strong>3. Sequential pipeline without parallelization:</strong></p><p>Steps run one after another. This pipeline takes ~15 minutes end-to-end. If your pipeline takes hours, use Airflow/Prefect with parallel tasks.</p><p><strong>4. MAPE as primary metric:</strong></p><p>Works for this dataset (prices between $50k-$500k). <strong>Doesn&rsquo;t work</strong> if you have values close to zero (division by zero) or if you want to penalize large errors disproportionately (use RMSE).</p><p><strong>5. Data drift detection absent:</strong></p><p>As the Production Checklist explains (Section 13.7), drift monitoring should be added <strong>after 3-6 months in production</strong>, not Day 1. You need baseline of normal behavior first.</p><p><strong>6. Synthetic KMeans in the API:</strong></p><p>The Transform Pattern (Section 13.1) recreates clusters with ~2% drift vs training. <strong>Impact on MAPE: &lt;0.3%.</strong> If you need 100% bit-by-bit reproducibility, serialize the real KMeans (cost: 96KB per model version).</p><h3 id=whats-missing-and-when-to-add-it-1>What&rsquo;s Missing (And When To Add It)<a hidden class=anchor aria-hidden=true href=#whats-missing-and-when-to-add-it-1>#</a></h3><p>As Section 13 (Production Patterns) details, this project is at <strong>Level 3/5 of MLOps Maturity</strong>. What&rsquo;s missing:</p><p><strong>Month 1-3 (Stabilization):</strong></p><ul><li>Deep health check endpoint with dependency status</li><li>Retry with exponential backoff on GCS calls</li><li>Automatic alerts in W&amp;B when MAPE > threshold</li></ul><p><strong>Month 4-6 (Monitoring):</strong></p><ul><li>Evidently AI for data drift detection (PSI tracking)</li><li>Prediction distribution monitoring (detect broken model)</li><li>Automatic retraining trigger when PSI > 0.2</li></ul><p><strong>Month 7-12 (Optimization):</strong></p><ul><li>Circuit breaker on MLflow calls (avoid cascading timeouts)</li><li>Redis for prediction caching (if latency &lt;10ms is critical)</li><li>Grafana dashboards for business metrics</li></ul><p><strong>DON&rsquo;T do (until you scale 10x):</strong></p><ul><li>Feature Store (features are lightweight, &lt;3ms)</li><li>Kafka streaming (Cloud Run with HTTP is sufficient)</li><li>Kubernetes (Cloud Run autoscales without complexity)</li><li>Multi-model A/B testing (until clear use case)</li></ul><h3 id=the-uncomfortable-truth-about-mlops>The Uncomfortable Truth About MLOps<a hidden class=anchor aria-hidden=true href=#the-uncomfortable-truth-about-mlops>#</a></h3><p>90% of ML models never reach production. Of those that do, 60% fail in the first 6 months.</p><p><strong>Why?</strong></p><p>Not because the models are bad. It&rsquo;s because:</p><ul><li>The engineer who trained the model is no longer at the company</li><li>Nobody knows what hyperparameters were used</li><li>Preprocessing in production is different from training</li><li>There are no tests, so every change breaks something</li><li>Deployment is manual, takes 3 hours and fails 1 out of 3 times</li><li>There&rsquo;s no monitoring, model fails silently for months</li></ul><p>This project avoids all those problems. <strong>Not because it&rsquo;s perfect</strong>, but because it implements basic software engineering principles:</p><ul><li><strong>Versioning:</strong> Everything (data, code, models, config)</li><li><strong>Testing:</strong> 87% coverage, CI on each commit</li><li><strong>Reproducibility:</strong> Fixed seeds, Dockerized environments</li><li><strong>Observability:</strong> Logs, metrics, tracking</li><li><strong>Automation:</strong> Deployment without human intervention</li></ul><h3 id=the-most-important-lesson>The Most Important Lesson<a hidden class=anchor aria-hidden=true href=#the-most-important-lesson>#</a></h3><p>Chip Huyen says it better than I can in &ldquo;Designing Machine Learning Systems&rdquo;:</p><blockquote><p>&ldquo;The best ML system is not the one with the highest accuracy. It&rsquo;s the one that&rsquo;s reliable, maintainable, and meets business requirements.&rdquo;</p></blockquote><p>This project doesn&rsquo;t have the best model. You can probably improve MAPE from 8.2% to 7.5% with hand-tuned XGBoost.</p><p><strong>But that doesn&rsquo;t matter.</strong></p><p>What matters is that this system:</p><ul><li>Runs reliably 24/7</li><li>Can be updated without downtime</li><li>Has automatic rollback if something fails</li><li>Any team member can understand and modify the code</li><li>Logs enough information to debug problems</li><li>Costs &lt;$100/month on GCP (up to 1M predictions)</li></ul><p><strong>That 0.7% improvement in MAPE isn&rsquo;t worth it if the system is impossible to maintain.</strong></p><h3 id=who-this-post-is-for>Who This Post Is For<a hidden class=anchor aria-hidden=true href=#who-this-post-is-for>#</a></h3><p>If you are:</p><ul><li><strong>Data Scientist</strong> trying to take your first model to production → This is your roadmap</li><li><strong>ML Engineer</strong> explaining why &ldquo;you can&rsquo;t just deploy the notebook&rdquo; → Send this post</li><li><strong>Engineering Manager</strong> evaluating if your team does MLOps correctly → Use Section 13.7 as checklist</li><li><strong>Student</strong> wanting to learn MLOps beyond tutorials → This is real code, not synthetic</li></ul><h3 id=the-next-step>The Next Step<a hidden class=anchor aria-hidden=true href=#the-next-step>#</a></h3><p>This post has 6,500+ lines because I didn&rsquo;t want to simplify. MLOps is complex. There are trade-offs in every decision.</p><p>But don&rsquo;t let complexity paralyze you. <strong>Start simple, iterate, improve.</strong></p><ol><li><strong>Week 1:</strong> Basic versioning (git + requirements.txt)</li><li><strong>Week 2:</strong> Basic tests (at least smoke tests)</li><li><strong>Week 3:</strong> Docker for consistent deployment</li><li><strong>Week 4:</strong> Basic CI (GitHub Actions running tests)</li><li><strong>Month 2:</strong> MLflow for model registry</li><li><strong>Month 3:</strong> Basic monitoring (W&amp;B or Prometheus)</li></ol><p><strong>You don&rsquo;t need to implement everything on Day 1.</strong> This project took months to reach this state.</p><h3 id=the-last-word>The Last Word<a hidden class=anchor aria-hidden=true href=#the-last-word>#</a></h3><p><strong>Being an MLOps engineer is not just training models—it&rsquo;s building systems where models are one more piece.</strong></p><p>What separates a research project from a production product is:</p><ul><li><strong>Order:</strong> Everything in its place (not &ldquo;works on my machine&rdquo;)</li><li><strong>Testing:</strong> What isn&rsquo;t tested, breaks (87% coverage is not an accident)</li><li><strong>Observability:</strong> If you can&rsquo;t measure it, you can&rsquo;t improve it (W&amp;B + MLflow)</li><li><strong>Reproducibility:</strong> Today and in 6 months should give the same result (fixed seeds, Docker)</li><li><strong>Automation:</strong> Humans are bad at repetitive tasks (CI/CD)</li><li><strong>Humility:</strong> Recognizing what&rsquo;s missing and when to add it (Section 13.7)</li></ul><p>This post doesn&rsquo;t teach you to be better at machine learning.</p><p><strong>It teaches you to be better at machine learning engineering.</strong></p><p>And that difference is what separates models in notebooks from models in production creating real value.</p><hr><p>If you implement even 50% of what&rsquo;s in this post, your pipeline will be in the top 10% of ML projects in terms of engineering practices.</p><p>If you implement 80%, you&rsquo;ll be ready to scale to millions of predictions without restructuring everything.</p><p>100% is overkill for most projects. Use the Production Checklist (Section 13.7) to prioritize what you need and when.</p><hr><h2 id=references-and-resources>References and Resources<a hidden class=anchor aria-hidden=true href=#references-and-resources>#</a></h2><p><strong>Fundamental books:</strong></p><ul><li>Géron, A. (2022). <em>Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow</em> (3rd ed.). O&rsquo;Reilly.<ul><li><strong>Chapter 2:</strong> Base of this project (California Housing dataset, feature engineering, model selection)</li><li>Focus on ML, this post adds production infrastructure</li></ul></li><li>Huyen, C. (2022). <em>Designing Machine Learning Systems</em>. O&rsquo;Reilly.<ul><li><strong>Chapter 5:</strong> Feature stores and when you don&rsquo;t need one</li><li><strong>Chapter 6:</strong> Deployment patterns (Cascade, Circuit Breaker)</li><li><strong>Chapter 7:</strong> Transform Pattern and Training/Serving Skew (Sections 13.1 and 13.2 of this post)</li><li><strong>Chapter 8:</strong> Data Distribution Shifts and drift detection (Section 13.3)</li><li><strong>Complete book:</strong> If you only read one book about MLOps, make it this one</li></ul></li></ul><p><strong>Tools (with links to docs):</strong></p><ul><li><a href=https://mlflow.org/>MLflow</a>: Model registry and experiment tracking</li><li><a href=https://wandb.ai/>Weights & Biases</a>: Sweep and experiment visualization</li><li><a href=https://hydra.cc/>Hydra</a>: Configuration management with composable configs</li><li><a href=https://fastapi.tiangolo.com/>FastAPI</a>: REST API framework with Pydantic validation</li><li><a href=https://streamlit.io/>Streamlit</a>: Interactive frontend for ML apps</li><li><a href=https://cloud.google.com/storage>Google Cloud Storage</a>: Artifact storage</li><li><a href=https://evidentlyai.com/>Evidently AI</a>: Data drift detection (recommended for production)</li><li><a href=https://www.docker.com/>Docker</a>: Containerization and reproducibility</li></ul><p><strong>Complete repository:</strong></p><ul><li><a href=https://github.com/carlosjimenez88M/mlops-hand-on-ML-and-pytorch/tree/cap2-end_to_end/cap2-end_to_end>github.com/carlosjimenez88M/mlops-hand-on-ML-and-pytorch</a><ul><li><code>/api</code>: FastAPI with cascade fallback and Transform Pattern</li><li><code>/src</code>: Modular pipeline (01-07) with MLflow tracking</li><li><code>/tests</code>: 87% coverage with realistic fixtures</li><li><code>/.github/workflows</code>: Complete CI/CD with security scanning</li></ul></li></ul><hr><p><strong>Author:</strong> Carlos Daniel Jiménez
<strong>Email:</strong> <a href=mailto:danieljimenez88m@gmail.com>danieljimenez88m@gmail.com</a>
<strong>LinkedIn:</strong> <a href=https://linkedin.com/in/carlosdanieljimenez>linkedin.com/in/carlosdanieljimenez</a>
<strong>Date:</strong> January 2026</p><hr><h2 id=navigation>Navigation<a hidden class=anchor aria-hidden=true href=#navigation>#</a></h2><p><strong><a href=/post/anatomia-pipeline-mlops-part-2-en/>← Part 2: Deployment and Infrastructure</a></strong> | <strong><a href=/post/anatomia-pipeline-mlops-part-1-en/>← Part 1: Pipeline and Orchestration</a></strong></p><p><strong>Complete series:</strong></p><ol><li><a href=/post/anatomia-pipeline-mlops-part-1-en/>Part 1: Pipeline and Orchestration</a></li><li><a href=/post/anatomia-pipeline-mlops-part-2-en/>Part 2: Deployment and Infrastructure</a></li><li>Part 3: Production and Best Practices (current)</li></ol><p><strong>Repository:</strong> <a href=https://github.com/carlosjimenez88M/mlops-hand-on-ML-and-pytorch/tree/cap2-end_to_end/cap2-end_to_end>github.com/carlosjimenez88M/mlops-hand-on-ML-and-pytorch</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://carlosdanieljimenez.com/tags/mlops/>Mlops</a></li><li><a href=https://carlosdanieljimenez.com/tags/testing/>Testing</a></li><li><a href=https://carlosdanieljimenez.com/tags/production/>Production</a></li><li><a href=https://carlosdanieljimenez.com/tags/data-drift/>Data-Drift</a></li><li><a href=https://carlosdanieljimenez.com/tags/monitoring/>Monitoring</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://carlosdanieljimenez.com/>The Probability Engine</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div class=newsletter-cta><div class=newsletter-content><h3>📬 Did this help?</h3><p>I write about MLOps, Edge AI, and making models work outside the lab.
One email per month, max. No spam, no course pitches, just technical content.</p><form action=https://buttondown.com/api/emails/embed-subscribe/carlosjimenez88m method=post class=newsletter-form target=popupwindow onsubmit='window.open("https://buttondown.com/carlosjimenez88m","popupwindow")'><div class=form-group><input type=email name=email id=bd-email placeholder=your@email.com required aria-label="Email address">
<input type=submit value=Subscribe></div></form><p class=newsletter-stats>Join engineers building ML systems and Edge Computing infrastructure.</p></div></div><style>.newsletter-cta{margin:3rem auto 2rem;padding:2rem;max-width:650px;background:var(--entry);border:1px solid var(--border);border-radius:8px;text-align:center}.newsletter-content h3{margin:0 0 1rem;font-size:1.5rem;color:var(--primary)}.newsletter-content p{margin:0 0 1.5rem;color:var(--secondary);line-height:1.6}.newsletter-form{margin:1.5rem 0}.form-group{display:flex;gap:.5rem;max-width:500px;margin:0 auto;flex-wrap:wrap;justify-content:center}.newsletter-form input[type=email]{flex:1;min-width:250px;padding:.75rem 1rem;font-size:1rem;border:1px solid var(--border);border-radius:6px;background:var(--theme);color:var(--content);transition:border-color .2s ease}.newsletter-form input[type=email]:focus{outline:none;border-color:var(--primary);box-shadow:0 0 0 3px rgba(var(--primary-rgb),.1)}.newsletter-form input[type=submit]{padding:.75rem 2rem;font-size:1rem;font-weight:600;color:#fff;background:var(--primary);border:none;border-radius:6px;cursor:pointer;transition:opacity .2s ease}.newsletter-form input[type=submit]:hover{opacity:.9}.newsletter-stats{font-size:.875rem;color:var(--secondary);margin-top:1rem;font-style:italic}@media screen and (max-width:600px){.newsletter-cta{padding:1.5rem 1rem;margin:2rem .5rem 1rem}.newsletter-content h3{font-size:1.25rem}.form-group{flex-direction:column;width:100%}.newsletter-form input[type=email],.newsletter-form input[type=submit]{width:100%;min-width:100%}}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>