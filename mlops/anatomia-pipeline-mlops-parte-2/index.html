<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura | The Probability Engine</title>
<meta name=keywords content="mlops,ci-cd,docker,fastapi,github-actions,wandb,mlflow"><meta name=description content="Parte 2: CI/CD con GitHub Actions, comparación W&amp;B vs MLflow, containerización completa con Docker, y arquitectura de API con FastAPI en producción."><meta name=author content="Carlos Daniel Jiménez"><link rel=canonical href=https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-parte-2/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=16x16 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=icon type=image/png sizes=32x32 href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=apple-touch-icon href=https://carlosdanieljimenez.com/img/icon.jpeg><link rel=mask-icon href=https://carlosdanieljimenez.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-parte-2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=icon type=image/png href=/img/icon.jpeg><link rel=apple-touch-icon href=/img/icon.jpeg><link rel="shortcut icon" href=/img/icon.jpeg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-parte-2/"><meta property="og:site_name" content="The Probability Engine"><meta property="og:title" content="Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura"><meta property="og:description" content="Parte 2: CI/CD con GitHub Actions, comparación W&amp;B vs MLflow, containerización completa con Docker, y arquitectura de API con FastAPI en producción."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="mlops"><meta property="article:published_time" content="2026-01-13T00:00:00+00:00"><meta property="article:modified_time" content="2026-01-13T00:00:00+00:00"><meta property="article:tag" content="Mlops"><meta property="article:tag" content="Ci-Cd"><meta property="article:tag" content="Docker"><meta property="article:tag" content="Fastapi"><meta property="article:tag" content="Github-Actions"><meta property="article:tag" content="Wandb"><meta name=twitter:card content="summary"><meta name=twitter:title content="Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura"><meta name=twitter:description content="Parte 2: CI/CD con GitHub Actions, comparación W&amp;B vs MLflow, containerización completa con Docker, y arquitectura de API con FastAPI en producción."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"MLOps Guides","item":"https://carlosdanieljimenez.com/mlops/"},{"@type":"ListItem","position":2,"name":"Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura","item":"https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-parte-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura","name":"Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura","description":"Parte 2: CI/CD con GitHub Actions, comparación W\u0026B vs MLflow, containerización completa con Docker, y arquitectura de API con FastAPI en producción.","keywords":["mlops","ci-cd","docker","fastapi","github-actions","wandb","mlflow"],"articleBody":" Serie MLOps Completo: ← Parte 1: Pipeline | Parte 2 (actual) | Parte 3: Producción →\n8. CI/CD con GitHub Actions: Automatización del Pipeline Completo Por Qué CI/CD Es Crítico en MLOps Como MLOps engineer, uno de los mayores puntos de fricción es el deployment manual. Has entrenado un modelo excelente en tu laptop, pero llevarlo a producción requiere:\nSSH a un servidor Copiar archivos manualmente Instalar dependencias Cruzar los dedos Debuggear cuando algo explota GitHub Actions elimina esto. Cada commit dispara un pipeline automatizado que:\nEjecuta tests Valida que el código cumple estándares Entrena el modelo (opcional, en pipelines simples) Construye imágenes Docker Deploya a Cloud Run/ECS/Kubernetes La Arquitectura de CI/CD Para Este Proyecto Este proyecto implementa dos workflows separados:\n1. PR Validation Workflow Trigger: Cada pull request a main\nPropósito: Asegurar que el código es production-ready antes de mergear\n# .github/workflows/pr_validation.yaml name: PR Validation - Tests \u0026 Linting on: pull_request: branches: [main, master] paths: - 'src/**' - 'api/**' - 'tests/**' - 'pyproject.toml' - 'requirements.txt' jobs: lint: name: Lint Code runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Install uv run: pip install uv - name: Install dependencies run: | uv venv uv pip install -e . uv pip install ruff pytest pytest-cov - name: Run Ruff linter run: | source .venv/bin/activate ruff check src/ tests/ api/ - name: Run Ruff formatter check run: | source .venv/bin/activate ruff format --check src/ tests/ api/ unit-tests: name: Unit Tests runs-on: ubuntu-latest env: GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }} WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }} steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Install dependencies run: | pip install uv uv venv uv pip install -e . uv pip install pytest pytest-cov pytest-mock - name: Run unit tests with coverage run: | source .venv/bin/activate pytest tests/ -v \\ --cov=src \\ --cov=api/app \\ --cov-report=xml \\ --cov-report=term-missing \\ --cov-fail-under=70 - name: Upload coverage to Codecov uses: codecov/codecov-action@v4 with: file: ./coverage.xml flags: unittests name: codecov-umbrella integration-tests: name: Integration Tests (Pipeline E2E) runs-on: ubuntu-latest env: GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }} WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }} MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }} steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Authenticate to Google Cloud uses: google-github-actions/auth@v2 with: credentials_json: ${{ secrets.GCP_SA_KEY }} - name: Install dependencies run: | pip install uv uv venv uv pip install -e . - name: Run integration test (Steps 01-04) run: | source .venv/bin/activate python main.py main.execute_steps=[01_download_data,02_preprocessing_and_imputation,03_feature_engineering,04_segregation] timeout-minutes: 30 - name: Verify artifacts were created run: | gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/train/train.parquet gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/test/test.parquet Valor para el MLOps engineer:\nPreviene merges rotos: Si los tests fallan, el PR no puede mergearse Estándares de código: Ruff garantiza consistencia (importa cuando tienes 5+ contributors) Coverage tracking: Codecov muestra qué porcentaje del código está cubierto por tests Fast feedback: Sabes en 5 minutos si tu cambio rompió algo, no 3 horas después 2. Deployment Workflow Trigger: Push a main (después de merge de PR)\nPropósito: Construir y deployar el API a producción\n# .github/workflows/deploy_api.yaml name: Deploy API to Cloud Run on: push: branches: [main] paths: - 'api/**' - 'models/**' - '.github/workflows/deploy_api.yaml' env: PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} SERVICE_NAME: housing-price-api REGION: us-central1 jobs: build-and-deploy: name: Build Docker Image \u0026 Deploy runs-on: ubuntu-latest permissions: contents: read id-token: write steps: - name: Checkout code uses: actions/checkout@v4 - name: Authenticate to Google Cloud uses: google-github-actions/auth@v2 with: workload_identity_provider: ${{ secrets.WIF_PROVIDER }} service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }} - name: Set up Cloud SDK uses: google-github-actions/setup-gcloud@v2 - name: Configure Docker for GCR run: gcloud auth configure-docker gcr.io - name: Download trained model from GCS run: | mkdir -p api/models/trained gsutil cp gs://${{ secrets.GCS_BUCKET_NAME }}/models/trained/housing_price_model.pkl \\ api/models/trained/housing_price_model.pkl - name: Build Docker image run: | cd api docker build \\ --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \\ --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest \\ . - name: Push Docker image to GCR run: | docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest - name: Deploy to Cloud Run run: | gcloud run deploy ${{ env.SERVICE_NAME }} \\ --image gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \\ --platform managed \\ --region ${{ env.REGION }} \\ --allow-unauthenticated \\ --set-env-vars=\"GCS_BUCKET=${{ secrets.GCS_BUCKET_NAME }},WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}\" \\ --memory 2Gi \\ --cpu 2 \\ --max-instances 10 \\ --min-instances 1 \\ --timeout 300 - name: Get Cloud Run URL id: deploy-url run: | URL=$(gcloud run services describe ${{ env.SERVICE_NAME }} \\ --platform managed \\ --region ${{ env.REGION }} \\ --format 'value(status.url)') echo \"url=$URL\" \u003e\u003e $GITHUB_OUTPUT - name: Run smoke test run: | curl -X POST \"${{ steps.deploy-url.outputs.url }}/api/v1/predict\" \\ -H \"Content-Type: application/json\" \\ -d '{\"instances\":[{\"longitude\":-122.23,\"latitude\":37.88,\"housing_median_age\":41,\"total_rooms\":880,\"total_bedrooms\":129,\"population\":322,\"households\":126,\"median_income\":8.3252,\"ocean_proximity\":\"NEAR BAY\"}]}' - name: Notify deployment success if: success() run: | echo \"Deployment successful! API available at: ${{ steps.deploy-url.outputs.url }}\" Valor para el MLOps engineer:\nZero-downtime deployment: Cloud Run hace rolling updates automáticamente Rollback fácil: Si algo explota, haces gcloud run services update-traffic --to-revisions=PREVIOUS=100 Smoke test automático: Verifica que el API responde después del deploy Versionado de imágenes: Cada commit tiene su propia imagen Docker taggeada con SHA Secretos y Seguridad CRÍTICO: Nunca commitees secrets al repo. GitHub Actions usa GitHub Secrets para guardar:\nGCP_PROJECT_ID: ID del proyecto de GCP GCS_BUCKET_NAME: Nombre del bucket de GCS WANDB_API_KEY: API key de W\u0026B GCP_SA_KEY: Service account key (JSON) para autenticar en GCP WIF_PROVIDER / WIF_SERVICE_ACCOUNT: Workload Identity Federation (más seguro que SA keys) Configuración en GitHub:\nVe a repo → Settings → Secrets and variables → Actions Crea cada secret Los workflows acceden con ${{ secrets.SECRET_NAME }} Monitoreo de Deployments ¿Cómo saber si un deployment falló?\nGitHub Actions envía notificaciones a:\nEmail (configurado en GitHub profile) Slack (con GitHub app) Discord/Teams (con webhooks) Post-deployment monitoring:\n# Agregar step de validación post-deploy - name: Run API health check run: | for i in {1..5}; do STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" \"${{ steps.deploy-url.outputs.url }}/health\") if [ $STATUS -eq 200 ]; then echo \"Health check passed\" exit 0 fi echo \"Attempt $i failed, retrying...\" sleep 10 done echo \"Health check failed after 5 attempts\" exit 1 Estrategias Avanzadas de CI/CD 1. Pipeline de Reentrenamiento Automático Trigger: Cron schedule (ejemplo: semanalmente)\non: schedule: - cron: '0 2 * * 0' # Cada domingo a las 2 AM UTC jobs: retrain-model: runs-on: ubuntu-latest steps: - name: Run full pipeline run: python main.py - name: Compare metrics with production model run: | NEW_MAPE=$(python scripts/get_latest_mape.py) PROD_MAPE=$(python scripts/get_production_mape.py) if (( $(echo \"$NEW_MAPE \u003c $PROD_MAPE\" | bc -l) )); then echo \"New model is better, promoting to Production\" mlflow models transition --name housing_price_model --version latest --stage Production else echo \"New model is worse, keeping current Production model\" fi Valor: El modelo se reentrena automáticamente con datos nuevos. Si mejora, se promociona a Production. Si empeora, se descarta.\n2. Canary Deployments Problema: Un nuevo modelo puede tener bugs sutiles que no aparecen en tests.\nSolución: Deployar el nuevo modelo a solo 10% del tráfico, monitorear por 1 hora, luego migrar 100% si no hay errores.\n- name: Deploy canary (10% traffic) run: | gcloud run services update-traffic ${{ env.SERVICE_NAME }} \\ --to-revisions=LATEST=10,PREVIOUS=90 - name: Wait and monitor run: sleep 3600 # 1 hora - name: Check error rate run: | ERROR_RATE=$(python scripts/check_error_rate.py --minutes=60) if (( $(echo \"$ERROR_RATE \u003e 0.05\" | bc -l) )); then echo \"Error rate too high, rolling back\" gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=PREVIOUS=100 exit 1 fi - name: Promote to 100% traffic run: | gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=LATEST=100 Lo Que CI/CD Resuelve en MLOps Sin CI/CD:\nDeployment manual propenso a errores “Funciona en mi máquina” syndrome Testing inconsistente Rollback requiere pánico debugging No hay historial de qué se deployó cuándo Con CI/CD:\nDeployment automático en cada merge Tests garantizan que el código funciona Rollback es un comando Historial completo en GitHub Actions UI Cada deployment es reproducible El Valor Real Para el MLOps Engineer No es sobre automatizar por automatizar. Es sobre:\nReducir toil: Gastas tiempo resolviendo problemas interesantes, no copiando archivos manualmente Confianza: Sabes que el código funciona antes de llegar a producción Velocidad: De commit a producción en \u003c10 minutos Auditoría: Cada cambio está loggeado en GitHub Colaboración: Tu equipo puede deployar sin depender de ti Un MLOps engineer sin CI/CD es como un software engineer sin git—técnicamente posible, pero fundamentalmente broken.\n9. El Valor de MLOps: Por Qué Esto Importa La Pregunta Central “¿Por qué debería invertir tiempo en todo esto cuando puedo entrenar un modelo en un notebook en 30 minutos?”\nEsta es la pregunta que todo MLOps engineer ha escuchado. La respuesta corta: porque el notebook no escala.\nLa respuesta larga es lo que cubre esta sección.\nEl Problema Real: Research Code vs Production Code Research Code (Notebook) # notebook.ipynb # Cell 1 import pandas as pd df = pd.read_csv('housing.csv') # Cell 2 df = df.dropna() # Cell 3 from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor(n_estimators=100) model.fit(X_train, y_train) # Cell 4 import pickle pickle.dump(model, open('model.pkl', 'wb')) # Cell 5 # Wait, did I drop the right columns? # Let me rerun cell 2... oh no, I ran it twice # Now I have 0 rows, what happened? Problemas:\nNo reproducible (orden de ejecución importa) No testeable No versionable (git diffs son ilegibles) No escalable (qué pasa con 100GB de datos?) No auditable (qué params usaste?) Production Code (Este Pipeline) # src/model/05_model_selection/main.py @hydra.main(config_path=\".\", config_name=\"config\") def train(config: DictConfig) -\u003e None: \"\"\"Entrenar modelo con configuración versionada.\"\"\" # Cargar datos desde GCS (single source of truth) df = load_from_gcs(config.gcs_train_path) # Aplicar preprocessing pipeline serializado pipeline = joblib.load('artifacts/preprocessing_pipeline.pkl') X = pipeline.transform(df) # Entrenar con params de config model = RandomForestRegressor(**config.hyperparameters) model.fit(X, y) # Loggear a MLflow mlflow.log_params(config.hyperparameters) mlflow.log_metrics(evaluate(model, X_test, y_test)) mlflow.sklearn.log_model(model, \"model\") return model Beneficios:\nReproducible (mismo config = mismo output) Testeable (funciones puras, mocking) Versionable (git diff legible) Escalable (corre en local o en cluster) Auditable (MLflow tracking) Valor #1: Modularización de Código Por Qué Importa Escenario: Tu modelo tiene bug en preprocessing. En notebook, el preprocessing está mezclado con feature engineering, entrenamiento y evaluación en 300 líneas.\nEn este pipeline:\n# Bug está en preprocessing → solo editas src/data/02_preprocessing/ # Tests fallan → pytest tests/test_preprocessor.py # Fixeas → reejecutas solo step 02-07, no 01 Tiempo ahorrado: Horas por bug.\nSeparation of Concerns Este pipeline separa:\nData steps (01-04): Producen artifacts reutilizables Model steps (05-07): Consumen artifacts, producen modelos API: Consume modelos, produce predicciones Frontend: Consume API, produce UX Beneficio: Equipos pueden trabajar en paralelo. El data scientist modifica feature engineering sin tocar el API. El frontend engineer modifica UI sin entender Random Forests.\nValor #2: Working with Artifacts El Problema: “¿Dónde está el model_final_v3.pkl?” Sin artifact management:\nmodels/ ├── model_v1.pkl ├── model_v2.pkl ├── model_final.pkl ├── model_final_FINAL.pkl ├── model_final_REAL.pkl ├── model_production_2024_01_15.pkl # ¿Este es el de producción? └── model_old_backup.pkl # ¿Puedo borrarlo? Problemas:\nNo sabes qué hiperparámetros usa cada uno No sabes qué métricas logró No sabes con qué datos se entrenó Rollback = buscar el archivo correcto La Solución: Artifact Storage + Metadata 1. Google Cloud Storage para datos:\ngs://bucket-name/ ├── data/ │ ├── 01-raw/housing.parquet # Inmutable │ ├── 02-processed/housing_processed.parquet # Versionado por fecha │ ├── 03-features/housing_features.parquet │ └── 04-split/ │ ├── train/train.parquet │ └── test/test.parquet ├── artifacts/ │ ├── imputer.pkl # Preprocessing artifacts │ ├── preprocessing_pipeline.pkl │ └── scaler.pkl └── models/ └── trained/housing_price_model.pkl # Latest trained Beneficios:\nInmutabilidad: 01-raw/ nunca cambia, siempre puedes reejecutar el pipeline Versionamiento: Cada run tiene timestamp, puedes comparar versiones Compartir: Todo el equipo accede a los mismos datos, no “enviame el CSV por Slack” 2. MLflow para modelos:\n# Registrar modelo mlflow.sklearn.log_model(model, \"model\") # MLflow guarda automáticamente: # - El pickle del modelo # - Los hiperparámetros (n_estimators=200, max_depth=20) # - Las métricas (MAPE=7.8%, R²=0.87) # - Metadata (fecha, duración, usuario) # - Código (git commit SHA) # Cargar modelo en producción model = mlflow.pyfunc.load_model(\"models:/housing_price_model/Production\") Beneficios:\nVersionamiento semántico: v1, v2, v3 con stages (Staging/Production) Metadata rica: Sabes exactamente qué es cada versión Rollback trivial: transition v2 to Production Comparación: MLflow UI muestra tabla comparando todas las versiones 3. W\u0026B para experimentos:\n# Cada run de sweep loggea: wandb.log({ \"hyperparameters/n_estimators\": 200, \"hyperparameters/max_depth\": 20, \"metrics/mape\": 7.8, \"metrics/r2\": 0.87, \"plots/feature_importances\": wandb.Image(fig), \"dataset/train_size\": 16512, }) # W\u0026B dashboard: # - Tabla con 50 runs de sweep # - Filtrar por MAPE \u003c 8% # - Parallel coordinates plot mostrando relación entre hiperparámetros y MAPE # - Comparar top 5 runs side-by-side Beneficios:\nVisualización: Plots interactivos de cómo cada hiperparámetro afecta métricas Colaboración: Tu equipo ve tus experimentos en real-time Reproducibilidad: Cada run tiene link permanente con todo el contexto Valor #3: Pipeline Architecture Por Qué Un Pipeline, No Un Script Script único (run_all.py):\n# run_all.py (500 líneas) def main(): # Download data df = download_data() # Preprocess df = preprocess(df) # Feature engineering df = add_features(df) # Train model model = train_model(df) # Deploy deploy_model(model) Problemas:\nSi falla en train_model(), reejecutas TODO (incluyendo download lento) No puedes ejecutar solo feature engineering para experimentar Cambiar preprocessing requiere reentrenar todo No hay checkpoints intermedios Pipeline modular:\n# Ejecutar todo make run-pipeline # Ejecutar solo preprocessing make run-preprocessing # Ejecutar desde feature engineering en adelante python main.py main.execute_steps=[03_feature_engineering,04_segregation,05_model_selection] # Debugging: ejecutar solo step que falló python src/data/03_feature_engineering/main.py --debug Beneficios:\nEjecución selectiva: Solo reejecutas lo que cambió Debugging rápido: Testeas un step aislado Paralelización: Steps independientes pueden correr en paralelo Checkpointing: Si falla step 05, steps 01-04 ya están hechos El Contrato Entre Steps Cada step:\nInput: Path a artifact en GCS (ejemplo: data/02-processed/housing_processed.parquet) Output: Path a nuevo artifact en GCS (ejemplo: data/03-features/housing_features.parquet) Side effects: Logs a MLflow/W\u0026B # Step 03: Feature Engineering def run(config): # Input df = load_from_gcs(config.gcs_input_path) # Transform df_transformed = apply_feature_engineering(df) # Output save_to_gcs(df_transformed, config.gcs_output_path) # Side effects mlflow.log_artifact(\"preprocessing_pipeline.pkl\") wandb.log({\"optimization/optimal_k\": 8}) Este contrato permite que cada step sea:\nTesteado independientemente Desarrollado por diferentes personas Reemplazado sin afectar otros steps Valor #4: Production-Ready vs Research Code Checklist de Production-Ready Feature Research Code Este Pipeline Versionamiento Git (mal, notebooks) Git + GCS + MLflow Testing Manual (“lo corrí una vez”) pytest + CI Configuración Hardcoded YAML versionado Secretos Expuestos en código .env + GitHub Secrets Logs print() statements Logging estructurado Monitoring “Espero que funcione” W\u0026B + MLflow tracking Deployment Manual CI/CD automático Rollback Panic debugging Transition en MLflow Documentación README desactualizado Código autodocumentado + Markdown en MLflow Colaboración “Ejecuta estas 10 celdas en orden” make run-pipeline El Costo Real de No Hacer MLOps Escenario: Un modelo en producción tiene bug que causa predicciones incorrectas.\nSin MLOps (Research Code):\nDetectar el bug: Usuario reporta → 2 horas Reproducir el bug: Buscar qué código/datos se usaron → 4 horas Fixear: Correr notebook localmente → 1 hora Deployar: SSH, copiar pickle, restart server → 30 min Verificar: Correr tests manuales → 1 hora Total: 8.5 horas de downtime Con MLOps (Este Pipeline):\nDetectar el bug: Monitoring automático alerta → 5 min Rollback: transition v3 to Archived + transition v2 to Production → 2 min Fix: Identificar issue con MLflow metadata, fixear código → 1 hora Deployar: Push to GitHub → CI/CD automático → 10 min Verificar: Smoke tests automáticos pasan → 1 min Total: 1 hora 18 min de downtime (\u003e85% reducción) Ahorro anualizado: Si esto pasa 4 veces al año, ahorras 29 horas de tiempo de ingeniero.\nValor #5: Decisiones Respaldadas por Datos El Anti-Pattern “Usé Random Forest con n_estimators=100 porque eso es lo que hace todo el mundo.”\nProblema: No tienes evidencia de que es la mejor opción.\nEste Pipeline Cada decisión tiene métricas cuantificables:\n1. Imputación:\nComparó 4 estrategias (Simple median, Simple mean, KNN, IterativeImputer) IterativeImputer ganó con RMSE=0.52 (vs 0.78 de median) Plot de comparación en W\u0026B: wandb.ai/project/run/imputation_comparison 2. Feature Engineering:\nOptimizó K de 5 a 15 K=8 maximizó silhouette score (0.64) Plot de elbow method en W\u0026B 3. Hyperparameter Tuning:\nSweep bayesiano de 50 runs Optimal config: n_estimators=200, max_depth=20 MAPE mejoró de 8.5% a 7.8% Link a sweep: wandb.ai/project/sweeps/abc123 Beneficio: Seis meses después, cuando el stakeholder pregunta “¿por qué usamos este modelo?”, abres W\u0026B/MLflow y la respuesta está ahí con plots y métricas.\nEl ROI de MLOps Inversión inicial:\nSetup de GCS, MLflow, W\u0026B, CI/CD: 2-3 días Refactoring de código a pipeline modular: 1-2 semanas Retorno:\nDeployment time: 8 horas → 10 minutos (48x más rápido) Debugging time: 4 horas → 30 min (8x más rápido) Onboarding nuevos engineers: 1 semana → 1 día Confianza del equipo: “Espero que funcione” → “Sé que funciona” Para un equipo de 5 personas, el breakeven es ~1 mes.\nLa Lección Final Para MLOps Engineers No es sobre las herramientas. Puedes reemplazar:\nGCS → S3 → Azure Blob MLflow → Neptune → Comet W\u0026B → TensorBoard → MLflow GitHub Actions → GitLab CI → Jenkins Es sobre los principios:\nModularización: Código en módulos testeables, no notebooks monolíticos Artifact Management: Datos y modelos versionados, no model_final_v3.pkl Automatización: CI/CD elimina toil Observabilidad: Logs, métricas, tracking Reproducibilidad: Mismo input → mismo output Decisiones data-driven: Cada elección respaldada por métricas Cuando entiendes esto, eres un MLOps engineer. Cuando lo implementas, eres un buen MLOps engineer.\n9.5. W\u0026B vs MLflow: Por Qué Ambos, No Uno u Otro La Pregunta Incómoda “¿Por qué tienes Weights \u0026 Biases Y MLflow? ¿No son lo mismo?”\nEsta pregunta revela un malentendido fundamental sobre lo que hace cada herramienta. No son competidores—son aliados con responsabilidades diferentes. Entender esto separa un data scientist que experimenta de un MLOps engineer que construye sistemas.\nLa respuesta corta: W\u0026B es tu laboratorio de investigación. MLflow es tu cadena de producción.\nLa respuesta larga es lo que cubre esta sección, con ejemplos del código real de este proyecto.\nEl Problema Real: Experimentación vs Governance Fase 1: Experimentación (50-100 runs/día) Cuando estás en fase de experimentación:\nCorres 50 sweep runs probando combinaciones de hiperparámetros Necesitas ver en tiempo real cómo evoluciona cada run Quieres comparar visualmente 20 runs simultáneos Necesitas ver plots de convergencia, distribuciones de features, confusion matrices El overhead de logging debe ser mínimo (logging asíncrono) Herramienta correcta: Weights \u0026 Biases\nFase 2: Governance y Deployment (1-2 modelos/semana) Cuando subes un modelo a producción:\nNecesitas versionamiento semántico (v1, v2, v3) Necesitas stages (Staging → Production) Necesitas metadata rica (¿qué hiperparámetros? ¿qué datos? ¿qué commit?) Necesitas un API para cargar modelos (models:/housing_price_model/Production) Necesitas rollback trivial (transition v2 to Production) Herramienta correcta: MLflow Model Registry\nLa verdad incómoda: Ninguna herramienta hace bien ambas cosas.\nCómo Este Proyecto Usa W\u0026B 1. Hyperparameter Sweep (Step 06): Bayesian Optimization # src/model/06_sweep/main.py # Configuración del sweep (Bayesian optimization) sweep_config = { \"method\": \"bayes\", # Bayesian \u003e Grid \u003e Random \"metric\": { \"name\": \"wmape\", \"goal\": \"minimize\" }, \"early_terminate\": { \"type\": \"hyperband\", \"min_iter\": 3 }, \"parameters\": { \"n_estimators\": {\"min\": 50, \"max\": 300}, \"max_depth\": {\"min\": 5, \"max\": 30}, \"min_samples_split\": {\"min\": 2, \"max\": 20}, \"min_samples_leaf\": {\"min\": 1, \"max\": 10} } } # Inicializar sweep sweep_id = wandb.sweep(sweep=sweep_config, project=\"housing-mlops-gcp\") # Función de training que W\u0026B llama 50 veces def train(): run = wandb.init() # W\u0026B asigna hiperparámetros automáticamente # Obtener hiperparámetros sugeridos por Bayesian optimizer config = wandb.config # Entrenar modelo model = RandomForestRegressor( n_estimators=config.n_estimators, max_depth=config.max_depth, # ... ) model.fit(X_train, y_train) # Evaluar metrics = evaluate_model(model, X_test, y_test) # Log a W\u0026B (asíncrono, no bloquea) wandb.log({ \"hyperparameters/n_estimators\": config.n_estimators, \"hyperparameters/max_depth\": config.max_depth, \"metrics/mape\": metrics['mape'], \"metrics/wmape\": metrics['wmape'], # Optimizer usa esto \"metrics/r2\": metrics['r2'], \"plots/feature_importances\": wandb.Image(fig), }) run.finish() # Ejecutar 50 runs con Bayesian optimization wandb.agent(sweep_id, function=train, count=50) Lo que W\u0026B hace aquí que MLflow no puede:\nBayesian Optimization: W\u0026B sugiere los próximos hiperparámetros basándose en runs previos. No es random—usa Gaussian Processes para explorar el espacio eficientemente.\nRun 1: n_estimators=100, max_depth=15 → wMAPE=8.5% Run 2: n_estimators=200, max_depth=20 → wMAPE=7.9% # Mejor Run 3: n_estimators=250, max_depth=22 → wMAPE=7.8% # W\u0026B sugiere valores cercanos a Run 2 Early Termination (Hyperband): Si un run va mal en las primeras 3 iteraciones (epochs), W\u0026B lo mata automáticamente y prueba otros hiperparámetros. Ahorra ~40% de compute.\n\"early_terminate\": { \"type\": \"hyperband\", \"min_iter\": 3 # Mínimo 3 iteraciones antes de terminar } Parallel Coordinates Plot: Visualización interactiva mostrando qué combinación de hiperparámetros produce mejor wMAPE.\nInterpretación: Las líneas azules (runs con wMAPE bajo) convergen en n_estimators=200-250 y max_depth=20-25. Esto te dice visualmente dónde está el óptimo.\nLogging Asíncrono: wandb.log() no bloquea. Mientras el modelo entrena, W\u0026B sube métricas en background. Total overhead: \u003c1% del training time.\nMLflow no tiene:\nBayesian optimization (solo Grid/Random search vía scikit-learn) Early termination inteligente Parallel coordinates plots Logging asíncrono (mlflow.log es síncrono) 2. Real-Time Monitoring: Ver Runs Mientras Corren # En W\u0026B dashboard (web UI): # - Ver 50 runs simultáneos en tabla interactiva # - Filtrar por \"wmape \u003c 8.0%\" → muestra solo 12 runs # - Comparar top 5 runs side-by-side # - Ver plots de convergencia (MAPE vs iteration) Caso de uso real: Inicias un sweep de 50 runs a las 9 AM. A las 10 AM, desde tu laptop en la cafetería:\nAbres W\u0026B dashboard Ves que 30 runs ya terminaron Filtras por wmape \u003c 8.0% → 8 runs cumplen Comparas esos 8 runs → identificas que max_depth=20 aparece en todos Decisión: Cancelas el sweep, ajustas el range de max_depth a [18, 25], reinicias Valor: Retroalimentación inmediata sin SSH al server, sin leer logs en terminal. La experimentación es interactiva, no batch.\n3. Artifact Tracking Ligero (Referencias a GCS) # src/model/05_model_selection/main.py # Upload modelo a GCS model_gcs_uri = upload_model_to_gcs(model, \"models/05-selection/randomforest_best.pkl\") # gs://bucket/models/05-selection/randomforest_best.pkl # Log referencia en W\u0026B (NO sube el pickle, solo el URI) artifact = wandb.Artifact( name=\"best_model_selection\", type=\"model\", description=\"Best model selected: RandomForest\" ) artifact.add_reference(model_gcs_uri, name=\"best_model.pkl\") # Solo el URI run.log_artifact(artifact) W\u0026B no almacena el modelo—solo guarda el URI gs://.... El modelo vive en GCS.\nVentaja: No pagas doble storage (GCS + W\u0026B). W\u0026B es el índice, GCS es el almacén.\nCómo Este Proyecto Usa MLflow 1. Model Registry (Step 07): Versionamiento y Stages # src/model/07_registration/main.py with mlflow.start_run(run_name=\"model_registration\"): # Log modelo mlflow.sklearn.log_model(model, \"model\") # Log params y métricas mlflow.log_params({ \"n_estimators\": 200, \"max_depth\": 20, \"min_samples_split\": 2 }) mlflow.log_metrics({ \"mape\": 7.82, \"r2\": 0.8654 }) # Registrar en Model Registry client = MlflowClient() # Crear modelo registrado (si no existe) client.create_registered_model( name=\"housing_price_model\", description=\"Housing price prediction - Random Forest\" ) # Crear nueva versión model_version = client.create_model_version( name=\"housing_price_model\", source=f\"runs:/{run_id}/model\", run_id=run_id ) # Resultado: housing_price_model/v3 # Transicionar a stage client.transition_model_version_stage( name=\"housing_price_model\", version=model_version.version, stage=\"Staging\" # Staging → Production cuando se valide ) Lo que MLflow hace aquí que W\u0026B no puede:\nVersionamiento Semántico: Cada modelo es housing_price_model/v1, v2, v3. No son IDs aleatorios—son versiones incrementales.\nStages: Un modelo pasa por None → Staging → Production → Archived. Este lifecycle es explícito.\nv1: Production (actual en el API) v2: Staging (validándose) v3: None (recién entrenado) v4: Archived (deprecado) Model-as-Code API: Cargar modelo en el API es trivial:\n# api/app/core/model_loader.py model = mlflow.pyfunc.load_model(\"models:/housing_price_model/Production\") No necesitas saber:\nDónde está el pickle físicamente Qué versión es (MLflow resuelve “Production” → v1) Cómo deserializarlo (mlflow.pyfunc abstrae esto) Rollback en 10 Segundos:\n# Problema: v3 en Production tiene bug # Rollback a v2: mlflow models transition \\ --name housing_price_model \\ --version 2 \\ --stage Production # El API detecta el cambio y recarga v2 automáticamente Metadata Rica con Tags y Description:\n# Agregar tags searchables client.set_model_version_tag( \"housing_price_model\", version, \"training_date\", \"2026-01-13\" ) client.set_model_version_tag( \"housing_price_model\", version, \"sweep_id\", \"abc123xyz\" # Link al W\u0026B sweep ) # Description en Markdown client.update_model_version( name=\"housing_price_model\", version=version, description=\"\"\" # Housing Price Model v3 **Trained:** 2026-01-13 **Algorithm:** Random Forest **Metrics:** MAPE=7.8%, R²=0.865 **Sweep:** [W\u0026B Link](https://wandb.ai/project/sweeps/abc123) \"\"\" ) Resultado: 6 meses después, cuando un stakeholder pregunta “¿qué modelo está en Production?”, abres MLflow UI y toda la info está ahí—no en un Slack thread perdido.\nW\u0026B no tiene:\nModel Registry (solo artifact tracking básico) Stages (Staging/Production) API de carga (models:/name/stage) Transition history (quién cambió v2 a Production, cuándo, por qué) 2. Pipeline Orchestration (main.py) # main.py @hydra.main(config_path=\".\", config_name=\"config\") def go(config: DictConfig) -\u003e None: # MLflow orquesta steps como sub-runs # Step 01: Download mlflow.run( uri=\"src/data/01_download_data\", entry_point=\"main\", parameters={ \"file_url\": config.download_data.file_url, \"gcs_output_path\": config.download_data.gcs_output_path, # ... } ) # Step 02: Preprocessing mlflow.run( uri=\"src/data/02_preprocessing_and_imputation\", entry_point=\"main\", parameters={ \"gcs_input_path\": config.preprocessing.gcs_input_path, # ... } ) # ... Steps 03-07 MLflow crea un run jerárquico:\nParent Run: end_to_end_pipeline ├── Child Run: 01_download_data │ ├── params: file_url, gcs_output_path │ └── artifacts: housing.parquet ├── Child Run: 02_preprocessing_and_imputation │ ├── params: imputation_strategy │ └── artifacts: imputer.pkl, housing_processed.parquet ├── Child Run: 03_feature_engineering │ └── ... └── Child Run: 07_registration └── artifacts: model.pkl, model_config.yaml Valor: En MLflow UI, ves toda la ejecución del pipeline como un árbol. Cada step es auditable—qué params usó, cuánto tardó, qué artifacts produjo.\nW\u0026B no tiene orquestación de pipelines—solo tracking de runs individuales.\nLa División del Trabajo en Este Proyecto Responsabilidad W\u0026B MLflow Razón Bayesian hyperparameter optimization ✓ ✗ W\u0026B tiene sweep inteligente, MLflow solo Grid/Random Real-time dashboards ✓ ✗ W\u0026B UI es interactivo, MLflow UI es estático Parallel coordinates plots ✓ ✗ W\u0026B tiene visualizaciones avanzadas Early termination (Hyperband) ✓ ✗ W\u0026B implementa Hyperband/ASHA/Median stopping Model Registry con stages ✗ ✓ MLflow tiene Staging/Production, W\u0026B no Model-as-code API ✗ ✓ mlflow.pyfunc.load_model() es el estándar Rollback de modelos ✗ ✓ MLflow transition, W\u0026B no tiene concepto de stages Pipeline orchestration ✗ ✓ mlflow.run() ejecuta steps anidados Artifact storage (físico) ✗ ✗ Ambos apuntan a GCS, no duplican storage Logging asíncrono ✓ ✗ W\u0026B no bloquea training, MLflow sí Metadata searchable ✓ ✓ Ambos permiten tags/búsqueda, implementaciones diferentes El Flujo Completo: W\u0026B → MLflow Día 1-3: Experimentación (W\u0026B)\n# Ejecutar sweep de 50 runs make run-sweep # W\u0026B dashboard muestra: # - 50 runs en tabla # - Parallel coordinates plot # - Best run: n_estimators=200, max_depth=20, wMAPE=7.8% # - Sweep ID: abc123xyz Output: src/model/06_sweep/best_params.yaml\nhyperparameters: n_estimators: 200 max_depth: 20 min_samples_split: 2 min_samples_leaf: 1 metrics: mape: 7.82 wmape: 7.76 r2: 0.8654 sweep_id: abc123xyz # Link a W\u0026B best_run_id: def456ghi Día 4: Registration (MLflow)\n# Step 07 lee best_params.yaml python main.py main.execute_steps=[07_registration] # MLflow: # 1. Entrena modelo con best_params # 2. Registra como housing_price_model/v3 # 3. Transiciona a Staging # 4. Guarda metadata (incluyendo sweep_id) Día 5-7: Validación en Staging\n# API corre con modelo en Staging docker run -p 8080:8080 \\ -e MLFLOW_MODEL_NAME=housing_price_model \\ -e MLFLOW_MODEL_STAGE=Staging \\ housing-api:latest # Correr tests, validar métricas, revisar predicciones Día 8: Promoción a Production\nmlflow models transition \\ --name housing_price_model \\ --version 3 \\ --stage Production # API en producción auto-recarga v3 # v2 queda como fallback (stage: Archived) Si algo falla:\n# Rollback en 10 segundos mlflow models transition \\ --name housing_price_model \\ --version 2 \\ --stage Production Por Qué Ambos, Definitivamente Pregunta: “¿Puedo usar solo W\u0026B?”\nRespuesta: Puedes, pero pierdes:\nModel Registry (versionamiento, stages, rollback) API estándar para cargar modelos en producción Pipeline orchestration con runs jerárquicos Resultado: Terminas construyendo tu propio sistema de versionamiento de modelos con scripts custom—reinventando la rueda mal.\nPregunta: “¿Puedo usar solo MLflow?”\nRespuesta: Puedes, pero pierdes:\nBayesian optimization (tendrás que hacer Grid Search lento) Visualizaciones interactivas (parallel coordinates, real-time dashboards) Early termination inteligente (desperdicias compute) Resultado: Tus sweeps toman 3x más tiempo, y no tienes feedback visual de qué funciona.\nEl Costo Real W\u0026B:\nFree tier: 100GB storage, colaboradores ilimitados Team tier: $50/usuario/mes (para equipos \u003e5 personas) MLflow:\nOpen source, gratis Costo: Hosting del tracking server (Cloud Run: ~$20/mes para uso moderado) Storage: GCS (ya lo pagas para datos) Total para equipo de 5: ~$20-50/mes (si usas W\u0026B free tier) o ~$270/mes (si usas W\u0026B Team).\nROI: Si un sweep más eficiente ahorra 30 minutos de compute/día:\nCompute ahorrado: ~15 horas/mes En GCP: 15 horas × $2/hora (GPU) = $30/mes ahorrado solo en compute Más el tiempo de ingeniero (más valioso) Breakeven en \u003c1 mes.\nLa Lección Para MLOps Engineers No elijas herramientas por hype o popularidad. Elige por responsabilidades claras:\nExperimentación rápida e interactiva: W\u0026B, Neptune, Comet Governance y deployment: MLflow, Seldon, BentoML Artifact storage: GCS, S3, Azure Blob (no herramientas de tracking) Este proyecto usa:\nW\u0026B: Porque necesita sweep Bayesiano eficiente MLflow: Porque necesita Model Registry production-ready GCS: Porque necesita storage de alta disponibilidad No hay redundancia—hay especialización.\nCuando entiendes esto, dejas de preguntar “¿W\u0026B o MLflow?” y empiezas a preguntar “¿qué problema estoy resolviendo?”\nEsa es la diferencia entre usar herramientas y construir sistemas.\n10. Docker y MLflow: Containerización del Ecosistema Completo La Arquitectura de Tres Containers Este proyecto utiliza tres Dockerfiles distintos, cada uno optimizado para su propósito específico:\nPipeline Container (Dockerfile): Ejecuta el pipeline completo de entrenamiento con MLflow tracking API Container (api/Dockerfile): Sirve predicciones con FastAPI en producción Streamlit Container (streamlit_app/Dockerfile): Proporciona interfaz web interactiva Esta separación no es accidental—es una decisión arquitectónica que refleja los diferentes requisitos de cada componente. 1. Pipeline Container: Entrenamiento con MLflow Tracking Dockerfile del Pipeline # ================================================================= # Dockerfile for MLOps Pipeline Execution # Purpose: Run the complete training pipeline in containerized environment # ================================================================= FROM python:3.12-slim LABEL maintainer=\"danieljimenez88m@gmail.com\" LABEL description=\"Housing Price Prediction - MLOps Pipeline\" # Set working directory WORKDIR /app # Install system dependencies RUN apt-get update \u0026\u0026 apt-get install -y \\ gcc \\ g++ \\ git \\ curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* # Copy requirements first for better caching COPY pyproject.toml ./ COPY requirements.txt* ./ # Install Python dependencies RUN pip install --no-cache-dir --upgrade pip \u0026\u0026 \\ pip install --no-cache-dir uv \u0026\u0026 \\ uv pip install --system -e . # Copy application code COPY . . # Set environment variables ENV PYTHONUNBUFFERED=1 ENV PYTHONDONTWRITEBYTECODE=1 # Create necessary directories RUN mkdir -p mlruns outputs models # Default command runs the pipeline CMD [\"python\", \"main.py\"] Decisiones Técnicas Críticas 1. Por Qué gcc y g++\nRUN apt-get install -y gcc g++ git curl Muchos paquetes de ML (numpy, scipy, scikit-learn) compilan extensiones C/C++ durante la instalación. Sin estos compiladores, pip install falla con errores crípticos como “error: command ‘gcc’ failed”.\nTrade-off: Imagen más grande (~500MB vs ~150MB de Python slim puro), pero garantiza que todas las dependencias se instalan correctamente.\n2. Layer Caching Strategy\n# Copy requirements first for better caching COPY pyproject.toml ./ COPY requirements.txt* ./ RUN pip install ... # Copy application code AFTER COPY . . Docker cachea layers. Si cambias código Python pero no dependencias, Docker reutiliza la layer de pip install (que toma 5 minutos) y solo recopia el código (10 segundos).\nSin esta optimización: Cada cambio de código requiere reinstalar todas las dependencias.\n3. Directory Creation for MLflow\nRUN mkdir -p mlruns outputs models MLflow escribe artifacts a mlruns/ por defecto si no se configura un tracking server remoto. Si este directorio no existe con permisos correctos, MLflow falla silenciosamente.\noutputs/: Para plots y análisis intermedios models/: Para checkpoints de modelos antes de subir a GCS\nCómo Habilitar MLflow Tracking Opción 1: MLflow Local (Default)\nCuando ejecutas el pipeline en este container, MLflow escribe a mlruns/ dentro del container:\ndocker run --env-file .env housing-pipeline:latest # MLflow escribe a /app/mlruns/ # Para ver el UI: docker exec -it mlflow ui --host 0.0.0.0 --port 5000 Limitación: Los runs se pierden cuando el container se detiene.\nOpción 2: MLflow Remote Tracking Server\nPara persistir runs, configura un servidor MLflow separado:\n# docker-compose.yaml services: mlflow: image: ghcr.io/mlflow/mlflow:v2.9.2 container_name: mlflow-server ports: - \"5000:5000\" environment: - BACKEND_STORE_URI=sqlite:///mlflow.db - DEFAULT_ARTIFACT_ROOT=gs://your-bucket/mlflow-artifacts volumes: - mlflow-data:/mlflow command: \u003e mlflow server --backend-store-uri sqlite:///mlflow/mlflow.db --default-artifact-root gs://your-bucket/mlflow-artifacts --host 0.0.0.0 --port 5000 pipeline: build: . environment: - MLFLOW_TRACKING_URI=http://mlflow:5000 - GCP_PROJECT_ID=${GCP_PROJECT_ID} - GCS_BUCKET_NAME=${GCS_BUCKET_NAME} - WANDB_API_KEY=${WANDB_API_KEY} depends_on: - mlflow volumes: mlflow-data: Configuración en el código:\n# main.py import os import mlflow # Si MLFLOW_TRACKING_URI está configurado, usar ese server mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\") mlflow.set_tracking_uri(mlflow_uri) mlflow.set_experiment(\"housing_price_prediction\") with mlflow.start_run(): # Log params, metrics, artifacts mlflow.log_param(\"n_estimators\", 200) mlflow.log_metric(\"mape\", 7.82) mlflow.sklearn.log_model(model, \"model\") Opción 3: MLflow en Cloud (Production)\nPara producción, usa un servidor MLflow gestionado:\n# Deploy MLflow a Cloud Run (serverless) gcloud run deploy mlflow-server \\ --image ghcr.io/mlflow/mlflow:v2.9.2 \\ --platform managed \\ --region us-central1 \\ --set-env-vars=\"BACKEND_STORE_URI=postgresql://user:pass@host/mlflow_db,DEFAULT_ARTIFACT_ROOT=gs://bucket/mlflow\" \\ --allow-unauthenticated # Obtener URL MLFLOW_URL=$(gcloud run services describe mlflow-server --format 'value(status.url)') # Configurar en pipeline export MLFLOW_TRACKING_URI=$MLFLOW_URL Ejecución del Pipeline Container # Build docker build -t housing-pipeline:latest . # Run con env vars docker run \\ --env-file .env \\ -v $(pwd)/mlruns:/app/mlruns \\ housing-pipeline:latest # Run con steps específicos docker run \\ --env-file .env \\ housing-pipeline:latest \\ python main.py main.execute_steps=[03_feature_engineering,05_model_selection] # Ver logs en tiempo real docker logs -f Volume Mount (-v): Monta mlruns/ desde el host al container para persistir runs MLflow incluso después de que el container se detenga.\n2. API Container: Inference en Producción Dockerfile del API # ================================================================= # Dockerfile for Housing Price Prediction API # Purpose: Production-ready FastAPI service for Cloud Run deployment # ================================================================= FROM python:3.12-slim LABEL maintainer=\"danieljimenez88m@gmail.com\" LABEL description=\"Housing Price Prediction API - FastAPI Service\" WORKDIR /app # Install system dependencies (solo curl para healthcheck) RUN apt-get update \u0026\u0026 apt-get install -y \\ curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* # Copy requirements first for better caching COPY requirements.txt . # Install Python dependencies RUN pip install --no-cache-dir --upgrade pip \u0026\u0026 \\ pip install --no-cache-dir -r requirements.txt # Copy application code COPY app/ ./app/ # Create models directory RUN mkdir -p models # Set environment variables ENV PYTHONUNBUFFERED=1 ENV PYTHONDONTWRITEBYTECODE=1 ENV PORT=8080 # Expose port EXPOSE 8080 # Health check HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 # Run the application CMD exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT} Decisiones Técnicas Críticas 1. Imagen Más Ligera\nComparado con el pipeline container:\nNo necesita gcc/g++: Las dependencias ya están compiladas en wheels No necesita git: No clona repos Solo curl: Para el healthcheck Resultado: Imagen de ~200MB vs ~500MB del pipeline.\nPor qué importa: Cloud Run cobra por uso de memoria. Una imagen más pequeña = menos memoria = menos costo.\n2. Health Check Nativo\nHEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 Docker marca el container como “unhealthy” si el endpoint /health falla 3 veces consecutivas.\nCloud Run y Kubernetes usan esto para:\nNo enviar tráfico a containers unhealthy Reiniciar containers que fallan Reporting de uptime start-period=40s: Da 40 segundos al API para cargar el modelo antes de empezar health checks.\n3. Port Configuration Flexible\nENV PORT=8080 CMD exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT} Cloud Run inyecta PORT como env var (puede ser 8080, 8081, etc.). El API debe leer este valor, no hardcodearlo.\nexec: Reemplaza el shell process con uvicorn, permitiendo que Docker envíe signals (SIGTERM) directamente a uvicorn para graceful shutdown.\nCómo el API Carga el Modelo El API tiene tres estrategias de carga de modelo con fallback automático:\n# api/app/core/model_loader.py class ModelLoader: \"\"\"Carga modelo desde MLflow → GCS → Local con fallback.\"\"\" def load_model(self) -\u003e Any: \"\"\"Priority: MLflow \u003e GCS \u003e Local\"\"\" # Estrategia 1: Desde MLflow Registry if self.mlflow_model_name: try: model_uri = f\"models:/{self.mlflow_model_name}/{self.mlflow_stage}\" self._model = mlflow.pyfunc.load_model(model_uri) logger.info(f\"Loaded from MLflow: {model_uri}\") return self._model except Exception as e: logger.warning(f\"MLflow load failed: {e}, trying GCS...\") # Estrategia 2: Desde GCS if self.gcs_model_path: try: storage_client = storage.Client() bucket = storage_client.bucket(self.gcs_bucket) blob = bucket.blob(self.gcs_model_path) model_bytes = blob.download_as_bytes() self._model = pickle.loads(model_bytes) logger.info(f\"Loaded from GCS: gs://{self.gcs_bucket}/{self.gcs_model_path}\") return self._model except Exception as e: logger.warning(f\"GCS load failed: {e}, trying local...\") # Estrategia 3: Desde archivo local (fallback) if self.local_model_path and Path(self.local_model_path).exists(): with open(self.local_model_path, 'rb') as f: self._model = pickle.load(f) logger.info(f\"Loaded from local: {self.local_model_path}\") return self._model raise RuntimeError(\"No model could be loaded from any source\") Configuración con env vars:\n# Producción: Cargar desde MLflow docker run -p 8080:8080 \\ -e MLFLOW_TRACKING_URI=https://mlflow.example.com \\ -e MLFLOW_MODEL_NAME=housing_price_model \\ -e MLFLOW_MODEL_STAGE=Production \\ housing-api:latest # Staging: Cargar desde GCS docker run -p 8080:8080 \\ -e GCS_BUCKET=my-bucket \\ -e GCS_MODEL_PATH=models/trained/housing_price_model.pkl \\ housing-api:latest # Desarrollo: Cargar desde local docker run -p 8080:8080 \\ -v $(pwd)/models:/app/models \\ -e LOCAL_MODEL_PATH=/app/models/trained/housing_price_model.pkl \\ housing-api:latest 3. Streamlit Container: Frontend Interactivo Dockerfile de Streamlit # ================================================================= # Dockerfile for Streamlit Frontend # Purpose: Interactive web interface for housing price predictions # ================================================================= FROM python:3.12-slim LABEL maintainer=\"danieljimenez88m@gmail.com\" LABEL description=\"Housing Price Prediction - Streamlit Frontend\" WORKDIR /app RUN apt-get update \u0026\u0026 apt-get install -y curl \u0026\u0026 rm -rf /var/lib/apt/lists/* COPY requirements.txt . RUN pip install --no-cache-dir --upgrade pip \u0026\u0026 \\ pip install --no-cache-dir -r requirements.txt COPY app.py . # Create .streamlit directory for config RUN mkdir -p .streamlit # Streamlit configuration RUN echo '\\ [server]\\n\\ port = 8501\\n\\ address = \"0.0.0.0\"\\n\\ headless = true\\n\\ enableCORS = false\\n\\ enableXsrfProtection = true\\n\\ \\n\\ [browser]\\n\\ gatherUsageStats = false\\n\\ \\n\\ [theme]\\n\\ primaryColor = \"#FF4B4B\"\\n\\ backgroundColor = \"#FFFFFF\"\\n\\ secondaryBackgroundColor = \"#F0F2F6\"\\n\\ textColor = \"#262730\"\\n\\ font = \"sans serif\"\\n\\ ' \u003e .streamlit/config.toml ENV PYTHONUNBUFFERED=1 ENV PYTHONDONTWRITEBYTECODE=1 EXPOSE 8501 HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\ CMD curl -f http://localhost:8501/_stcore/health || exit 1 CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"] Decisiones Técnicas Críticas 1. Configuración Embedded\nRUN echo '...' \u003e .streamlit/config.toml Streamlit requiere configuración para correr en containers (headless mode, CORS, etc.). En lugar de commitear un archivo config.toml al repo, lo generamos en build time.\nVentajas:\nUn archivo menos en el repo Configuración versionada con el Dockerfile No hay riesgo de olvidar commitear el config 2. Health Check de Streamlit\nHEALTHCHECK CMD curl -f http://localhost:8501/_stcore/health || exit 1 Streamlit expone /_stcore/health automáticamente. Este endpoint retorna 200 si la app está running.\n3. Tema Personalizado\n[theme] primaryColor = \"#FF4B4B\" backgroundColor = \"#FFFFFF\" secondaryBackgroundColor = \"#F0F2F6\" textColor = \"#262730\" El tema define los colores de botones, backgrounds, etc. Esto da consistencia visual sin necesidad de CSS custom en cada componente.\nCómo Streamlit Se Conecta al API # streamlit_app/app.py import os import requests import streamlit as st # Read API URL from environment variable API_URL = os.getenv(\"API_URL\", \"http://localhost:8080\") API_PREDICT_ENDPOINT = f\"{API_URL}/api/v1/predict\" def make_prediction(features: Dict[str, Any]) -\u003e Dict[str, Any]: \"\"\"Call API to get prediction.\"\"\" payload = {\"instances\": [features]} try: response = requests.post( API_PREDICT_ENDPOINT, json=payload, timeout=10 ) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: st.error(f\"API Error: {e}\") return None # Streamlit UI st.title(\"Housing Price Prediction\") with st.form(\"prediction_form\"): longitude = st.number_input(\"Longitude\", value=-122.23) latitude = st.number_input(\"Latitude\", value=37.88) # ... más inputs submitted = st.form_submit_button(\"Predict\") if submitted: features = { \"longitude\": longitude, \"latitude\": latitude, # ... } result = make_prediction(features) if result: prediction = result[\"predictions\"][0][\"median_house_value\"] st.success(f\"Predicted Price: ${prediction:,.2f}\") Configuración de la URL del API:\n# Docker Compose: Usa service name docker-compose up # Streamlit automáticamente recibe API_URL=http://api:8080 # Local development: Usa localhost API_URL=http://localhost:8080 streamlit run app.py # Production: Usa Cloud Run URL API_URL=https://housing-api-xyz.run.app streamlit run app.py Docker Compose: Orquestación de los Tres Containers # docker-compose.yaml services: api: build: context: ./api dockerfile: Dockerfile container_name: housing-price-api ports: - \"8080:8080\" environment: - PORT=8080 - LOCAL_MODEL_PATH=/app/models/trained/housing_price_model.pkl - WANDB_API_KEY=${WANDB_API_KEY} volumes: - ./models:/app/models:ro restart: unless-stopped healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"] interval: 30s timeout: 10s retries: 3 networks: - mlops-network streamlit: build: context: ./streamlit_app dockerfile: Dockerfile container_name: housing-streamlit ports: - \"8501:8501\" environment: - API_URL=http://api:8080 depends_on: - api restart: unless-stopped healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8501/_stcore/health\"] interval: 30s timeout: 10s retries: 3 networks: - mlops-network networks: mlops-network: driver: bridge name: housing-mlops-network Decisiones Críticas:\n1. Network Isolation\nnetworks: - mlops-network Ambos containers están en la misma red Docker, permitiendo que Streamlit llame al API usando http://api:8080 (service name como hostname).\nSin esto: Tendrías que usar http://host.docker.internal:8080 (solo funciona en Docker Desktop) o la IP del host.\n2. Volume Mount Read-Only\nvolumes: - ./models:/app/models:ro El API monta models/ en read-only mode (:ro). El container puede leer el modelo pero no modificarlo.\nPor qué: Seguridad. Si el container es comprometido, un atacante no puede sobrescribir el modelo con uno malicioso.\n3. Dependency Order\ndepends_on: - api Docker Compose inicia el API antes que Streamlit. Esto evita que Streamlit falle al intentar conectarse a un API que aún no está corriendo.\nLimitación: depends_on solo espera a que el container inicie, no a que el API esté listo (healthcheck pass). Para eso, necesitas un init container o retry logic en Streamlit.\nComando Completo de Ejecución # 1. Build todas las imágenes docker-compose build # 2. Entrenar el modelo (pipeline container) docker run --env-file .env -v $(pwd)/models:/app/models housing-pipeline:latest # 3. Iniciar API + Streamlit docker-compose up -d # 4. Verificar health curl http://localhost:8080/health curl http://localhost:8501/_stcore/health # 5. Ver logs docker-compose logs -f # 6. Detener todo docker-compose down Lo Que Esta Arquitectura Resuelve Sin containers:\n“Funciona en mi máquina” syndrome Dependencias conflictivas (Python 3.9 vs 3.12) Setup manual en cada ambiente (dev, staging, prod) Con esta arquitectura:\nReproducibilidad: Mismo container corre en laptop, CI/CD, y producción Isolation: API no interfiere con Streamlit, pipeline no interfiere con API Deployment: docker push → gcloud run deploy en \u003c5 minutos Rollback: docker pull previous-image → restart Observability: Health checks automáticos, logs centralizados El valor real: Un data scientist sin experiencia en DevOps puede deployar a producción sin saber cómo configurar nginx, systemd, o virtual environments. Docker abstrae toda esa complejidad.\n10.5. Arquitectura del API: FastAPI en Producción Por Qué Esta Sección Importa Has visto pipelines de entrenamiento, sweep de hiperparámetros, y model registry. Pero el 90% del tiempo, tu modelo no está entrenando—está sirviendo predicciones en producción.\nUn API mal diseñado es el cuello de botella entre un modelo excelente y un producto útil. Esta sección desmenuza cómo este proyecto construye un API production-ready, no un prototipo de tutorial.\nLa Arquitectura General api/ ├── app/ │ ├── main.py # FastAPI app + lifespan management │ ├── core/ │ │ ├── config.py # Pydantic Settings (env vars) │ │ ├── model_loader.py # Multi-source model loading │ │ ├── wandb_logger.py # Prediction logging │ │ └── preprocessor.py # Feature engineering │ ├── routers/ │ │ └── predict.py # Prediction endpoints │ └── models/ │ └── schemas.py # Pydantic request/response models ├── requirements.txt ├── Dockerfile └── tests/ Decisión arquitectónica: Separation of concerns por capas:\nCore: Lógica de negocio (cargar modelo, logging, config) Routers: Endpoints HTTP (rutas, validación de requests) Models: Schemas de datos (Pydantic) Por qué no todo en main.py? Porque cuando el API crece (agregar autenticación, rate limiting, múltiples modelos), cada capa se extiende independientemente sin tocar el resto.\n1. Lifespan Management: El Patrón Que Evita Latencia en Primera Request El Problema Que Resuelve Anti-pattern común:\n# BAD: Cargar modelo en cada request @app.post(\"/predict\") def predict(features): model = pickle.load(open(\"model.pkl\", \"rb\")) # 5 segundos cada request return model.predict(features) Problemas:\nPrimera request toma 5 segundos (cargar modelo) Cada request subsecuente también (no hay caching) Si 10 requests concurrentes → 10 cargas del modelo (50 segundos total) La Solución: asynccontextmanager # api/app/main.py @asynccontextmanager async def lifespan(app: FastAPI): \"\"\" Lifecycle manager for the FastAPI application. Loads the model on startup and cleans up on shutdown. \"\"\" logger.info(\"Starting up API...\") # STARTUP: Cargar modelo UNA VEZ wandb_logger = WandBLogger( project=settings.WANDB_PROJECT, enabled=True ) model_loader = ModelLoader( local_model_path=settings.LOCAL_MODEL_PATH, gcs_bucket=settings.GCS_BUCKET, gcs_model_path=settings.GCS_MODEL_PATH, mlflow_model_name=settings.MLFLOW_MODEL_NAME, mlflow_model_stage=settings.MLFLOW_MODEL_STAGE, mlflow_tracking_uri=settings.MLFLOW_TRACKING_URI ) try: logger.info(\"Loading model...\") model_loader.load_model() # Toma 5 segundos, pero SOLO una vez logger.info(f\"Model loaded: {model_loader.model_version}\") # Guardar en app state (disponible para todos los endpoints) app.state.model_loader = model_loader app.state.wandb_logger = wandb_logger except Exception as e: logger.error(f\"Failed to load model: {str(e)}\") logger.warning(\"API will start but predictions will fail\") yield # API corre aquí # SHUTDOWN: Cleanup logger.info(\"Shutting down API...\") wandb_logger.close() # Usar lifespan en FastAPI app = FastAPI( title=\"Housing Price Prediction API\", version=\"1.0.0\", lifespan=lifespan # CRÍTICO ) Lo que hace:\nStartup (antes de yield):\nCarga modelo en memoria (5 segundos, una sola vez) Inicializa W\u0026B logger Guarda ambos en app.state (singleton pattern) Running (después de yield):\nTodas las requests usan el modelo cacheado en app.state.model_loader Latencia por request: \u003c50ms (solo inference, no I/O) Shutdown (después del context manager):\nCierra W\u0026B run (flush pending logs) Libera recursos Resultado:\nPrimera request: \u003c50ms (modelo ya cargado) Requests subsecuentes: \u003c50ms 10 requests concurrentes: \u003c100ms promedio (paralelizable) Trade-off: Startup time de 5-10 segundos. Aceptable para producción—mejor que 5 segundos por request.\n2. Configuration Management: Pydantic Settings con Prioridades El Pattern: Settings-as-Code # api/app/core/config.py from pydantic_settings import BaseSettings class Settings(BaseSettings): PROJECT_NAME: str = \"Housing Price Prediction API\" VERSION: str = \"1.0.0\" API_V1_STR: str = \"/api/v1\" # Model - MLflow (priority 1) MLFLOW_MODEL_NAME: str = \"\" MLFLOW_MODEL_STAGE: str = \"Production\" MLFLOW_TRACKING_URI: str = \"\" # Model - GCS (priority 2) GCS_BUCKET: str = \"\" GCS_MODEL_PATH: str = \"models/trained/housing_price_model.pkl\" # Model - Local (priority 3, fallback) LOCAL_MODEL_PATH: str = \"models/trained/housing_price_model.pkl\" # Weights \u0026 Biases WANDB_API_KEY: str = \"\" WANDB_PROJECT: str = \"housing-mlops-api\" class Config: env_file = \".env\" # Lee de .env automáticamente case_sensitive = True # MLFLOW_MODEL_NAME != mlflow_model_name Por qué Pydantic Settings:\nType Safety: settings.VERSION es str, no Optional[Any] Validation: Si MLFLOW_MODEL_STAGE no es string, falla en startup (no en la primera request) Auto .env loading: No necesitas python-dotenv manualmente Default values: LOCAL_MODEL_PATH tiene default, MLFLOW_MODEL_NAME no Uso en código:\nfrom app.core.config import Settings settings = Settings() # Lee env vars + .env if settings.MLFLOW_MODEL_NAME: # Type-safe check model = load_from_mlflow(settings.MLFLOW_MODEL_NAME) La Estrategia de Prioridades (Cascade Fallback) Intenta cargar de: 1. MLflow Registry (si MLFLOW_MODEL_NAME está configurado) ↓ Si falla 2. GCS (si GCS_BUCKET está configurado) ↓ Si falla 3. Local filesystem (siempre disponible como último recurso) ↓ Si falla 4. API inicia pero `/predict` retorna 500 Configuración por ambiente:\n# Producción (.env.production) MLFLOW_MODEL_NAME=housing_price_model MLFLOW_MODEL_STAGE=Production MLFLOW_TRACKING_URI=https://mlflow.company.com # GCS y Local quedan vacíos → no se usan # Staging (.env.staging) MLFLOW_MODEL_NAME=housing_price_model MLFLOW_MODEL_STAGE=Staging # Mismo setup, diferente stage # Desarrollo local (.env.local) LOCAL_MODEL_PATH=models/trained/housing_price_model.pkl # Sin MLflow ni GCS → carga de local directo Valor: Un solo codebase, múltiples ambientes. No hay if ENVIRONMENT == \"production\" en el código.\n3. Model Loader: Multi-Source con Fallback Inteligente La Arquitectura del Loader # api/app/core/model_loader.py class ModelLoader: \"\"\"Handles loading ML models from various sources.\"\"\" def __init__( self, local_model_path: Optional[str] = None, gcs_bucket: Optional[str] = None, gcs_model_path: Optional[str] = None, mlflow_model_name: Optional[str] = None, mlflow_model_stage: Optional[str] = None, mlflow_tracking_uri: Optional[str] = None ): self.local_model_path = local_model_path self.gcs_bucket = gcs_bucket self.gcs_model_path = gcs_model_path self.mlflow_model_name = mlflow_model_name self.mlflow_model_stage = mlflow_model_stage self.mlflow_tracking_uri = mlflow_tracking_uri self._model: Optional[Any] = None # Cacheado en memoria self._model_version: str = \"unknown\" self._preprocessor = HousingPreprocessor() def load_model(self) -\u003e Any: \"\"\"Load model with cascade fallback strategy.\"\"\" # Priority 1: MLflow Registry if self.mlflow_model_name: try: logger.info(f\"Attempting MLflow load: {self.mlflow_model_name}/{self.mlflow_model_stage}\") self._model = self.load_from_mlflow( self.mlflow_model_name, self.mlflow_model_stage, self.mlflow_tracking_uri ) return self._model except Exception as e: logger.warning(f\"MLflow load failed: {str(e)}, trying GCS...\") # Priority 2: GCS if self.gcs_bucket and self.gcs_model_path: try: logger.info(f\"Attempting GCS load: gs://{self.gcs_bucket}/{self.gcs_model_path}\") self._model = self.load_from_gcs(self.gcs_bucket, self.gcs_model_path) return self._model except Exception as e: logger.warning(f\"GCS load failed: {str(e)}, trying local...\") # Priority 3: Local filesystem if self.local_model_path and Path(self.local_model_path).exists(): logger.info(f\"Attempting local load: {self.local_model_path}\") self._model = self.load_from_local(self.local_model_path) return self._model # All strategies failed raise RuntimeError( \"Could not load model from any source. \" \"Check MLflow/GCS/local configuration.\" ) def predict(self, features: pd.DataFrame) -\u003e np.ndarray: \"\"\"Make predictions with preprocessing.\"\"\" if not self.is_loaded: raise RuntimeError(\"Model not loaded\") # Apply same preprocessing que el training pipeline processed_features = self._preprocessor.transform(features) # Predict predictions = self._model.predict(processed_features) return predictions @property def is_loaded(self) -\u003e bool: \"\"\"Check if model is loaded.\"\"\" return self._model is not None Decisiones Técnicas Críticas 1. Por Qué MLflow Es Priority 1\n# MLflow load model = mlflow.sklearn.load_model(\"models:/housing_price_model/Production\") Ventajas sobre GCS/Local:\nModel URI abstrae storage: El modelo puede estar en S3, GCS, HDFS—MLflow lo resuelve Stage resolution: Production automáticamente resuelve a la versión correcta (v1, v2, etc.) Metadata incluida: MLflow también carga conda.yaml, requirements.txt, metadata de features Rollback trivial: Cambias stage en MLflow UI, API recarga automáticamente en próximo restart 2. GCS Como Fallback (No Primary)\n# GCS load from google.cloud import storage client = storage.Client() bucket = client.bucket(\"my-bucket\") blob = bucket.blob(\"models/trained/housing_price_model.pkl\") model_bytes = blob.download_as_bytes() model = pickle.loads(model_bytes) Por qué no primary:\nNo hay versionamiento: models/trained/housing_price_model.pkl es siempre el “latest”—no puedes cargar v1 vs v2 sin cambiar el path No metadata: Solo obtienes el pickle, no sabes qué hiperparámetros/features espera No stages: No existe concepto de Staging vs Production Cuándo usar GCS como primary:\nMLflow no está disponible (outage) Setup simple (solo un modelo, no necesitas registry) Budget constraint (evitar hosting de MLflow) 3. Local Como Last Resort\n# Local load with open(\"models/trained/housing_price_model.pkl\", \"rb\") as f: model = pickle.load(f) Solo para:\nDesarrollo local (no quieres depender de GCS/MLflow) Debugging (modelo roto en GCS, testeas con una copia local) CI/CD tests (GitHub Actions no tiene acceso a GCS) Nunca para producción real—si GCS y MLflow están down, tienes problemas más grandes que el modelo.\n4. Preprocessing Pipeline Embebido\nself._preprocessor = HousingPreprocessor() def predict(self, features: pd.DataFrame) -\u003e np.ndarray: processed_features = self._preprocessor.transform(features) predictions = self._model.predict(processed_features) return predictions Por qué crítico: El modelo espera features procesadas (one-hot encoding de ocean_proximity, feature engineering de clusters). Si el cliente envía raw features, el modelo falla.\nOpciones de implementación:\nA) Preprocessing en el API (este proyecto):\n# Cliente envía raw features {\"ocean_proximity\": \"NEAR BAY\", \"longitude\": -122.23, ...} # API aplica preprocessing processed = preprocessor.transform(raw_features) # Modelo recibe features procesadas predictions = model.predict(processed) B) Preprocessing en el cliente (mal para APIs públicos):\n# Cliente debe saber exact preprocessing processed = client_side_preprocessing(raw_features) # ¿Qué hace esto? # API solo hace inference predictions = model.predict(processed) Trade-offs:\nApproach Ventaja Desventaja Preprocessing en API Cliente no necesita saber preprocessing API más complejo, latencia +5ms Preprocessing en cliente API simple, latencia baja Cliente debe replicar preprocessing exacto Para APIs públicos: Siempre preprocessing en el API. Los clientes no deben conocer detalles internos del modelo.\nPara APIs internos: Depende. Si el cliente es otro servicio que controlas, puedes hacer preprocessing ahí para reducir latencia.\n4. Request/Response Validation: Pydantic Schemas El Anti-Pattern: Validación Manual # BAD: Validación manual propensa a errores @app.post(\"/predict\") def predict(request: dict): if \"longitude\" not in request: return {\"error\": \"missing longitude\"} if not isinstance(request[\"longitude\"], (int, float)): return {\"error\": \"longitude must be number\"} if request[\"longitude\"] \u003c -180 or request[\"longitude\"] \u003e 180: return {\"error\": \"longitude out of range\"} # ... 50 líneas más de validación manual Problemas:\nCódigo repetitivo y frágil Errores inconsistentes (\"missing longitude\" vs \"longitude is required\") No hay documentación automática (OpenAPI) Difícil de testear La Solución: Pydantic Schemas # api/app/models/schemas.py from pydantic import BaseModel, Field, field_validator class HousingFeatures(BaseModel): \"\"\"Input features for housing price prediction.\"\"\" longitude: float = Field( ..., # Required description=\"Longitude coordinate\", ge=-180, # greater or equal le=180 # less or equal ) latitude: float = Field(..., description=\"Latitude coordinate\", ge=-90, le=90) housing_median_age: float = Field(..., description=\"Median age of houses\", ge=0) total_rooms: float = Field(..., description=\"Total number of rooms\", ge=0) total_bedrooms: float = Field(..., description=\"Total number of bedrooms\", ge=0) population: float = Field(..., description=\"Block population\", ge=0) households: float = Field(..., description=\"Number of households\", ge=0) median_income: float = Field(..., description=\"Median income\", ge=0) ocean_proximity: str = Field(..., description=\"Proximity to ocean\") @field_validator('ocean_proximity') @classmethod def validate_ocean_proximity(cls, v: str) -\u003e str: \"\"\"Validate ocean proximity values.\"\"\" valid_values = ['\u003c1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'] if v.upper() not in valid_values: raise ValueError( f\"ocean_proximity must be one of: {', '.join(valid_values)}\" ) return v.upper() # Normaliza a uppercase model_config = { \"json_schema_extra\": { \"examples\": [{ \"longitude\": -122.23, \"latitude\": 37.88, \"housing_median_age\": 41.0, \"total_rooms\": 880.0, \"total_bedrooms\": 129.0, \"population\": 322.0, \"households\": 126.0, \"median_income\": 8.3252, \"ocean_proximity\": \"NEAR BAY\" }] } } Lo que esto da automáticamente:\nValidación de tipos:\n{\"longitude\": \"not a number\"} // Rechazado: ValidationError Validación de rangos:\n{\"longitude\": -200} // Rechazado: must be \u003e= -180 Validación custom:\n{\"ocean_proximity\": \"INVALID\"} // Rechazado: must be one of [...] Documentación automática en /docs:\nSwagger UI muestra todos los fields Descriptions, constraints, ejemplos Try-it-out funciona out-of-the-box Serialización type-safe:\nfeatures = HousingFeatures(**request_json) features.longitude # Type: float (no Optional[Any]) Batch Predictions Support class PredictionRequest(BaseModel): \"\"\"Request model for single or batch predictions.\"\"\" instances: List[HousingFeatures] = Field( ..., description=\"List of housing features for prediction\", min_length=1 # Al menos una instancia ) Uso:\n{ \"instances\": [ {\"longitude\": -122.23, ...}, // Predict house 1 {\"longitude\": -118.45, ...}, // Predict house 2 {\"longitude\": -121.89, ...} // Predict house 3 ] } Por qué soportar batch:\nLatencia reducida: 3 requests individuales = 150ms. 1 batch de 3 = 60ms. Costo reducido: Menos HTTP overhead (headers, handshake, etc.) Inference eficiente: El modelo puede vectorizar operaciones Trade-off: Batch size muy grande (\u003e1000) puede causar timeouts. Implementar límite:\ninstances: List[HousingFeatures] = Field( ..., min_length=1, max_length=100 # Máximo 100 predicciones por request ) Response Schema class PredictionResult(BaseModel): \"\"\"Individual prediction result.\"\"\" predicted_price: float = Field(..., description=\"Predicted median house value\") confidence_interval: Optional[dict] = Field( None, description=\"Confidence interval (if available)\" ) class PredictionResponse(BaseModel): \"\"\"Response model for predictions.\"\"\" predictions: List[PredictionResult] = Field(..., description=\"List of predictions\") model_version: str = Field(..., description=\"Model version used\") model_config = { \"json_schema_extra\": { \"examples\": [{ \"predictions\": [{ \"predicted_price\": 452600.0, \"confidence_interval\": None }], \"model_version\": \"randomforest_v1\" }] } } model_version en response: Crucial para debugging. Si un cliente reporta predicciones incorrectas, el model_version te dice qué modelo usó (v1, v2, Production, etc.).\n5. Router Pattern: Endpoints y Error Handling La Estructura del Router # api/app/routers/predict.py from fastapi import APIRouter, HTTPException, status router = APIRouter(prefix=\"/api/v1\", tags=[\"predictions\"]) # Global instances (set by main.py) model_loader: ModelLoader = None wandb_logger: WandBLogger = None def set_model_loader(loader: ModelLoader) -\u003e None: \"\"\"Dependency injection pattern.\"\"\" global model_loader model_loader = loader Por qué prefix=\"/api/v1\":\n/api/v1/predict ← Versión 1 del API /api/v2/predict ← Versión 2 (breaking changes) Puedes correr ambas versiones simultáneamente durante migración:\nClientes legacy usan /api/v1/ Clientes nuevos usan /api/v2/ Deprecas v1 después de 6 meses Sin versionamiento: Breaking change → todos los clientes se rompen al mismo tiempo.\nEl Endpoint Principal: POST /api/v1/predict @router.post( \"/predict\", response_model=PredictionResponse, status_code=status.HTTP_200_OK, responses={ 400: {\"model\": ErrorResponse, \"description\": \"Invalid input data\"}, 500: {\"model\": ErrorResponse, \"description\": \"Prediction failed\"}, }, summary=\"Predict housing prices\", description=\"Make predictions for housing prices based on input features\" ) async def predict(request: PredictionRequest) -\u003e PredictionResponse: \"\"\" Predict housing prices for given features. \"\"\" # 1. Check model loaded if model_loader is None or not model_loader.is_loaded: raise HTTPException( status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=\"Model not loaded\" ) start_time = time.time() try: # 2. Convert Pydantic models to DataFrame features_list = [instance.model_dump() for instance in request.instances] df = pd.DataFrame(features_list) # 3. Make predictions predictions = model_loader.predict(df) # 4. Calculate metrics response_time_ms = (time.time() - start_time) * 1000 # 5. Format response results = [ PredictionResult(predicted_price=float(pred)) for pred in predictions ] # 6. Log to W\u0026B (async, no bloquea) if wandb_logger: wandb_logger.log_prediction( features=features_list, predictions=[float(p) for p in predictions], model_version=model_loader.model_version, response_time_ms=response_time_ms ) return PredictionResponse( predictions=results, model_version=model_loader.model_version ) except ValueError as e: # Validation error (ej: feature fuera de rango esperado) if wandb_logger: wandb_logger.log_error(\"validation_error\", str(e), features_list) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=f\"Invalid input data: {str(e)}\" ) except Exception as e: # Unexpected error (ej: modelo corrupto, OOM) if wandb_logger: wandb_logger.log_error(\"prediction_error\", str(e)) raise HTTPException( status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f\"Prediction failed: {str(e)}\" ) Decisiones de error handling:\ntry: # Inference except ValueError: # Cliente envió datos inválidos → 400 Bad Request # Loguear a W\u0026B para análisis return 400 except Exception: # Error inesperado (bug en el código/modelo) → 500 Internal Server Error # Loguear a W\u0026B para alerting return 500 Por qué distinguir 400 vs 500:\n400: Culpa del cliente. No retries automáticos. 500: Culpa del servidor. Cliente puede retry. Logging de errores a W\u0026B: Permite detectar patrones. Si ves 1000 validation_error para ocean_proximity=\"INVALID\", agregas un mensaje de error más claro.\n6. W\u0026B Logging: Observability en Producción Por Qué Loguear Predicciones Pregunta: “¿Para qué loguear cada predicción si ya tengo logs de uvicorn?”\nRespuesta: Los logs de uvicorn te dicen:\nQué endpoint se llamó HTTP status code Cuánto tardó Los logs de W\u0026B te dicen:\nQué features se usaron Qué predicción se hizo Distribución de predicciones (¿todas están en $200k-$500k? ¿hay outliers?) Latencia promedio por request Error rate (¿cuántos requests fallan?) Caso de uso real: Stakeholder reporta “las predicciones están muy altas últimamente”. Abres W\u0026B dashboard:\nprediction/mean: $450k (antes: $380k) features/median_income: 9.2 (antes: 7.5) Conclusión: No hay bug—simplemente los clientes están consultando casas en áreas más caras (median_income más alto). Sin W\u0026B, estarías debuggeando código por horas.\nLa Implementación # api/app/core/wandb_logger.py class WandBLogger: def __init__(self, project: str = \"housing-mlops-api\", enabled: bool = True): self.enabled = enabled and bool(os.getenv(\"WANDB_API_KEY\")) if self.enabled: self._run = wandb.init( project=self.project, job_type=\"api-inference\", # Distinguir de training runs config={ \"environment\": os.getenv(\"ENVIRONMENT\", \"production\"), \"model_version\": os.getenv(\"MODEL_VERSION\", \"unknown\") }, reinit=True # Permite múltiples init() en mismo proceso ) def log_prediction( self, features: List[Dict], predictions: List[float], model_version: str, response_time_ms: float ) -\u003e None: if not self.enabled: return # Métricas agregadas wandb.log({ \"prediction/count\": len(predictions), \"prediction/mean\": sum(predictions) / len(predictions), \"prediction/min\": min(predictions), \"prediction/max\": max(predictions), \"performance/response_time_ms\": response_time_ms, \"model/version\": model_version, \"timestamp\": datetime.now().isoformat() }) # Feature distributions (sample first 100) if len(features) \u003c= 100: for i, (feat, pred) in enumerate(zip(features, predictions)): wandb.log({ f\"features/instance_{i}/median_income\": feat[\"median_income\"], f\"predictions/instance_{i}\": pred }) Por qué job_type=\"api-inference\":\nEn W\u0026B dashboard, puedes filtrar por job type:\ntraining: Runs del pipeline de entrenamiento sweep: Runs del hyperparameter sweep api-inference: Predicciones en producción Por qué reinit=True: Un proceso de uvicorn puede vivir días. reinit=True permite crear múltiples W\u0026B runs dentro del mismo proceso (uno por startup/restart).\nPor qué sample first 100: Loguear 10,000 features individuales por request sería demasiado overhead. Muestrear 100 da distribución representativa sin matar performance.\nW\u0026B Dashboard en Producción # Métricas a monitorear: prediction/count: Requests per minute (RPM) - Esperado: 100-500 RPM - Alerta: \u003c10 RPM (¿está caído?) o \u003e2000 RPM (¿DDoS?) prediction/mean: Precio promedio predicho - Esperado: $300k-$450k (según mercado) - Alerta: \u003e$1M (modelo roto) o \u003c$50k (data drift) performance/response_time_ms: Latencia - Esperado: 30-60ms - Alerta: \u003e200ms (modelo lento o CPU throttling) error/count: Errores por minuto - Esperado: 0-5 errores/min - Alerta: \u003e50 errores/min (investigate immediately) 7. CORS y Security # api/app/main.py app.add_middleware( CORSMiddleware, allow_origins=[ \"http://localhost:3000\", # Frontend local (React/Streamlit) \"http://localhost:8080\", \"https://app.company.com\", # Frontend en producción ], allow_credentials=False, # No cookies (API es stateless) allow_methods=[\"GET\", \"POST\"], # Solo métodos necesarios allow_headers=[\"Content-Type\", \"Authorization\"], max_age=3600, # Cache preflight requests por 1 hora ) Por qué restricted origins:\nAnti-pattern (permissive):\nallow_origins=[\"*\"] # MALO: Cualquier sitio puede llamar tu API Problema: Un sitio malicioso evil.com puede hacer requests a tu API desde el navegador del usuario, potencialmente:\nConsumir tu cuota de GCP (si no hay auth) Hacer predicciones spam DoS attack Pattern correcto (restrictive):\nallow_origins=[\"https://app.company.com\"] # Solo tu frontend Para desarrollo local: Agregar http://localhost:3000 temporalmente, remover en producción.\nPor qué allow_credentials=False: Este API es stateless—no usa cookies ni sesiones. allow_credentials=True sería innecesario y una superficie de ataque adicional.\n8. El Flujo Completo de Una Request Request:\ncurl -X POST http://localhost:8080/api/v1/predict \\ -H \"Content-Type: application/json\" \\ -d '{ \"instances\": [{ \"longitude\": -122.23, \"latitude\": 37.88, \"housing_median_age\": 41, \"total_rooms\": 880, \"total_bedrooms\": 129, \"population\": 322, \"households\": 126, \"median_income\": 8.3252, \"ocean_proximity\": \"NEAR BAY\" }] }' El viaje interno (\u003c 50ms):\n1. FastAPI recibe request (1ms) ├─ CORS middleware valida origin └─ Router match: POST /api/v1/predict 2. Pydantic validation (2ms) ├─ Parse JSON → PredictionRequest object ├─ Validate types (longitude: float ✓) ├─ Validate ranges (longitude: -122.23, dentro de [-180, 180] ✓) └─ Custom validator (ocean_proximity: \"NEAR BAY\" → válido ✓) 3. Endpoint handler: predict() (40ms) ├─ Check model_loader.is_loaded (0.1ms) ├─ Convert Pydantic → DataFrame (1ms) ├─ Preprocessing (5ms) │ ├─ One-hot encode ocean_proximity │ ├─ Compute cluster similarity features │ └─ Scale numerical features ├─ Model inference (30ms) │ └─ RandomForest.predict(processed_features) ├─ Format response (1ms) └─ Log to W\u0026B (async, \u003c1ms non-blocking) 4. FastAPI serializa response (2ms) └─ PredictionResponse → JSON 5. HTTP response enviado (1ms) Total: ~50ms Response:\n{ \"predictions\": [{ \"predicted_price\": 452600.0, \"confidence_interval\": null }], \"model_version\": \"models:/housing_price_model/Production\" } 9. Lo Que Esta Arquitectura Logra Sin esta arquitectura (API naive):\nCargar modelo en cada request (5 segundos/request) Validación manual propensa a errores Sin observability (debugging es adivinar) Sin versionamiento de API (breaking changes rompen clientes) CORS abierto (vulnerability) Con esta arquitectura:\nLatencia: \u003c50ms por predicción (modelo cacheado) Confiabilidad: Pydantic garantiza requests válidos antes de llegar al modelo Observability: W\u0026B dashboard muestra distribución de predicciones, latencia, errores Maintainability: Separation of concerns (core/routers/models) Security: CORS restrictivo, error handling robusto Versionamiento: /api/v1/ permite evolucionar el API sin romper clientes El valor real: Este API puede escalar de 10 requests/min a 10,000 requests/min sin cambios en el código—solo agregar más containers con load balancer. La arquitectura ya está lista.\n11. Estrategias de Selección de Modelos y Parámetros Navegación ← Parte 1: Pipeline y Orquestación | Parte 3: Producción y Best Practices →\nEn la Parte 3 cubriremos:\nEstrategias de selección de modelos y parámetros Testing: Fixtures, mocking y coverage real Patrones de producción (Transform Pattern, Data Drift, Feature Stores) Checklist de Production Readiness ","wordCount":"9806","inLanguage":"en","datePublished":"2026-01-13T00:00:00Z","dateModified":"2026-01-13T00:00:00Z","author":{"@type":"Person","name":"Carlos Daniel Jiménez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://carlosdanieljimenez.com/mlops/anatomia-pipeline-mlops-parte-2/"},"publisher":{"@type":"Organization","name":"The Probability Engine","logo":{"@type":"ImageObject","url":"https://carlosdanieljimenez.com/img/icon.jpeg"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://carlosdanieljimenez.com/ accesskey=h title="The Probability Engine (Alt + H)">The Probability Engine</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://carlosdanieljimenez.com/mlops/ title=MLOps><span>MLOps</span></a></li><li><a href=https://carlosdanieljimenez.com/agentic-ai/ title="Agentic AI"><span>Agentic AI</span></a></li><li><a href=https://carlosdanieljimenez.com/tidytuesday/ title=TidyTuesday><span>TidyTuesday</span></a></li><li><a href=https://carlosdanieljimenez.com/post/ title=Posts><span>Posts</span></a></li><li><a href=https://carlosdanieljimenez.com/edge-computing/ title="Edge Computing"><span>Edge Computing</span></a></li><li><a href=https://carlosdanieljimenez.com/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://carlosdanieljimenez.com/about/ title=About><span>About</span></a></li><li><a href=https://carlosdanieljimenez.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Anatomía de un Pipeline MLOps - Parte 2: Deployment e Infraestructura</h1><div class=post-description>Parte 2: CI/CD con GitHub Actions, comparación W&amp;B vs MLflow, containerización completa con Docker, y arquitectura de API con FastAPI en producción.</div><div class=post-meta><span title='2026-01-13 00:00:00 +0000 UTC'>January 13, 2026</span>&nbsp;·&nbsp;<span>Carlos Daniel Jiménez</span></div></header><div class=post-content><blockquote><p><strong>Serie MLOps Completo:</strong> <a href=/mlops/anatomia-pipeline-mlops-parte-1/>← Parte 1: Pipeline</a> | <strong>Parte 2 (actual)</strong> | <a href=/mlops/anatomia-pipeline-mlops-parte-3/>Parte 3: Producción →</a></p></blockquote><p><a name=github-actions></a></p><h2 id=8-cicd-con-github-actions-automatización-del-pipeline-completo>8. CI/CD con GitHub Actions: Automatización del Pipeline Completo<a hidden class=anchor aria-hidden=true href=#8-cicd-con-github-actions-automatización-del-pipeline-completo>#</a></h2><h3 id=por-qué-cicd-es-crítico-en-mlops>Por Qué CI/CD Es Crítico en MLOps<a hidden class=anchor aria-hidden=true href=#por-qué-cicd-es-crítico-en-mlops>#</a></h3><p>Como MLOps engineer, uno de los mayores puntos de fricción es el deployment manual. Has entrenado un modelo excelente en tu laptop, pero llevarlo a producción requiere:</p><ol><li>SSH a un servidor</li><li>Copiar archivos manualmente</li><li>Instalar dependencias</li><li>Cruzar los dedos</li><li>Debuggear cuando algo explota</li></ol><p><strong>GitHub Actions elimina esto.</strong> Cada commit dispara un pipeline automatizado que:</p><ul><li>Ejecuta tests</li><li>Valida que el código cumple estándares</li><li>Entrena el modelo (opcional, en pipelines simples)</li><li>Construye imágenes Docker</li><li>Deploya a Cloud Run/ECS/Kubernetes</li></ul><h3 id=la-arquitectura-de-cicd-para-este-proyecto>La Arquitectura de CI/CD Para Este Proyecto<a hidden class=anchor aria-hidden=true href=#la-arquitectura-de-cicd-para-este-proyecto>#</a></h3><p>Este proyecto implementa <strong>dos workflows separados</strong>:</p><h4 id=1-pr-validation-workflow>1. PR Validation Workflow<a hidden class=anchor aria-hidden=true href=#1-pr-validation-workflow>#</a></h4><p><strong>Trigger:</strong> Cada pull request a <code>main</code></p><p><strong>Propósito:</strong> Asegurar que el código es production-ready antes de mergear</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># .github/workflows/pr_validation.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>PR Validation - Tests &amp; Linting</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>pull_request</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>branches</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>main, master]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;src/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;api/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;tests/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;pyproject.toml&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;requirements.txt&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>lint</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Lint Code</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Python 3.12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/setup-python@v5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>python-version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.12&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install uv</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>pip install uv</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install dependencies</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          uv venv
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install -e .
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install ruff pytest pytest-cov</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run Ruff linter</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          ruff check src/ tests/ api/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run Ruff formatter check</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          ruff format --check src/ tests/ api/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>unit-tests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Unit Tests</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCP_PROJECT_ID</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_PROJECT_ID }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCS_BUCKET_NAME</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCS_BUCKET_NAME }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>WANDB_API_KEY</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WANDB_API_KEY }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Python 3.12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/setup-python@v5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>python-version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.12&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install dependencies</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          pip install uv
</span></span></span><span class=line><span class=cl><span class=sd>          uv venv
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install -e .
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install pytest pytest-cov pytest-mock</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run unit tests with coverage</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          pytest tests/ -v \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov=src \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov=api/app \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov-report=xml \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov-report=term-missing \
</span></span></span><span class=line><span class=cl><span class=sd>            --cov-fail-under=70</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Upload coverage to Codecov</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>codecov/codecov-action@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>file</span><span class=p>:</span><span class=w> </span><span class=l>./coverage.xml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>flags</span><span class=p>:</span><span class=w> </span><span class=l>unittests</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>codecov-umbrella</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>integration-tests</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Integration Tests (Pipeline E2E)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCP_PROJECT_ID</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_PROJECT_ID }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>GCS_BUCKET_NAME</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCS_BUCKET_NAME }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>WANDB_API_KEY</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WANDB_API_KEY }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>MLFLOW_TRACKING_URI</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.MLFLOW_TRACKING_URI }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Python 3.12</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/setup-python@v5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>python-version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.12&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Authenticate to Google Cloud</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>google-github-actions/auth@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>credentials_json</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_SA_KEY }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Install dependencies</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          pip install uv
</span></span></span><span class=line><span class=cl><span class=sd>          uv venv
</span></span></span><span class=line><span class=cl><span class=sd>          uv pip install -e .</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run integration test (Steps 01-04)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          source .venv/bin/activate
</span></span></span><span class=line><span class=cl><span class=sd>          python main.py main.execute_steps=[01_download_data,02_preprocessing_and_imputation,03_feature_engineering,04_segregation]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>timeout-minutes</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Verify artifacts were created</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/train/train.parquet
</span></span></span><span class=line><span class=cl><span class=sd>          gsutil ls gs://${{ secrets.GCS_BUCKET_NAME }}/data/04-split/test/test.parquet</span><span class=w>
</span></span></span></code></pre></div><p><strong>Valor para el MLOps engineer:</strong></p><ul><li><strong>Previene merges rotos:</strong> Si los tests fallan, el PR no puede mergearse</li><li><strong>Estándares de código:</strong> Ruff garantiza consistencia (importa cuando tienes 5+ contributors)</li><li><strong>Coverage tracking:</strong> Codecov muestra qué porcentaje del código está cubierto por tests</li><li><strong>Fast feedback:</strong> Sabes en 5 minutos si tu cambio rompió algo, no 3 horas después</li></ul><h4 id=2-deployment-workflow>2. Deployment Workflow<a hidden class=anchor aria-hidden=true href=#2-deployment-workflow>#</a></h4><p><strong>Trigger:</strong> Push a <code>main</code> (después de merge de PR)</p><p><strong>Propósito:</strong> Construir y deployar el API a producción</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># .github/workflows/deploy_api.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Deploy API to Cloud Run</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>push</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>branches</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>main]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;api/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;models/**&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s1>&#39;.github/workflows/deploy_api.yaml&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>env</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>PROJECT_ID</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.GCP_PROJECT_ID }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>SERVICE_NAME</span><span class=p>:</span><span class=w> </span><span class=l>housing-price-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>REGION</span><span class=p>:</span><span class=w> </span><span class=l>us-central1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>build-and-deploy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Build Docker Image &amp; Deploy</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>permissions</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>contents</span><span class=p>:</span><span class=w> </span><span class=l>read</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>id-token</span><span class=p>:</span><span class=w> </span><span class=l>write</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Checkout code</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Authenticate to Google Cloud</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>google-github-actions/auth@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>workload_identity_provider</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WIF_PROVIDER }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>service_account</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.WIF_SERVICE_ACCOUNT }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Set up Cloud SDK</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>google-github-actions/setup-gcloud@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Configure Docker for GCR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>gcloud auth configure-docker gcr.io</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Download trained model from GCS</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          mkdir -p api/models/trained
</span></span></span><span class=line><span class=cl><span class=sd>          gsutil cp gs://${{ secrets.GCS_BUCKET_NAME }}/models/trained/housing_price_model.pkl \
</span></span></span><span class=line><span class=cl><span class=sd>            api/models/trained/housing_price_model.pkl</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Build Docker image</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          cd api
</span></span></span><span class=line><span class=cl><span class=sd>          docker build \
</span></span></span><span class=line><span class=cl><span class=sd>            --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --tag gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest \
</span></span></span><span class=line><span class=cl><span class=sd>            .</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Push Docker image to GCR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }}
</span></span></span><span class=line><span class=cl><span class=sd>          docker push gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Deploy to Cloud Run</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          gcloud run deploy ${{ env.SERVICE_NAME }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --image gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --platform managed \
</span></span></span><span class=line><span class=cl><span class=sd>            --region ${{ env.REGION }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --allow-unauthenticated \
</span></span></span><span class=line><span class=cl><span class=sd>            --set-env-vars=&#34;GCS_BUCKET=${{ secrets.GCS_BUCKET_NAME }},WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}&#34; \
</span></span></span><span class=line><span class=cl><span class=sd>            --memory 2Gi \
</span></span></span><span class=line><span class=cl><span class=sd>            --cpu 2 \
</span></span></span><span class=line><span class=cl><span class=sd>            --max-instances 10 \
</span></span></span><span class=line><span class=cl><span class=sd>            --min-instances 1 \
</span></span></span><span class=line><span class=cl><span class=sd>            --timeout 300</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Get Cloud Run URL</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>deploy-url</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          URL=$(gcloud run services describe ${{ env.SERVICE_NAME }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --platform managed \
</span></span></span><span class=line><span class=cl><span class=sd>            --region ${{ env.REGION }} \
</span></span></span><span class=line><span class=cl><span class=sd>            --format &#39;value(status.url)&#39;)
</span></span></span><span class=line><span class=cl><span class=sd>          echo &#34;url=$URL&#34; &gt;&gt; $GITHUB_OUTPUT</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run smoke test</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          curl -X POST &#34;${{ steps.deploy-url.outputs.url }}/api/v1/predict&#34; \
</span></span></span><span class=line><span class=cl><span class=sd>            -H &#34;Content-Type: application/json&#34; \
</span></span></span><span class=line><span class=cl><span class=sd>            -d &#39;{&#34;instances&#34;:[{&#34;longitude&#34;:-122.23,&#34;latitude&#34;:37.88,&#34;housing_median_age&#34;:41,&#34;total_rooms&#34;:880,&#34;total_bedrooms&#34;:129,&#34;population&#34;:322,&#34;households&#34;:126,&#34;median_income&#34;:8.3252,&#34;ocean_proximity&#34;:&#34;NEAR BAY&#34;}]}&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Notify deployment success</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>if</span><span class=p>:</span><span class=w> </span><span class=l>success()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          echo &#34;Deployment successful! API available at: ${{ steps.deploy-url.outputs.url }}&#34;</span><span class=w>
</span></span></span></code></pre></div><p><strong>Valor para el MLOps engineer:</strong></p><ul><li><strong>Zero-downtime deployment:</strong> Cloud Run hace rolling updates automáticamente</li><li><strong>Rollback fácil:</strong> Si algo explota, haces <code>gcloud run services update-traffic --to-revisions=PREVIOUS=100</code></li><li><strong>Smoke test automático:</strong> Verifica que el API responde después del deploy</li><li><strong>Versionado de imágenes:</strong> Cada commit tiene su propia imagen Docker taggeada con SHA</li></ul><h3 id=secretos-y-seguridad>Secretos y Seguridad<a hidden class=anchor aria-hidden=true href=#secretos-y-seguridad>#</a></h3><p><strong>CRÍTICO:</strong> Nunca commitees secrets al repo. GitHub Actions usa <strong>GitHub Secrets</strong> para guardar:</p><ul><li><code>GCP_PROJECT_ID</code>: ID del proyecto de GCP</li><li><code>GCS_BUCKET_NAME</code>: Nombre del bucket de GCS</li><li><code>WANDB_API_KEY</code>: API key de W&amp;B</li><li><code>GCP_SA_KEY</code>: Service account key (JSON) para autenticar en GCP</li><li><code>WIF_PROVIDER</code> / <code>WIF_SERVICE_ACCOUNT</code>: Workload Identity Federation (más seguro que SA keys)</li></ul><p><strong>Configuración en GitHub:</strong></p><ol><li>Ve a repo → Settings → Secrets and variables → Actions</li><li>Crea cada secret</li><li>Los workflows acceden con <code>${{ secrets.SECRET_NAME }}</code></li></ol><h3 id=monitoreo-de-deployments>Monitoreo de Deployments<a hidden class=anchor aria-hidden=true href=#monitoreo-de-deployments>#</a></h3><p><strong>¿Cómo saber si un deployment falló?</strong></p><p>GitHub Actions envía notificaciones a:</p><ul><li>Email (configurado en GitHub profile)</li><li>Slack (con GitHub app)</li><li>Discord/Teams (con webhooks)</li></ul><p><strong>Post-deployment monitoring:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># Agregar step de validación post-deploy</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run API health check</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    for i in {1..5}; do
</span></span></span><span class=line><span class=cl><span class=sd>      STATUS=$(curl -s -o /dev/null -w &#34;%{http_code}&#34; &#34;${{ steps.deploy-url.outputs.url }}/health&#34;)
</span></span></span><span class=line><span class=cl><span class=sd>      if [ $STATUS -eq 200 ]; then
</span></span></span><span class=line><span class=cl><span class=sd>        echo &#34;Health check passed&#34;
</span></span></span><span class=line><span class=cl><span class=sd>        exit 0
</span></span></span><span class=line><span class=cl><span class=sd>      fi
</span></span></span><span class=line><span class=cl><span class=sd>      echo &#34;Attempt $i failed, retrying...&#34;
</span></span></span><span class=line><span class=cl><span class=sd>      sleep 10
</span></span></span><span class=line><span class=cl><span class=sd>    done
</span></span></span><span class=line><span class=cl><span class=sd>    echo &#34;Health check failed after 5 attempts&#34;
</span></span></span><span class=line><span class=cl><span class=sd>    exit 1</span><span class=w>
</span></span></span></code></pre></div><h3 id=estrategias-avanzadas-de-cicd>Estrategias Avanzadas de CI/CD<a hidden class=anchor aria-hidden=true href=#estrategias-avanzadas-de-cicd>#</a></h3><h4 id=1-pipeline-de-reentrenamiento-automático>1. Pipeline de Reentrenamiento Automático<a hidden class=anchor aria-hidden=true href=#1-pipeline-de-reentrenamiento-automático>#</a></h4><p><strong>Trigger:</strong> Cron schedule (ejemplo: semanalmente)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>schedule</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>cron</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;0 2 * * 0&#39;</span><span class=w>  </span><span class=c># Cada domingo a las 2 AM UTC</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>retrain-model</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Run full pipeline</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>python main.py</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Compare metrics with production model</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          NEW_MAPE=$(python scripts/get_latest_mape.py)
</span></span></span><span class=line><span class=cl><span class=sd>          PROD_MAPE=$(python scripts/get_production_mape.py)
</span></span></span><span class=line><span class=cl><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>          if (( $(echo &#34;$NEW_MAPE &lt; $PROD_MAPE&#34; | bc -l) )); then
</span></span></span><span class=line><span class=cl><span class=sd>            echo &#34;New model is better, promoting to Production&#34;
</span></span></span><span class=line><span class=cl><span class=sd>            mlflow models transition --name housing_price_model --version latest --stage Production
</span></span></span><span class=line><span class=cl><span class=sd>          else
</span></span></span><span class=line><span class=cl><span class=sd>            echo &#34;New model is worse, keeping current Production model&#34;
</span></span></span><span class=line><span class=cl><span class=sd>          fi</span><span class=w>
</span></span></span></code></pre></div><p><strong>Valor:</strong> El modelo se reentrena automáticamente con datos nuevos. Si mejora, se promociona a Production. Si empeora, se descarta.</p><h4 id=2-canary-deployments>2. Canary Deployments<a hidden class=anchor aria-hidden=true href=#2-canary-deployments>#</a></h4><p><strong>Problema:</strong> Un nuevo modelo puede tener bugs sutiles que no aparecen en tests.</p><p><strong>Solución:</strong> Deployar el nuevo modelo a solo 10% del tráfico, monitorear por 1 hora, luego migrar 100% si no hay errores.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Deploy canary (10% traffic)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    gcloud run services update-traffic ${{ env.SERVICE_NAME }} \
</span></span></span><span class=line><span class=cl><span class=sd>      --to-revisions=LATEST=10,PREVIOUS=90</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Wait and monitor</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>sleep 3600 </span><span class=w> </span><span class=c># 1 hora</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Check error rate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    ERROR_RATE=$(python scripts/check_error_rate.py --minutes=60)
</span></span></span><span class=line><span class=cl><span class=sd>    if (( $(echo &#34;$ERROR_RATE &gt; 0.05&#34; | bc -l) )); then
</span></span></span><span class=line><span class=cl><span class=sd>      echo &#34;Error rate too high, rolling back&#34;
</span></span></span><span class=line><span class=cl><span class=sd>      gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=PREVIOUS=100
</span></span></span><span class=line><span class=cl><span class=sd>      exit 1
</span></span></span><span class=line><span class=cl><span class=sd>    fi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Promote to 100% traffic</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>    gcloud run services update-traffic ${{ env.SERVICE_NAME }} --to-revisions=LATEST=100</span><span class=w>
</span></span></span></code></pre></div><h3 id=lo-que-cicd-resuelve-en-mlops>Lo Que CI/CD Resuelve en MLOps<a hidden class=anchor aria-hidden=true href=#lo-que-cicd-resuelve-en-mlops>#</a></h3><p><strong>Sin CI/CD:</strong></p><ul><li>Deployment manual propenso a errores</li><li>&ldquo;Funciona en mi máquina&rdquo; syndrome</li><li>Testing inconsistente</li><li>Rollback requiere pánico debugging</li><li>No hay historial de qué se deployó cuándo</li></ul><p><strong>Con CI/CD:</strong></p><ul><li>Deployment automático en cada merge</li><li>Tests garantizan que el código funciona</li><li>Rollback es un comando</li><li>Historial completo en GitHub Actions UI</li><li>Cada deployment es reproducible</li></ul><h3 id=el-valor-real-para-el-mlops-engineer>El Valor Real Para el MLOps Engineer<a hidden class=anchor aria-hidden=true href=#el-valor-real-para-el-mlops-engineer>#</a></h3><p><strong>No es sobre automatizar por automatizar.</strong> Es sobre:</p><ol><li><strong>Reducir toil:</strong> Gastas tiempo resolviendo problemas interesantes, no copiando archivos manualmente</li><li><strong>Confianza:</strong> Sabes que el código funciona antes de llegar a producción</li><li><strong>Velocidad:</strong> De commit a producción en &lt;10 minutos</li><li><strong>Auditoría:</strong> Cada cambio está loggeado en GitHub</li><li><strong>Colaboración:</strong> Tu equipo puede deployar sin depender de ti</li></ol><p><strong>Un MLOps engineer sin CI/CD es como un software engineer sin git—técnicamente posible, pero fundamentalmente broken.</strong></p><hr><p><a name=mlops-value-proposition></a></p><h2 id=9-el-valor-de-mlops-por-qué-esto-importa>9. El Valor de MLOps: Por Qué Esto Importa<a hidden class=anchor aria-hidden=true href=#9-el-valor-de-mlops-por-qué-esto-importa>#</a></h2><h3 id=la-pregunta-central>La Pregunta Central<a hidden class=anchor aria-hidden=true href=#la-pregunta-central>#</a></h3><p>&ldquo;¿Por qué debería invertir tiempo en todo esto cuando puedo entrenar un modelo en un notebook en 30 minutos?&rdquo;</p><p>Esta es la pregunta que todo MLOps engineer ha escuchado. La respuesta corta: <strong>porque el notebook no escala.</strong></p><p>La respuesta larga es lo que cubre esta sección.</p><h3 id=el-problema-real-research-code-vs-production-code>El Problema Real: Research Code vs Production Code<a hidden class=anchor aria-hidden=true href=#el-problema-real-research-code-vs-production-code>#</a></h3><h4 id=research-code-notebook>Research Code (Notebook)<a hidden class=anchor aria-hidden=true href=#research-code-notebook>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># notebook.ipynb</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 1</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;housing.csv&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 2</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 3</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestRegressor</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 4</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pickle</span>
</span></span><span class=line><span class=cl><span class=n>pickle</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;model.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;wb&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cell 5</span>
</span></span><span class=line><span class=cl><span class=c1># Wait, did I drop the right columns?</span>
</span></span><span class=line><span class=cl><span class=c1># Let me rerun cell 2... oh no, I ran it twice</span>
</span></span><span class=line><span class=cl><span class=c1># Now I have 0 rows, what happened?</span>
</span></span></code></pre></div><p><strong>Problemas:</strong></p><ul><li>No reproducible (orden de ejecución importa)</li><li>No testeable</li><li>No versionable (git diffs son ilegibles)</li><li>No escalable (qué pasa con 100GB de datos?)</li><li>No auditable (qué params usaste?)</li></ul><h4 id=production-code-este-pipeline>Production Code (Este Pipeline)<a hidden class=anchor aria-hidden=true href=#production-code-este-pipeline>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/05_model_selection/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@hydra.main</span><span class=p>(</span><span class=n>config_path</span><span class=o>=</span><span class=s2>&#34;.&#34;</span><span class=p>,</span> <span class=n>config_name</span><span class=o>=</span><span class=s2>&#34;config&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Entrenar modelo con configuración versionada.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Cargar datos desde GCS (single source of truth)</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>load_from_gcs</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>gcs_train_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Aplicar preprocessing pipeline serializado</span>
</span></span><span class=line><span class=cl>    <span class=n>pipeline</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;artifacts/preprocessing_pipeline.pkl&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Entrenar con params de config</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>hyperparameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Loggear a MLflow</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_params</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hyperparameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>(</span><span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></div><p><strong>Beneficios:</strong></p><ul><li>Reproducible (mismo config = mismo output)</li><li>Testeable (funciones puras, mocking)</li><li>Versionable (git diff legible)</li><li>Escalable (corre en local o en cluster)</li><li>Auditable (MLflow tracking)</li></ul><h3 id=valor-1-modularización-de-código>Valor #1: Modularización de Código<a hidden class=anchor aria-hidden=true href=#valor-1-modularización-de-código>#</a></h3><h4 id=por-qué-importa>Por Qué Importa<a hidden class=anchor aria-hidden=true href=#por-qué-importa>#</a></h4><p><strong>Escenario:</strong> Tu modelo tiene bug en preprocessing. En notebook, el preprocessing está mezclado con feature engineering, entrenamiento y evaluación en 300 líneas.</p><p><strong>En este pipeline:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Bug está en preprocessing → solo editas src/data/02_preprocessing/</span>
</span></span><span class=line><span class=cl><span class=c1># Tests fallan → pytest tests/test_preprocessor.py</span>
</span></span><span class=line><span class=cl><span class=c1># Fixeas → reejecutas solo step 02-07, no 01</span>
</span></span></code></pre></div><p><strong>Tiempo ahorrado:</strong> Horas por bug.</p><h4 id=separation-of-concerns>Separation of Concerns<a hidden class=anchor aria-hidden=true href=#separation-of-concerns>#</a></h4><p>Este pipeline separa:</p><ol><li><strong>Data steps (01-04):</strong> Producen artifacts reutilizables</li><li><strong>Model steps (05-07):</strong> Consumen artifacts, producen modelos</li><li><strong>API:</strong> Consume modelos, produce predicciones</li><li><strong>Frontend:</strong> Consume API, produce UX</li></ol><p><strong>Beneficio:</strong> Equipos pueden trabajar en paralelo. El data scientist modifica feature engineering sin tocar el API. El frontend engineer modifica UI sin entender Random Forests.</p><h3 id=valor-2-working-with-artifacts>Valor #2: Working with Artifacts<a hidden class=anchor aria-hidden=true href=#valor-2-working-with-artifacts>#</a></h3><h4 id=el-problema-dónde-está-el-model_final_v3pkl>El Problema: &ldquo;¿Dónde está el model_final_v3.pkl?&rdquo;<a hidden class=anchor aria-hidden=true href=#el-problema-dónde-está-el-model_final_v3pkl>#</a></h4><p>Sin artifact management:</p><pre tabindex=0><code>models/
├── model_v1.pkl
├── model_v2.pkl
├── model_final.pkl
├── model_final_FINAL.pkl
├── model_final_REAL.pkl
├── model_production_2024_01_15.pkl  # ¿Este es el de producción?
└── model_old_backup.pkl  # ¿Puedo borrarlo?
</code></pre><p><strong>Problemas:</strong></p><ul><li>No sabes qué hiperparámetros usa cada uno</li><li>No sabes qué métricas logró</li><li>No sabes con qué datos se entrenó</li><li>Rollback = buscar el archivo correcto</li></ul><h4 id=la-solución-artifact-storage--metadata>La Solución: Artifact Storage + Metadata<a hidden class=anchor aria-hidden=true href=#la-solución-artifact-storage--metadata>#</a></h4><p><strong>1. Google Cloud Storage para datos:</strong></p><pre tabindex=0><code>gs://bucket-name/
├── data/
│   ├── 01-raw/housing.parquet                    # Inmutable
│   ├── 02-processed/housing_processed.parquet    # Versionado por fecha
│   ├── 03-features/housing_features.parquet
│   └── 04-split/
│       ├── train/train.parquet
│       └── test/test.parquet
├── artifacts/
│   ├── imputer.pkl                               # Preprocessing artifacts
│   ├── preprocessing_pipeline.pkl
│   └── scaler.pkl
└── models/
    └── trained/housing_price_model.pkl           # Latest trained
</code></pre><p><strong>Beneficios:</strong></p><ul><li><strong>Inmutabilidad:</strong> <code>01-raw/</code> nunca cambia, siempre puedes reejecutar el pipeline</li><li><strong>Versionamiento:</strong> Cada run tiene timestamp, puedes comparar versiones</li><li><strong>Compartir:</strong> Todo el equipo accede a los mismos datos, no &ldquo;enviame el CSV por Slack&rdquo;</li></ul><p><strong>2. MLflow para modelos:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Registrar modelo</span>
</span></span><span class=line><span class=cl><span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MLflow guarda automáticamente:</span>
</span></span><span class=line><span class=cl><span class=c1># - El pickle del modelo</span>
</span></span><span class=line><span class=cl><span class=c1># - Los hiperparámetros (n_estimators=200, max_depth=20)</span>
</span></span><span class=line><span class=cl><span class=c1># - Las métricas (MAPE=7.8%, R²=0.87)</span>
</span></span><span class=line><span class=cl><span class=c1># - Metadata (fecha, duración, usuario)</span>
</span></span><span class=line><span class=cl><span class=c1># - Código (git commit SHA)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cargar modelo en producción</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>pyfunc</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Beneficios:</strong></p><ul><li><strong>Versionamiento semántico:</strong> v1, v2, v3 con stages (Staging/Production)</li><li><strong>Metadata rica:</strong> Sabes exactamente qué es cada versión</li><li><strong>Rollback trivial:</strong> <code>transition v2 to Production</code></li><li><strong>Comparación:</strong> MLflow UI muestra tabla comparando todas las versiones</li></ul><p><strong>3. W&amp;B para experimentos:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Cada run de sweep loggea:</span>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;hyperparameters/n_estimators&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;hyperparameters/max_depth&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metrics/mape&#34;</span><span class=p>:</span> <span class=mf>7.8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metrics/r2&#34;</span><span class=p>:</span> <span class=mf>0.87</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;plots/feature_importances&#34;</span><span class=p>:</span> <span class=n>wandb</span><span class=o>.</span><span class=n>Image</span><span class=p>(</span><span class=n>fig</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;dataset/train_size&#34;</span><span class=p>:</span> <span class=mi>16512</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># W&amp;B dashboard:</span>
</span></span><span class=line><span class=cl><span class=c1># - Tabla con 50 runs de sweep</span>
</span></span><span class=line><span class=cl><span class=c1># - Filtrar por MAPE &lt; 8%</span>
</span></span><span class=line><span class=cl><span class=c1># - Parallel coordinates plot mostrando relación entre hiperparámetros y MAPE</span>
</span></span><span class=line><span class=cl><span class=c1># - Comparar top 5 runs side-by-side</span>
</span></span></code></pre></div><p><strong>Beneficios:</strong></p><ul><li><strong>Visualización:</strong> Plots interactivos de cómo cada hiperparámetro afecta métricas</li><li><strong>Colaboración:</strong> Tu equipo ve tus experimentos en real-time</li><li><strong>Reproducibilidad:</strong> Cada run tiene link permanente con todo el contexto</li></ul><h3 id=valor-3-pipeline-architecture>Valor #3: Pipeline Architecture<a hidden class=anchor aria-hidden=true href=#valor-3-pipeline-architecture>#</a></h3><h4 id=por-qué-un-pipeline-no-un-script>Por Qué Un Pipeline, No Un Script<a hidden class=anchor aria-hidden=true href=#por-qué-un-pipeline-no-un-script>#</a></h4><p><strong>Script único (run_all.py):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># run_all.py (500 líneas)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1># Download data</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>download_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Preprocess</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>preprocess</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Feature engineering</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>add_features</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Train model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>train_model</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Deploy</span>
</span></span><span class=line><span class=cl>    <span class=n>deploy_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Problemas:</strong></p><ul><li>Si falla en train_model(), reejecutas TODO (incluyendo download lento)</li><li>No puedes ejecutar solo feature engineering para experimentar</li><li>Cambiar preprocessing requiere reentrenar todo</li><li>No hay checkpoints intermedios</li></ul><p><strong>Pipeline modular:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Ejecutar todo</span>
</span></span><span class=line><span class=cl>make run-pipeline
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ejecutar solo preprocessing</span>
</span></span><span class=line><span class=cl>make run-preprocessing
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ejecutar desde feature engineering en adelante</span>
</span></span><span class=line><span class=cl>python main.py main.execute_steps<span class=o>=[</span>03_feature_engineering,04_segregation,05_model_selection<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Debugging: ejecutar solo step que falló</span>
</span></span><span class=line><span class=cl>python src/data/03_feature_engineering/main.py --debug
</span></span></code></pre></div><p><strong>Beneficios:</strong></p><ul><li><strong>Ejecución selectiva:</strong> Solo reejecutas lo que cambió</li><li><strong>Debugging rápido:</strong> Testeas un step aislado</li><li><strong>Paralelización:</strong> Steps independientes pueden correr en paralelo</li><li><strong>Checkpointing:</strong> Si falla step 05, steps 01-04 ya están hechos</li></ul><h4 id=el-contrato-entre-steps>El Contrato Entre Steps<a hidden class=anchor aria-hidden=true href=#el-contrato-entre-steps>#</a></h4><p>Cada step:</p><ul><li><strong>Input:</strong> Path a artifact en GCS (ejemplo: <code>data/02-processed/housing_processed.parquet</code>)</li><li><strong>Output:</strong> Path a nuevo artifact en GCS (ejemplo: <code>data/03-features/housing_features.parquet</code>)</li><li><strong>Side effects:</strong> Logs a MLflow/W&amp;B</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Step 03: Feature Engineering</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>run</span><span class=p>(</span><span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Input</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span> <span class=o>=</span> <span class=n>load_from_gcs</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>gcs_input_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Transform</span>
</span></span><span class=line><span class=cl>    <span class=n>df_transformed</span> <span class=o>=</span> <span class=n>apply_feature_engineering</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Output</span>
</span></span><span class=line><span class=cl>    <span class=n>save_to_gcs</span><span class=p>(</span><span class=n>df_transformed</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>gcs_output_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Side effects</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_artifact</span><span class=p>(</span><span class=s2>&#34;preprocessing_pipeline.pkl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span><span class=s2>&#34;optimization/optimal_k&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>})</span>
</span></span></code></pre></div><p>Este <strong>contrato</strong> permite que cada step sea:</p><ul><li>Testeado independientemente</li><li>Desarrollado por diferentes personas</li><li>Reemplazado sin afectar otros steps</li></ul><h3 id=valor-4-production-ready-vs-research-code>Valor #4: Production-Ready vs Research Code<a hidden class=anchor aria-hidden=true href=#valor-4-production-ready-vs-research-code>#</a></h3><h4 id=checklist-de-production-ready>Checklist de Production-Ready<a hidden class=anchor aria-hidden=true href=#checklist-de-production-ready>#</a></h4><table><thead><tr><th>Feature</th><th>Research Code</th><th>Este Pipeline</th></tr></thead><tbody><tr><td><strong>Versionamiento</strong></td><td>Git (mal, notebooks)</td><td>Git + GCS + MLflow</td></tr><tr><td><strong>Testing</strong></td><td>Manual (&ldquo;lo corrí una vez&rdquo;)</td><td>pytest + CI</td></tr><tr><td><strong>Configuración</strong></td><td>Hardcoded</td><td>YAML versionado</td></tr><tr><td><strong>Secretos</strong></td><td>Expuestos en código</td><td>.env + GitHub Secrets</td></tr><tr><td><strong>Logs</strong></td><td>print() statements</td><td>Logging estructurado</td></tr><tr><td><strong>Monitoring</strong></td><td>&ldquo;Espero que funcione&rdquo;</td><td>W&amp;B + MLflow tracking</td></tr><tr><td><strong>Deployment</strong></td><td>Manual</td><td>CI/CD automático</td></tr><tr><td><strong>Rollback</strong></td><td>Panic debugging</td><td>Transition en MLflow</td></tr><tr><td><strong>Documentación</strong></td><td>README desactualizado</td><td>Código autodocumentado + Markdown en MLflow</td></tr><tr><td><strong>Colaboración</strong></td><td>&ldquo;Ejecuta estas 10 celdas en orden&rdquo;</td><td><code>make run-pipeline</code></td></tr></tbody></table><h4 id=el-costo-real-de-no-hacer-mlops>El Costo Real de No Hacer MLOps<a hidden class=anchor aria-hidden=true href=#el-costo-real-de-no-hacer-mlops>#</a></h4><p><strong>Escenario:</strong> Un modelo en producción tiene bug que causa predicciones incorrectas.</p><p><strong>Sin MLOps (Research Code):</strong></p><ol><li>Detectar el bug: Usuario reporta → 2 horas</li><li>Reproducir el bug: Buscar qué código/datos se usaron → 4 horas</li><li>Fixear: Correr notebook localmente → 1 hora</li><li>Deployar: SSH, copiar pickle, restart server → 30 min</li><li>Verificar: Correr tests manuales → 1 hora</li><li><strong>Total: 8.5 horas de downtime</strong></li></ol><p><strong>Con MLOps (Este Pipeline):</strong></p><ol><li>Detectar el bug: Monitoring automático alerta → 5 min</li><li>Rollback: <code>transition v3 to Archived</code> + <code>transition v2 to Production</code> → 2 min</li><li>Fix: Identificar issue con MLflow metadata, fixear código → 1 hora</li><li>Deployar: Push to GitHub → CI/CD automático → 10 min</li><li>Verificar: Smoke tests automáticos pasan → 1 min</li><li><strong>Total: 1 hora 18 min de downtime (>85% reducción)</strong></li></ol><p><strong>Ahorro anualizado:</strong> Si esto pasa 4 veces al año, ahorras 29 horas de tiempo de ingeniero.</p><h3 id=valor-5-decisiones-respaldadas-por-datos>Valor #5: Decisiones Respaldadas por Datos<a hidden class=anchor aria-hidden=true href=#valor-5-decisiones-respaldadas-por-datos>#</a></h3><h4 id=el-anti-pattern>El Anti-Pattern<a hidden class=anchor aria-hidden=true href=#el-anti-pattern>#</a></h4><p>&ldquo;Usé Random Forest con <code>n_estimators=100</code> porque eso es lo que hace todo el mundo.&rdquo;</p><p><strong>Problema:</strong> No tienes evidencia de que es la mejor opción.</p><h4 id=este-pipeline>Este Pipeline<a hidden class=anchor aria-hidden=true href=#este-pipeline>#</a></h4><p>Cada decisión tiene métricas cuantificables:</p><p><strong>1. Imputación:</strong></p><ul><li>Comparó 4 estrategias (Simple median, Simple mean, KNN, IterativeImputer)</li><li>IterativeImputer ganó con RMSE=0.52 (vs 0.78 de median)</li><li>Plot de comparación en W&amp;B: <code>wandb.ai/project/run/imputation_comparison</code></li></ul><p><strong>2. Feature Engineering:</strong></p><ul><li>Optimizó K de 5 a 15</li><li>K=8 maximizó silhouette score (0.64)</li><li>Plot de elbow method en W&amp;B</li></ul><p><strong>3. Hyperparameter Tuning:</strong></p><ul><li>Sweep bayesiano de 50 runs</li><li>Optimal config: <code>n_estimators=200, max_depth=20</code></li><li>MAPE mejoró de 8.5% a 7.8%</li><li>Link a sweep: <code>wandb.ai/project/sweeps/abc123</code></li></ul><p><strong>Beneficio:</strong> Seis meses después, cuando el stakeholder pregunta &ldquo;¿por qué usamos este modelo?&rdquo;, abres W&amp;B/MLflow y la respuesta está ahí con plots y métricas.</p><h3 id=el-roi-de-mlops>El ROI de MLOps<a hidden class=anchor aria-hidden=true href=#el-roi-de-mlops>#</a></h3><p><strong>Inversión inicial:</strong></p><ul><li>Setup de GCS, MLflow, W&amp;B, CI/CD: 2-3 días</li><li>Refactoring de código a pipeline modular: 1-2 semanas</li></ul><p><strong>Retorno:</strong></p><ul><li>Deployment time: 8 horas → 10 minutos (48x más rápido)</li><li>Debugging time: 4 horas → 30 min (8x más rápido)</li><li>Onboarding nuevos engineers: 1 semana → 1 día</li><li>Confianza del equipo: &ldquo;Espero que funcione&rdquo; → &ldquo;Sé que funciona&rdquo;</li></ul><p><strong>Para un equipo de 5 personas, el breakeven es ~1 mes.</strong></p><h3 id=la-lección-final-para-mlops-engineers>La Lección Final Para MLOps Engineers<a hidden class=anchor aria-hidden=true href=#la-lección-final-para-mlops-engineers>#</a></h3><p><strong>No es sobre las herramientas.</strong> Puedes reemplazar:</p><ul><li>GCS → S3 → Azure Blob</li><li>MLflow → Neptune → Comet</li><li>W&amp;B → TensorBoard → MLflow</li><li>GitHub Actions → GitLab CI → Jenkins</li></ul><p><strong>Es sobre los principios:</strong></p><ol><li><strong>Modularización:</strong> Código en módulos testeables, no notebooks monolíticos</li><li><strong>Artifact Management:</strong> Datos y modelos versionados, no <code>model_final_v3.pkl</code></li><li><strong>Automatización:</strong> CI/CD elimina toil</li><li><strong>Observabilidad:</strong> Logs, métricas, tracking</li><li><strong>Reproducibilidad:</strong> Mismo input → mismo output</li><li><strong>Decisiones data-driven:</strong> Cada elección respaldada por métricas</li></ol><p><strong>Cuando entiendes esto, eres un MLOps engineer. Cuando lo implementas, eres un buen MLOps engineer.</strong></p><hr><p><a name=wandb-vs-mlflow></a></p><h2 id=95-wb-vs-mlflow-por-qué-ambos-no-uno-u-otro>9.5. W&amp;B vs MLflow: Por Qué Ambos, No Uno u Otro<a hidden class=anchor aria-hidden=true href=#95-wb-vs-mlflow-por-qué-ambos-no-uno-u-otro>#</a></h2><h3 id=la-pregunta-incómoda>La Pregunta Incómoda<a hidden class=anchor aria-hidden=true href=#la-pregunta-incómoda>#</a></h3><p>&ldquo;¿Por qué tienes Weights & Biases Y MLflow? ¿No son lo mismo?&rdquo;</p><p>Esta pregunta revela un malentendido fundamental sobre lo que hace cada herramienta. No son competidores—son <strong>aliados con responsabilidades diferentes</strong>. Entender esto separa un data scientist que experimenta de un MLOps engineer que construye sistemas.</p><p>La respuesta corta: <strong>W&amp;B es tu laboratorio de investigación. MLflow es tu cadena de producción.</strong></p><p>La respuesta larga es lo que cubre esta sección, con ejemplos del código real de este proyecto.</p><hr><h3 id=el-problema-real-experimentación-vs-governance>El Problema Real: Experimentación vs Governance<a hidden class=anchor aria-hidden=true href=#el-problema-real-experimentación-vs-governance>#</a></h3><h4 id=fase-1-experimentación-50-100-runsdía>Fase 1: Experimentación (50-100 runs/día)<a hidden class=anchor aria-hidden=true href=#fase-1-experimentación-50-100-runsdía>#</a></h4><p>Cuando estás en fase de experimentación:</p><ul><li>Corres 50 sweep runs probando combinaciones de hiperparámetros</li><li>Necesitas ver <strong>en tiempo real</strong> cómo evoluciona cada run</li><li>Quieres comparar visualmente 20 runs simultáneos</li><li>Necesitas ver plots de convergencia, distribuciones de features, confusion matrices</li><li>El overhead de logging debe ser mínimo (logging asíncrono)</li></ul><p><strong>Herramienta correcta:</strong> Weights & Biases</p><h4 id=fase-2-governance-y-deployment-1-2-modelossemana>Fase 2: Governance y Deployment (1-2 modelos/semana)<a hidden class=anchor aria-hidden=true href=#fase-2-governance-y-deployment-1-2-modelossemana>#</a></h4><p>Cuando subes un modelo a producción:</p><ul><li>Necesitas versionamiento semántico (v1, v2, v3)</li><li>Necesitas stages (Staging → Production)</li><li>Necesitas metadata rica (¿qué hiperparámetros? ¿qué datos? ¿qué commit?)</li><li>Necesitas un API para cargar modelos (<code>models:/housing_price_model/Production</code>)</li><li>Necesitas rollback trivial (transition v2 to Production)</li></ul><p><strong>Herramienta correcta:</strong> MLflow Model Registry</p><p><strong>La verdad incómoda:</strong> Ninguna herramienta hace bien ambas cosas.</p><hr><h3 id=cómo-este-proyecto-usa-wb>Cómo Este Proyecto Usa W&amp;B<a hidden class=anchor aria-hidden=true href=#cómo-este-proyecto-usa-wb>#</a></h3><h4 id=1-hyperparameter-sweep-step-06-bayesian-optimization>1. Hyperparameter Sweep (Step 06): Bayesian Optimization<a hidden class=anchor aria-hidden=true href=#1-hyperparameter-sweep-step-06-bayesian-optimization>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/06_sweep/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Configuración del sweep (Bayesian optimization)</span>
</span></span><span class=line><span class=cl><span class=n>sweep_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;method&#34;</span><span class=p>:</span> <span class=s2>&#34;bayes&#34;</span><span class=p>,</span>  <span class=c1># Bayesian &gt; Grid &gt; Random</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;metric&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;wmape&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;goal&#34;</span><span class=p>:</span> <span class=s2>&#34;minimize&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;early_terminate&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;hyperband&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_iter&#34;</span><span class=p>:</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;parameters&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>50</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>300</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>30</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_samples_leaf&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;min&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;max&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Inicializar sweep</span>
</span></span><span class=line><span class=cl><span class=n>sweep_id</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>sweep</span><span class=p>(</span><span class=n>sweep</span><span class=o>=</span><span class=n>sweep_config</span><span class=p>,</span> <span class=n>project</span><span class=o>=</span><span class=s2>&#34;housing-mlops-gcp&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Función de training que W&amp;B llama 50 veces</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>run</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>init</span><span class=p>()</span>  <span class=c1># W&amp;B asigna hiperparámetros automáticamente</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Obtener hiperparámetros sugeridos por Bayesian optimizer</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Entrenar modelo</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>n_estimators</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>n_estimators</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_depth</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>max_depth</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=c1># ...</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Evaluar</span>
</span></span><span class=line><span class=cl>    <span class=n>metrics</span> <span class=o>=</span> <span class=n>evaluate_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Log a W&amp;B (asíncrono, no bloquea)</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;hyperparameters/n_estimators&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>n_estimators</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;hyperparameters/max_depth&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>max_depth</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;metrics/mape&#34;</span><span class=p>:</span> <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;mape&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;metrics/wmape&#34;</span><span class=p>:</span> <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;wmape&#39;</span><span class=p>],</span>  <span class=c1># Optimizer usa esto</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;metrics/r2&#34;</span><span class=p>:</span> <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;r2&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;plots/feature_importances&#34;</span><span class=p>:</span> <span class=n>wandb</span><span class=o>.</span><span class=n>Image</span><span class=p>(</span><span class=n>fig</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>run</span><span class=o>.</span><span class=n>finish</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ejecutar 50 runs con Bayesian optimization</span>
</span></span><span class=line><span class=cl><span class=n>wandb</span><span class=o>.</span><span class=n>agent</span><span class=p>(</span><span class=n>sweep_id</span><span class=p>,</span> <span class=n>function</span><span class=o>=</span><span class=n>train</span><span class=p>,</span> <span class=n>count</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Lo que W&amp;B hace aquí que MLflow no puede:</strong></p><ol><li><p><strong>Bayesian Optimization</strong>: W&amp;B sugiere los próximos hiperparámetros basándose en runs previos. No es random—usa Gaussian Processes para explorar el espacio eficientemente.</p><pre tabindex=0><code>Run 1: n_estimators=100, max_depth=15 → wMAPE=8.5%
Run 2: n_estimators=200, max_depth=20 → wMAPE=7.9%  # Mejor
Run 3: n_estimators=250, max_depth=22 → wMAPE=7.8%  # W&amp;B sugiere valores cercanos a Run 2
</code></pre></li><li><p><strong>Early Termination (Hyperband)</strong>: Si un run va mal en las primeras 3 iteraciones (epochs), W&amp;B lo mata automáticamente y prueba otros hiperparámetros. Ahorra ~40% de compute.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;early_terminate&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;hyperband&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;min_iter&#34;</span><span class=p>:</span> <span class=mi>3</span>  <span class=c1># Mínimo 3 iteraciones antes de terminar</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><p><strong>Parallel Coordinates Plot</strong>: Visualización interactiva mostrando qué combinación de hiperparámetros produce mejor wMAPE.</p><p><img alt="W&amp;B Parallel Coordinates" loading=lazy src=https://docs.wandb.ai/assets/images/parallel-coordinates.png></p><p><strong>Interpretación:</strong> Las líneas azules (runs con wMAPE bajo) convergen en <code>n_estimators=200-250</code> y <code>max_depth=20-25</code>. Esto te dice visualmente dónde está el óptimo.</p></li><li><p><strong>Logging Asíncrono</strong>: <code>wandb.log()</code> no bloquea. Mientras el modelo entrena, W&amp;B sube métricas en background. Total overhead: &lt;1% del training time.</p></li></ol><p><strong>MLflow no tiene:</strong></p><ul><li>Bayesian optimization (solo Grid/Random search vía scikit-learn)</li><li>Early termination inteligente</li><li>Parallel coordinates plots</li><li>Logging asíncrono (mlflow.log es síncrono)</li></ul><hr><h4 id=2-real-time-monitoring-ver-runs-mientras-corren>2. Real-Time Monitoring: Ver Runs Mientras Corren<a hidden class=anchor aria-hidden=true href=#2-real-time-monitoring-ver-runs-mientras-corren>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># En W&amp;B dashboard (web UI):</span>
</span></span><span class=line><span class=cl><span class=c1># - Ver 50 runs simultáneos en tabla interactiva</span>
</span></span><span class=line><span class=cl><span class=c1># - Filtrar por &#34;wmape &lt; 8.0%&#34; → muestra solo 12 runs</span>
</span></span><span class=line><span class=cl><span class=c1># - Comparar top 5 runs side-by-side</span>
</span></span><span class=line><span class=cl><span class=c1># - Ver plots de convergencia (MAPE vs iteration)</span>
</span></span></code></pre></div><p><strong>Caso de uso real:</strong> Inicias un sweep de 50 runs a las 9 AM. A las 10 AM, desde tu laptop en la cafetería:</p><ol><li>Abres W&amp;B dashboard</li><li>Ves que 30 runs ya terminaron</li><li>Filtras por <code>wmape &lt; 8.0%</code> → 8 runs cumplen</li><li>Comparas esos 8 runs → identificas que <code>max_depth=20</code> aparece en todos</li><li><strong>Decisión:</strong> Cancelas el sweep, ajustas el range de <code>max_depth</code> a [18, 25], reinicias</li></ol><p><strong>Valor:</strong> Retroalimentación inmediata sin SSH al server, sin leer logs en terminal. La experimentación es <strong>interactiva</strong>, no batch.</p><hr><h4 id=3-artifact-tracking-ligero-referencias-a-gcs>3. Artifact Tracking Ligero (Referencias a GCS)<a hidden class=anchor aria-hidden=true href=#3-artifact-tracking-ligero-referencias-a-gcs>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/05_model_selection/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Upload modelo a GCS</span>
</span></span><span class=line><span class=cl><span class=n>model_gcs_uri</span> <span class=o>=</span> <span class=n>upload_model_to_gcs</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;models/05-selection/randomforest_best.pkl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># gs://bucket/models/05-selection/randomforest_best.pkl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Log referencia en W&amp;B (NO sube el pickle, solo el URI)</span>
</span></span><span class=line><span class=cl><span class=n>artifact</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>Artifact</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=o>=</span><span class=s2>&#34;best_model_selection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nb>type</span><span class=o>=</span><span class=s2>&#34;model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Best model selected: RandomForest&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>artifact</span><span class=o>.</span><span class=n>add_reference</span><span class=p>(</span><span class=n>model_gcs_uri</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;best_model.pkl&#34;</span><span class=p>)</span>  <span class=c1># Solo el URI</span>
</span></span><span class=line><span class=cl><span class=n>run</span><span class=o>.</span><span class=n>log_artifact</span><span class=p>(</span><span class=n>artifact</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>W&amp;B no almacena el modelo</strong>—solo guarda el URI <code>gs://...</code>. El modelo vive en GCS.</p><p><strong>Ventaja:</strong> No pagas doble storage (GCS + W&amp;B). W&amp;B es el índice, GCS es el almacén.</p><hr><h3 id=cómo-este-proyecto-usa-mlflow>Cómo Este Proyecto Usa MLflow<a hidden class=anchor aria-hidden=true href=#cómo-este-proyecto-usa-mlflow>#</a></h3><h4 id=1-model-registry-step-07-versionamiento-y-stages>1. Model Registry (Step 07): Versionamiento y Stages<a hidden class=anchor aria-hidden=true href=#1-model-registry-step-07-versionamiento-y-stages>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/model/07_registration/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>(</span><span class=n>run_name</span><span class=o>=</span><span class=s2>&#34;model_registration&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Log modelo</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Log params y métricas</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_params</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=mi>20</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;min_samples_split&#34;</span><span class=p>:</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>({</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;mape&#34;</span><span class=p>:</span> <span class=mf>7.82</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;r2&#34;</span><span class=p>:</span> <span class=mf>0.8654</span>
</span></span><span class=line><span class=cl>    <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Registrar en Model Registry</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span> <span class=o>=</span> <span class=n>MlflowClient</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Crear modelo registrado (si no existe)</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span><span class=o>.</span><span class=n>create_registered_model</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Housing price prediction - Random Forest&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Crear nueva versión</span>
</span></span><span class=line><span class=cl>    <span class=n>model_version</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>create_model_version</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>source</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;runs:/</span><span class=si>{</span><span class=n>run_id</span><span class=si>}</span><span class=s2>/model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>run_id</span><span class=o>=</span><span class=n>run_id</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Resultado: housing_price_model/v3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Transicionar a stage</span>
</span></span><span class=line><span class=cl>    <span class=n>client</span><span class=o>.</span><span class=n>transition_model_version_stage</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>version</span><span class=o>=</span><span class=n>model_version</span><span class=o>.</span><span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>stage</span><span class=o>=</span><span class=s2>&#34;Staging&#34;</span>  <span class=c1># Staging → Production cuando se valide</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p><strong>Lo que MLflow hace aquí que W&amp;B no puede:</strong></p><ol><li><p><strong>Versionamiento Semántico</strong>: Cada modelo es <code>housing_price_model/v1</code>, <code>v2</code>, <code>v3</code>. No son IDs aleatorios—son versiones incrementales.</p></li><li><p><strong>Stages</strong>: Un modelo pasa por <code>None → Staging → Production → Archived</code>. Este lifecycle es explícito.</p><pre tabindex=0><code>v1: Production (actual en el API)
v2: Staging (validándose)
v3: None (recién entrenado)
v4: Archived (deprecado)
</code></pre></li><li><p><strong>Model-as-Code API</strong>: Cargar modelo en el API es trivial:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/model_loader.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>pyfunc</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>No necesitas saber:</strong></p><ul><li>Dónde está el pickle físicamente</li><li>Qué versión es (MLflow resuelve &ldquo;Production&rdquo; → v1)</li><li>Cómo deserializarlo (mlflow.pyfunc abstrae esto)</li></ul></li><li><p><strong>Rollback en 10 Segundos</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Problema: v3 en Production tiene bug</span>
</span></span><span class=line><span class=cl><span class=c1># Rollback a v2:</span>
</span></span><span class=line><span class=cl>mlflow models transition <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stage Production
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># El API detecta el cambio y recarga v2 automáticamente</span>
</span></span></code></pre></div></li><li><p><strong>Metadata Rica con Tags y Description</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Agregar tags searchables</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>set_model_version_tag</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;training_date&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;2026-01-13&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>set_model_version_tag</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sweep_id&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;abc123xyz&#34;</span>  <span class=c1># Link al W&amp;B sweep</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Description en Markdown</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>update_model_version</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=o>=</span><span class=s2>&#34;housing_price_model&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=o>=</span><span class=n>version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    # Housing Price Model v3
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    **Trained:** 2026-01-13
</span></span></span><span class=line><span class=cl><span class=s2>    **Algorithm:** Random Forest
</span></span></span><span class=line><span class=cl><span class=s2>    **Metrics:** MAPE=7.8%, R²=0.865
</span></span></span><span class=line><span class=cl><span class=s2>    **Sweep:** [W&amp;B Link](https://wandb.ai/project/sweeps/abc123)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>Resultado:</strong> 6 meses después, cuando un stakeholder pregunta &ldquo;¿qué modelo está en Production?&rdquo;, abres MLflow UI y toda la info está ahí—no en un Slack thread perdido.</p></li></ol><p><strong>W&amp;B no tiene:</strong></p><ul><li>Model Registry (solo artifact tracking básico)</li><li>Stages (Staging/Production)</li><li>API de carga (<code>models:/name/stage</code>)</li><li>Transition history (quién cambió v2 a Production, cuándo, por qué)</li></ul><hr><h4 id=2-pipeline-orchestration-mainpy>2. Pipeline Orchestration (main.py)<a hidden class=anchor aria-hidden=true href=#2-pipeline-orchestration-mainpy>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@hydra.main</span><span class=p>(</span><span class=n>config_path</span><span class=o>=</span><span class=s2>&#34;.&#34;</span><span class=p>,</span> <span class=n>config_name</span><span class=o>=</span><span class=s2>&#34;config&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>go</span><span class=p>(</span><span class=n>config</span><span class=p>:</span> <span class=n>DictConfig</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># MLflow orquesta steps como sub-runs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 01: Download</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>run</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>uri</span><span class=o>=</span><span class=s2>&#34;src/data/01_download_data&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>entry_point</span><span class=o>=</span><span class=s2>&#34;main&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>parameters</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;file_url&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>download_data</span><span class=o>.</span><span class=n>file_url</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;gcs_output_path&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>download_data</span><span class=o>.</span><span class=n>gcs_output_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Step 02: Preprocessing</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>run</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>uri</span><span class=o>=</span><span class=s2>&#34;src/data/02_preprocessing_and_imputation&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>entry_point</span><span class=o>=</span><span class=s2>&#34;main&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>parameters</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;gcs_input_path&#34;</span><span class=p>:</span> <span class=n>config</span><span class=o>.</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>gcs_input_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># ... Steps 03-07</span>
</span></span></code></pre></div><p><strong>MLflow crea un run jerárquico:</strong></p><pre tabindex=0><code>Parent Run: end_to_end_pipeline
├── Child Run: 01_download_data
│   ├── params: file_url, gcs_output_path
│   └── artifacts: housing.parquet
├── Child Run: 02_preprocessing_and_imputation
│   ├── params: imputation_strategy
│   └── artifacts: imputer.pkl, housing_processed.parquet
├── Child Run: 03_feature_engineering
│   └── ...
└── Child Run: 07_registration
    └── artifacts: model.pkl, model_config.yaml
</code></pre><p><strong>Valor:</strong> En MLflow UI, ves toda la ejecución del pipeline como un árbol. Cada step es auditable—qué params usó, cuánto tardó, qué artifacts produjo.</p><p><strong>W&amp;B no tiene orquestación de pipelines</strong>—solo tracking de runs individuales.</p><hr><h3 id=la-división-del-trabajo-en-este-proyecto>La División del Trabajo en Este Proyecto<a hidden class=anchor aria-hidden=true href=#la-división-del-trabajo-en-este-proyecto>#</a></h3><table><thead><tr><th>Responsabilidad</th><th>W&amp;B</th><th>MLflow</th><th>Razón</th></tr></thead><tbody><tr><td><strong>Bayesian hyperparameter optimization</strong></td><td>✓</td><td>✗</td><td>W&amp;B tiene sweep inteligente, MLflow solo Grid/Random</td></tr><tr><td><strong>Real-time dashboards</strong></td><td>✓</td><td>✗</td><td>W&amp;B UI es interactivo, MLflow UI es estático</td></tr><tr><td><strong>Parallel coordinates plots</strong></td><td>✓</td><td>✗</td><td>W&amp;B tiene visualizaciones avanzadas</td></tr><tr><td><strong>Early termination (Hyperband)</strong></td><td>✓</td><td>✗</td><td>W&amp;B implementa Hyperband/ASHA/Median stopping</td></tr><tr><td><strong>Model Registry con stages</strong></td><td>✗</td><td>✓</td><td>MLflow tiene Staging/Production, W&amp;B no</td></tr><tr><td><strong>Model-as-code API</strong></td><td>✗</td><td>✓</td><td><code>mlflow.pyfunc.load_model()</code> es el estándar</td></tr><tr><td><strong>Rollback de modelos</strong></td><td>✗</td><td>✓</td><td>MLflow transition, W&amp;B no tiene concepto de stages</td></tr><tr><td><strong>Pipeline orchestration</strong></td><td>✗</td><td>✓</td><td><code>mlflow.run()</code> ejecuta steps anidados</td></tr><tr><td><strong>Artifact storage (físico)</strong></td><td>✗</td><td>✗</td><td>Ambos apuntan a GCS, no duplican storage</td></tr><tr><td><strong>Logging asíncrono</strong></td><td>✓</td><td>✗</td><td>W&amp;B no bloquea training, MLflow sí</td></tr><tr><td><strong>Metadata searchable</strong></td><td>✓</td><td>✓</td><td>Ambos permiten tags/búsqueda, implementaciones diferentes</td></tr></tbody></table><hr><h3 id=el-flujo-completo-wb--mlflow>El Flujo Completo: W&amp;B → MLflow<a hidden class=anchor aria-hidden=true href=#el-flujo-completo-wb--mlflow>#</a></h3><p><strong>Día 1-3: Experimentación (W&amp;B)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Ejecutar sweep de 50 runs</span>
</span></span><span class=line><span class=cl>make run-sweep
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># W&amp;B dashboard muestra:</span>
</span></span><span class=line><span class=cl><span class=c1># - 50 runs en tabla</span>
</span></span><span class=line><span class=cl><span class=c1># - Parallel coordinates plot</span>
</span></span><span class=line><span class=cl><span class=c1># - Best run: n_estimators=200, max_depth=20, wMAPE=7.8%</span>
</span></span><span class=line><span class=cl><span class=c1># - Sweep ID: abc123xyz</span>
</span></span></code></pre></div><p><strong>Output:</strong> <code>src/model/06_sweep/best_params.yaml</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>hyperparameters</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>n_estimators</span><span class=p>:</span><span class=w> </span><span class=m>200</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_depth</span><span class=p>:</span><span class=w> </span><span class=m>20</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>min_samples_split</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>min_samples_leaf</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metrics</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mape</span><span class=p>:</span><span class=w> </span><span class=m>7.82</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>wmape</span><span class=p>:</span><span class=w> </span><span class=m>7.76</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>r2</span><span class=p>:</span><span class=w> </span><span class=m>0.8654</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>sweep_id</span><span class=p>:</span><span class=w> </span><span class=l>abc123xyz </span><span class=w> </span><span class=c># Link a W&amp;B</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>best_run_id</span><span class=p>:</span><span class=w> </span><span class=l>def456ghi</span><span class=w>
</span></span></span></code></pre></div><p><strong>Día 4: Registration (MLflow)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Step 07 lee best_params.yaml</span>
</span></span><span class=line><span class=cl>python main.py main.execute_steps<span class=o>=[</span>07_registration<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MLflow:</span>
</span></span><span class=line><span class=cl><span class=c1># 1. Entrena modelo con best_params</span>
</span></span><span class=line><span class=cl><span class=c1># 2. Registra como housing_price_model/v3</span>
</span></span><span class=line><span class=cl><span class=c1># 3. Transiciona a Staging</span>
</span></span><span class=line><span class=cl><span class=c1># 4. Guarda metadata (incluyendo sweep_id)</span>
</span></span></code></pre></div><p><strong>Día 5-7: Validación en Staging</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># API corre con modelo en Staging</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Staging <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Correr tests, validar métricas, revisar predicciones</span>
</span></span></code></pre></div><p><strong>Día 8: Promoción a Production</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mlflow models transition <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stage Production
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API en producción auto-recarga v3</span>
</span></span><span class=line><span class=cl><span class=c1># v2 queda como fallback (stage: Archived)</span>
</span></span></code></pre></div><p><strong>Si algo falla:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Rollback en 10 segundos</span>
</span></span><span class=line><span class=cl>mlflow models transition <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stage Production
</span></span></code></pre></div><hr><h3 id=por-qué-ambos-definitivamente>Por Qué Ambos, Definitivamente<a hidden class=anchor aria-hidden=true href=#por-qué-ambos-definitivamente>#</a></h3><p><strong>Pregunta:</strong> &ldquo;¿Puedo usar solo W&amp;B?&rdquo;</p><p><strong>Respuesta:</strong> Puedes, pero pierdes:</p><ul><li>Model Registry (versionamiento, stages, rollback)</li><li>API estándar para cargar modelos en producción</li><li>Pipeline orchestration con runs jerárquicos</li></ul><p><strong>Resultado:</strong> Terminas construyendo tu propio sistema de versionamiento de modelos con scripts custom—reinventando la rueda mal.</p><p><strong>Pregunta:</strong> &ldquo;¿Puedo usar solo MLflow?&rdquo;</p><p><strong>Respuesta:</strong> Puedes, pero pierdes:</p><ul><li>Bayesian optimization (tendrás que hacer Grid Search lento)</li><li>Visualizaciones interactivas (parallel coordinates, real-time dashboards)</li><li>Early termination inteligente (desperdicias compute)</li></ul><p><strong>Resultado:</strong> Tus sweeps toman 3x más tiempo, y no tienes feedback visual de qué funciona.</p><hr><h3 id=el-costo-real>El Costo Real<a hidden class=anchor aria-hidden=true href=#el-costo-real>#</a></h3><p><strong>W&amp;B:</strong></p><ul><li>Free tier: 100GB storage, colaboradores ilimitados</li><li>Team tier: $50/usuario/mes (para equipos >5 personas)</li></ul><p><strong>MLflow:</strong></p><ul><li>Open source, gratis</li><li>Costo: Hosting del tracking server (Cloud Run: ~$20/mes para uso moderado)</li><li>Storage: GCS (ya lo pagas para datos)</li></ul><p><strong>Total para equipo de 5:</strong> ~$20-50/mes (si usas W&amp;B free tier) o ~$270/mes (si usas W&amp;B Team).</p><p><strong>ROI:</strong> Si un sweep más eficiente ahorra 30 minutos de compute/día:</p><ul><li>Compute ahorrado: ~15 horas/mes</li><li>En GCP: 15 horas × $2/hora (GPU) = $30/mes ahorrado solo en compute</li><li>Más el tiempo de ingeniero (más valioso)</li></ul><p><strong>Breakeven en &lt;1 mes.</strong></p><hr><h3 id=la-lección-para-mlops-engineers>La Lección Para MLOps Engineers<a hidden class=anchor aria-hidden=true href=#la-lección-para-mlops-engineers>#</a></h3><p><strong>No elijas herramientas por hype o popularidad.</strong> Elige por <strong>responsabilidades claras</strong>:</p><ol><li><strong>Experimentación rápida e interactiva:</strong> W&amp;B, Neptune, Comet</li><li><strong>Governance y deployment:</strong> MLflow, Seldon, BentoML</li><li><strong>Artifact storage:</strong> GCS, S3, Azure Blob (no herramientas de tracking)</li></ol><p><strong>Este proyecto usa:</strong></p><ul><li><strong>W&amp;B:</strong> Porque necesita sweep Bayesiano eficiente</li><li><strong>MLflow:</strong> Porque necesita Model Registry production-ready</li><li><strong>GCS:</strong> Porque necesita storage de alta disponibilidad</li></ul><p><strong>No hay redundancia—hay especialización.</strong></p><p>Cuando entiendes esto, dejas de preguntar &ldquo;¿W&amp;B o MLflow?&rdquo; y empiezas a preguntar &ldquo;¿qué problema estoy resolviendo?&rdquo;</p><p><strong>Esa es la diferencia entre usar herramientas y construir sistemas.</strong></p><hr><p><a name=docker-mlflow></a></p><h2 id=10-docker-y-mlflow-containerización-del-ecosistema-completo>10. Docker y MLflow: Containerización del Ecosistema Completo<a hidden class=anchor aria-hidden=true href=#10-docker-y-mlflow-containerización-del-ecosistema-completo>#</a></h2><h3 id=la-arquitectura-de-tres-containers>La Arquitectura de Tres Containers<a hidden class=anchor aria-hidden=true href=#la-arquitectura-de-tres-containers>#</a></h3><p>Este proyecto utiliza <strong>tres Dockerfiles distintos</strong>, cada uno optimizado para su propósito específico:</p><ol><li><strong>Pipeline Container (<code>Dockerfile</code>)</strong>: Ejecuta el pipeline completo de entrenamiento con MLflow tracking</li><li><strong>API Container (<code>api/Dockerfile</code>)</strong>: Sirve predicciones con FastAPI en producción</li><li><strong>Streamlit Container (<code>streamlit_app/Dockerfile</code>)</strong>: Proporciona interfaz web interactiva</li></ol><h2>Esta separación no es accidental—es una decisión arquitectónica que refleja los diferentes requisitos de cada componente.
<img loading=lazy src=img/app1.png></h2><h3 id=1-pipeline-container-entrenamiento-con-mlflow-tracking>1. Pipeline Container: Entrenamiento con MLflow Tracking<a hidden class=anchor aria-hidden=true href=#1-pipeline-container-entrenamiento-con-mlflow-tracking>#</a></h3><h4 id=dockerfile-del-pipeline>Dockerfile del Pipeline<a hidden class=anchor aria-hidden=true href=#dockerfile-del-pipeline>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Dockerfile for MLOps Pipeline Execution</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Purpose: Run the complete training pipeline in containerized environment</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> python:3.12-slim</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>maintainer</span><span class=o>=</span><span class=s2>&#34;danieljimenez88m@gmail.com&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>description</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction - MLOps Pipeline&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set working directory</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install system dependencies</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    gcc <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    g++ <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    git <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy requirements first for better caching</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> pyproject.toml ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt* ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install Python dependencies</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir --upgrade pip <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    pip install --no-cache-dir uv <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    uv pip install --system -e .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy application code</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> . .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set environment variables</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># Create necessary directories</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p mlruns outputs models<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Default command runs the pipeline</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;python&#34;</span><span class=p>,</span> <span class=s2>&#34;main.py&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><h4 id=decisiones-técnicas-críticas>Decisiones Técnicas Críticas<a hidden class=anchor aria-hidden=true href=#decisiones-técnicas-críticas>#</a></h4><p><strong>1. Por Qué <code>gcc</code> y <code>g++</code></strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> apt-get install -y gcc g++ git curl<span class=err>
</span></span></span></code></pre></div><p>Muchos paquetes de ML (numpy, scipy, scikit-learn) compilan extensiones C/C++ durante la instalación. Sin estos compiladores, <code>pip install</code> falla con errores crípticos como &ldquo;error: command &lsquo;gcc&rsquo; failed&rdquo;.</p><p><strong>Trade-off:</strong> Imagen más grande (~500MB vs ~150MB de Python slim puro), pero garantiza que todas las dependencias se instalan correctamente.</p><p><strong>2. Layer Caching Strategy</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># Copy requirements first for better caching</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> pyproject.toml ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt* ./<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install ...<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy application code AFTER</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> . .<span class=err>
</span></span></span></code></pre></div><p>Docker cachea layers. Si cambias código Python pero no dependencias, Docker reutiliza la layer de <code>pip install</code> (que toma 5 minutos) y solo recopia el código (10 segundos).</p><p><strong>Sin esta optimización:</strong> Cada cambio de código requiere reinstalar todas las dependencias.</p><p><strong>3. Directory Creation for MLflow</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> mkdir -p mlruns outputs models<span class=err>
</span></span></span></code></pre></div><p>MLflow escribe artifacts a <code>mlruns/</code> por defecto si no se configura un tracking server remoto. Si este directorio no existe con permisos correctos, MLflow falla silenciosamente.</p><p><strong><code>outputs/</code></strong>: Para plots y análisis intermedios
<strong><code>models/</code></strong>: Para checkpoints de modelos antes de subir a GCS</p><h4 id=cómo-habilitar-mlflow-tracking>Cómo Habilitar MLflow Tracking<a hidden class=anchor aria-hidden=true href=#cómo-habilitar-mlflow-tracking>#</a></h4><p><strong>Opción 1: MLflow Local (Default)</strong></p><p>Cuando ejecutas el pipeline en este container, MLflow escribe a <code>mlruns/</code> dentro del container:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run --env-file .env housing-pipeline:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># MLflow escribe a /app/mlruns/</span>
</span></span><span class=line><span class=cl><span class=c1># Para ver el UI:</span>
</span></span><span class=line><span class=cl>docker <span class=nb>exec</span> -it &lt;container-id&gt; mlflow ui --host 0.0.0.0 --port <span class=m>5000</span>
</span></span></code></pre></div><p><strong>Limitación:</strong> Los runs se pierden cuando el container se detiene.</p><p><strong>Opción 2: MLflow Remote Tracking Server</strong></p><p>Para persistir runs, configura un servidor MLflow separado:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># docker-compose.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mlflow</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>ghcr.io/mlflow/mlflow:v2.9.2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>mlflow-server</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;5000:5000&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>BACKEND_STORE_URI=sqlite:///mlflow.db</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>DEFAULT_ARTIFACT_ROOT=gs://your-bucket/mlflow-artifacts</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlflow-data:/mlflow</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>&gt;</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      mlflow server
</span></span></span><span class=line><span class=cl><span class=sd>      --backend-store-uri sqlite:///mlflow/mlflow.db
</span></span></span><span class=line><span class=cl><span class=sd>      --default-artifact-root gs://your-bucket/mlflow-artifacts
</span></span></span><span class=line><span class=cl><span class=sd>      --host 0.0.0.0
</span></span></span><span class=line><span class=cl><span class=sd>      --port 5000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>pipeline</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w> </span><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>MLFLOW_TRACKING_URI=http://mlflow:5000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>GCP_PROJECT_ID=${GCP_PROJECT_ID}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>GCS_BUCKET_NAME=${GCS_BUCKET_NAME}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>WANDB_API_KEY=${WANDB_API_KEY}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlflow</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mlflow-data</span><span class=p>:</span><span class=w>
</span></span></span></code></pre></div><p><strong>Configuración en el código:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># main.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>mlflow</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Si MLFLOW_TRACKING_URI está configurado, usar ese server</span>
</span></span><span class=line><span class=cl><span class=n>mlflow_uri</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;MLFLOW_TRACKING_URI&#34;</span><span class=p>,</span> <span class=s2>&#34;file:./mlruns&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mlflow</span><span class=o>.</span><span class=n>set_tracking_uri</span><span class=p>(</span><span class=n>mlflow_uri</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>mlflow</span><span class=o>.</span><span class=n>set_experiment</span><span class=p>(</span><span class=s2>&#34;housing_price_prediction&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=c1># Log params, metrics, artifacts</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_param</span><span class=p>(</span><span class=s2>&#34;n_estimators&#34;</span><span class=p>,</span> <span class=mi>200</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metric</span><span class=p>(</span><span class=s2>&#34;mape&#34;</span><span class=p>,</span> <span class=mf>7.82</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;model&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Opción 3: MLflow en Cloud (Production)</strong></p><p>Para producción, usa un servidor MLflow gestionado:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Deploy MLflow a Cloud Run (serverless)</span>
</span></span><span class=line><span class=cl>gcloud run deploy mlflow-server <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --image ghcr.io/mlflow/mlflow:v2.9.2 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --platform managed <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --region us-central1 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set-env-vars<span class=o>=</span><span class=s2>&#34;BACKEND_STORE_URI=postgresql://user:pass@host/mlflow_db,DEFAULT_ARTIFACT_ROOT=gs://bucket/mlflow&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --allow-unauthenticated
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Obtener URL</span>
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_URL</span><span class=o>=</span><span class=k>$(</span>gcloud run services describe mlflow-server --format <span class=s1>&#39;value(status.url)&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Configurar en pipeline</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MLFLOW_TRACKING_URI</span><span class=o>=</span><span class=nv>$MLFLOW_URL</span>
</span></span></code></pre></div><h4 id=ejecución-del-pipeline-container>Ejecución del Pipeline Container<a hidden class=anchor aria-hidden=true href=#ejecución-del-pipeline-container>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Build</span>
</span></span><span class=line><span class=cl>docker build -t housing-pipeline:latest .
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run con env vars</span>
</span></span><span class=line><span class=cl>docker run <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --env-file .env <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/mlruns:/app/mlruns <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-pipeline:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run con steps específicos</span>
</span></span><span class=line><span class=cl>docker run <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --env-file .env <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-pipeline:latest <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  python main.py main.execute_steps<span class=o>=[</span>03_feature_engineering,05_model_selection<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ver logs en tiempo real</span>
</span></span><span class=line><span class=cl>docker logs -f &lt;container-id&gt;
</span></span></code></pre></div><p><strong>Volume Mount (<code>-v</code>)</strong>: Monta <code>mlruns/</code> desde el host al container para persistir runs MLflow incluso después de que el container se detenga.</p><hr><h3 id=2-api-container-inference-en-producción>2. API Container: Inference en Producción<a hidden class=anchor aria-hidden=true href=#2-api-container-inference-en-producción>#</a></h3><h4 id=dockerfile-del-api>Dockerfile del API<a hidden class=anchor aria-hidden=true href=#dockerfile-del-api>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Dockerfile for Housing Price Prediction API</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Purpose: Production-ready FastAPI service for Cloud Run deployment</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> python:3.12-slim</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>maintainer</span><span class=o>=</span><span class=s2>&#34;danieljimenez88m@gmail.com&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>description</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction API - FastAPI Service&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install system dependencies (solo curl para healthcheck)</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy requirements first for better caching</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install Python dependencies</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir --upgrade pip <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    pip install --no-cache-dir -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy application code</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> app/ ./app/<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Create models directory</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p models<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set environment variables</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PORT</span><span class=o>=</span><span class=m>8080</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># Expose port</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>EXPOSE</span><span class=s> 8080</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Health check</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>HEALTHCHECK --interval=30s --timeout=10s </span>--start-period<span class=o>=</span>40s --retries<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    CMD curl -f http://localhost:8080/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Run the application</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=nb>exec</span> uvicorn app.main:app --host 0.0.0.0 --port <span class=si>${</span><span class=nv>PORT</span><span class=si>}</span><span class=err>
</span></span></span></code></pre></div><h4 id=decisiones-técnicas-críticas-1>Decisiones Técnicas Críticas<a hidden class=anchor aria-hidden=true href=#decisiones-técnicas-críticas-1>#</a></h4><p><strong>1. Imagen Más Ligera</strong></p><p>Comparado con el pipeline container:</p><ul><li><strong>No necesita <code>gcc</code>/<code>g++</code></strong>: Las dependencias ya están compiladas en wheels</li><li><strong>No necesita <code>git</code></strong>: No clona repos</li><li><strong>Solo <code>curl</code></strong>: Para el healthcheck</li></ul><p><strong>Resultado:</strong> Imagen de ~200MB vs ~500MB del pipeline.</p><p><strong>Por qué importa:</strong> Cloud Run cobra por uso de memoria. Una imagen más pequeña = menos memoria = menos costo.</p><p><strong>2. Health Check Nativo</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=err>HEALTHCHECK --interval=30s --timeout=10s </span>--start-period<span class=o>=</span>40s --retries<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    CMD curl -f http://localhost:8080/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span></code></pre></div><p>Docker marca el container como &ldquo;unhealthy&rdquo; si el endpoint <code>/health</code> falla 3 veces consecutivas.</p><p><strong>Cloud Run</strong> y <strong>Kubernetes</strong> usan esto para:</p><ul><li>No enviar tráfico a containers unhealthy</li><li>Reiniciar containers que fallan</li><li>Reporting de uptime</li></ul><p><strong>start-period=40s</strong>: Da 40 segundos al API para cargar el modelo antes de empezar health checks.</p><p><strong>3. Port Configuration Flexible</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PORT</span><span class=o>=</span><span class=m>8080</span>
</span></span><span class=line><span class=cl><span class=k>CMD</span> <span class=nb>exec</span> uvicorn app.main:app --host 0.0.0.0 --port <span class=si>${</span><span class=nv>PORT</span><span class=si>}</span><span class=err>
</span></span></span></code></pre></div><p>Cloud Run inyecta <code>PORT</code> como env var (puede ser 8080, 8081, etc.). El API debe leer este valor, no hardcodearlo.</p><p><strong><code>exec</code></strong>: Reemplaza el shell process con uvicorn, permitiendo que Docker envíe signals (SIGTERM) directamente a uvicorn para graceful shutdown.</p><h4 id=cómo-el-api-carga-el-modelo>Cómo el API Carga el Modelo<a hidden class=anchor aria-hidden=true href=#cómo-el-api-carga-el-modelo>#</a></h4><p>El API tiene <strong>tres estrategias de carga de modelo</strong> con fallback automático:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/model_loader.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ModelLoader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Carga modelo desde MLflow → GCS → Local con fallback.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>load_model</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Any</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Priority: MLflow &gt; GCS &gt; Local&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Estrategia 1: Desde MLflow Registry</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>model_uri</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;models:/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_stage</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>pyfunc</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=n>model_uri</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loaded from MLflow: </span><span class=si>{</span><span class=n>model_uri</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MLflow load failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>, trying GCS...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Estrategia 2: Desde GCS</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>storage_client</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>bucket</span> <span class=o>=</span> <span class=n>storage_client</span><span class=o>.</span><span class=n>bucket</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>blob</span> <span class=o>=</span> <span class=n>bucket</span><span class=o>.</span><span class=n>blob</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=n>model_bytes</span> <span class=o>=</span> <span class=n>blob</span><span class=o>.</span><span class=n>download_as_bytes</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>model_bytes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loaded from GCS: gs://</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GCS load failed: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>, trying local...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Estrategia 3: Desde archivo local (fallback)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=ow>and</span> <span class=n>Path</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span><span class=o>.</span><span class=n>exists</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>,</span> <span class=s1>&#39;rb&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loaded from local: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;No model could be loaded from any source&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Configuración con env vars:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Producción: Cargar desde MLflow</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_TRACKING_URI</span><span class=o>=</span>https://mlflow.example.com <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Production <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Staging: Cargar desde GCS</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>GCS_BUCKET</span><span class=o>=</span>my-bucket <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>GCS_MODEL_PATH</span><span class=o>=</span>models/trained/housing_price_model.pkl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Desarrollo: Cargar desde local</span>
</span></span><span class=line><span class=cl>docker run -p 8080:8080 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/models:/app/models <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -e <span class=nv>LOCAL_MODEL_PATH</span><span class=o>=</span>/app/models/trained/housing_price_model.pkl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  housing-api:latest
</span></span></code></pre></div><hr><h3 id=3-streamlit-container-frontend-interactivo>3. Streamlit Container: Frontend Interactivo<a hidden class=anchor aria-hidden=true href=#3-streamlit-container-frontend-interactivo>#</a></h3><h4 id=dockerfile-de-streamlit>Dockerfile de Streamlit<a hidden class=anchor aria-hidden=true href=#dockerfile-de-streamlit>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Dockerfile for Streamlit Frontend</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Purpose: Interactive web interface for housing price predictions</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># =================================================================</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> python:3.12-slim</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>maintainer</span><span class=o>=</span><span class=s2>&#34;danieljimenez88m@gmail.com&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>LABEL</span> <span class=nv>description</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction - Streamlit Frontend&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=s> /app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y curl <span class=o>&amp;&amp;</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir --upgrade pip <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    pip install --no-cache-dir -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> app.py .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Create .streamlit directory for config</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> mkdir -p .streamlit<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Streamlit configuration</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> <span class=nb>echo</span> <span class=s1>&#39;\
</span></span></span><span class=line><span class=cl><span class=s1>[server]\n\
</span></span></span><span class=line><span class=cl><span class=s1>port = 8501\n\
</span></span></span><span class=line><span class=cl><span class=s1>address = &#34;0.0.0.0&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>headless = true\n\
</span></span></span><span class=line><span class=cl><span class=s1>enableCORS = false\n\
</span></span></span><span class=line><span class=cl><span class=s1>enableXsrfProtection = true\n\
</span></span></span><span class=line><span class=cl><span class=s1>\n\
</span></span></span><span class=line><span class=cl><span class=s1>[browser]\n\
</span></span></span><span class=line><span class=cl><span class=s1>gatherUsageStats = false\n\
</span></span></span><span class=line><span class=cl><span class=s1>\n\
</span></span></span><span class=line><span class=cl><span class=s1>[theme]\n\
</span></span></span><span class=line><span class=cl><span class=s1>primaryColor = &#34;#FF4B4B&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>backgroundColor = &#34;#FFFFFF&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>secondaryBackgroundColor = &#34;#F0F2F6&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>textColor = &#34;#262730&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>font = &#34;sans serif&#34;\n\
</span></span></span><span class=line><span class=cl><span class=s1>&#39;</span> &gt; .streamlit/config.toml<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PYTHONUNBUFFERED</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>ENV</span> <span class=nv>PYTHONDONTWRITEBYTECODE</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>EXPOSE</span><span class=s> 8501</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>HEALTHCHECK --interval=30s --timeout=10s </span>--start-period<span class=o>=</span>40s --retries<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    CMD curl -f http://localhost:8501/_stcore/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;streamlit&#34;</span><span class=p>,</span> <span class=s2>&#34;run&#34;</span><span class=p>,</span> <span class=s2>&#34;app.py&#34;</span><span class=p>,</span> <span class=s2>&#34;--server.port=8501&#34;</span><span class=p>,</span> <span class=s2>&#34;--server.address=0.0.0.0&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><h4 id=decisiones-técnicas-críticas-2>Decisiones Técnicas Críticas<a hidden class=anchor aria-hidden=true href=#decisiones-técnicas-críticas-2>#</a></h4><p><strong>1. Configuración Embedded</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> <span class=nb>echo</span> <span class=s1>&#39;...&#39;</span> &gt; .streamlit/config.toml<span class=err>
</span></span></span></code></pre></div><p>Streamlit requiere configuración para correr en containers (headless mode, CORS, etc.). En lugar de commitear un archivo <code>config.toml</code> al repo, lo generamos en build time.</p><p><strong>Ventajas:</strong></p><ul><li>Un archivo menos en el repo</li><li>Configuración versionada con el Dockerfile</li><li>No hay riesgo de olvidar commitear el config</li></ul><p><strong>2. Health Check de Streamlit</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=err>HEALTHCHECK</span> CMD curl -f http://localhost:8501/_stcore/health <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span><span class=err>
</span></span></span></code></pre></div><p>Streamlit expone <code>/_stcore/health</code> automáticamente. Este endpoint retorna 200 si la app está running.</p><p><strong>3. Tema Personalizado</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-toml data-lang=toml><span class=line><span class=cl><span class=p>[</span><span class=nx>theme</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nx>primaryColor</span> <span class=p>=</span> <span class=s2>&#34;#FF4B4B&#34;</span>
</span></span><span class=line><span class=cl><span class=nx>backgroundColor</span> <span class=p>=</span> <span class=s2>&#34;#FFFFFF&#34;</span>
</span></span><span class=line><span class=cl><span class=nx>secondaryBackgroundColor</span> <span class=p>=</span> <span class=s2>&#34;#F0F2F6&#34;</span>
</span></span><span class=line><span class=cl><span class=nx>textColor</span> <span class=p>=</span> <span class=s2>&#34;#262730&#34;</span>
</span></span></code></pre></div><p>El tema define los colores de botones, backgrounds, etc. Esto da consistencia visual sin necesidad de CSS custom en cada componente.</p><h4 id=cómo-streamlit-se-conecta-al-api>Cómo Streamlit Se Conecta al API<a hidden class=anchor aria-hidden=true href=#cómo-streamlit-se-conecta-al-api>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># streamlit_app/app.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>streamlit</span> <span class=k>as</span> <span class=nn>st</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Read API URL from environment variable</span>
</span></span><span class=line><span class=cl><span class=n>API_URL</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;API_URL&#34;</span><span class=p>,</span> <span class=s2>&#34;http://localhost:8080&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>API_PREDICT_ENDPOINT</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>API_URL</span><span class=si>}</span><span class=s2>/api/v1/predict&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>make_prediction</span><span class=p>(</span><span class=n>features</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Call API to get prediction.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>payload</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;instances&#34;</span><span class=p>:</span> <span class=p>[</span><span class=n>features</span><span class=p>]}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>API_PREDICT_ENDPOINT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>json</span><span class=o>=</span><span class=n>payload</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>timeout</span><span class=o>=</span><span class=mi>10</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span><span class=o>.</span><span class=n>raise_for_status</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=n>requests</span><span class=o>.</span><span class=n>exceptions</span><span class=o>.</span><span class=n>RequestException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>st</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;API Error: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Streamlit UI</span>
</span></span><span class=line><span class=cl><span class=n>st</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Housing Price Prediction&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>st</span><span class=o>.</span><span class=n>form</span><span class=p>(</span><span class=s2>&#34;prediction_form&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>longitude</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>number_input</span><span class=p>(</span><span class=s2>&#34;Longitude&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=-</span><span class=mf>122.23</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>latitude</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>number_input</span><span class=p>(</span><span class=s2>&#34;Latitude&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=mf>37.88</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... más inputs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>submitted</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>form_submit_button</span><span class=p>(</span><span class=s2>&#34;Predict&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>submitted</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>features</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;longitude&#34;</span><span class=p>:</span> <span class=n>longitude</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;latitude&#34;</span><span class=p>:</span> <span class=n>latitude</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=n>make_prediction</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>result</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>prediction</span> <span class=o>=</span> <span class=n>result</span><span class=p>[</span><span class=s2>&#34;predictions&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;median_house_value&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>st</span><span class=o>.</span><span class=n>success</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Predicted Price: $</span><span class=si>{</span><span class=n>prediction</span><span class=si>:</span><span class=s2>,.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Configuración de la URL del API:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Docker Compose: Usa service name</span>
</span></span><span class=line><span class=cl>docker-compose up
</span></span><span class=line><span class=cl><span class=c1># Streamlit automáticamente recibe API_URL=http://api:8080</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Local development: Usa localhost</span>
</span></span><span class=line><span class=cl><span class=nv>API_URL</span><span class=o>=</span>http://localhost:8080 streamlit run app.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Production: Usa Cloud Run URL</span>
</span></span><span class=line><span class=cl><span class=nv>API_URL</span><span class=o>=</span>https://housing-api-xyz.run.app streamlit run app.py
</span></span></code></pre></div><hr><h3 id=docker-compose-orquestación-de-los-tres-containers>Docker Compose: Orquestación de los Tres Containers<a hidden class=anchor aria-hidden=true href=#docker-compose-orquestación-de-los-tres-containers>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># docker-compose.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>api</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>./api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dockerfile</span><span class=p>:</span><span class=w> </span><span class=l>Dockerfile</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>housing-price-api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;8080:8080&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>PORT=8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>LOCAL_MODEL_PATH=/app/models/trained/housing_price_model.pkl</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>WANDB_API_KEY=${WANDB_API_KEY}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>./models:/app/models:ro</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>healthcheck</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>test</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;curl&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-f&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;http://localhost:8080/health&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>interval</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=l>10s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>retries</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlops-network</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>streamlit</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>./streamlit_app</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dockerfile</span><span class=p>:</span><span class=w> </span><span class=l>Dockerfile</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>housing-streamlit</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;8501:8501&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>API_URL=http://api:8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>healthcheck</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>test</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;CMD&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;curl&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-f&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;http://localhost:8501/_stcore/health&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>interval</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>timeout</span><span class=p>:</span><span class=w> </span><span class=l>10s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>retries</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>mlops-network</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mlops-network</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>driver</span><span class=p>:</span><span class=w> </span><span class=l>bridge</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>housing-mlops-network</span><span class=w>
</span></span></span></code></pre></div><p><strong>Decisiones Críticas:</strong></p><p><strong>1. Network Isolation</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>networks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>mlops-network</span><span class=w>
</span></span></span></code></pre></div><p>Ambos containers están en la misma red Docker, permitiendo que Streamlit llame al API usando <code>http://api:8080</code> (service name como hostname).</p><p><strong>Sin esto:</strong> Tendrías que usar <code>http://host.docker.internal:8080</code> (solo funciona en Docker Desktop) o la IP del host.</p><p><strong>2. Volume Mount Read-Only</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>./models:/app/models:ro</span><span class=w>
</span></span></span></code></pre></div><p>El API monta <code>models/</code> en <strong>read-only mode (<code>:ro</code>)</strong>. El container puede leer el modelo pero no modificarlo.</p><p><strong>Por qué:</strong> Seguridad. Si el container es comprometido, un atacante no puede sobrescribir el modelo con uno malicioso.</p><p><strong>3. Dependency Order</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>api</span><span class=w>
</span></span></span></code></pre></div><p>Docker Compose inicia el API antes que Streamlit. Esto evita que Streamlit falle al intentar conectarse a un API que aún no está corriendo.</p><p><strong>Limitación:</strong> <code>depends_on</code> solo espera a que el container <strong>inicie</strong>, no a que el API esté <strong>listo</strong> (healthcheck pass). Para eso, necesitas un init container o retry logic en Streamlit.</p><hr><h3 id=comando-completo-de-ejecución>Comando Completo de Ejecución<a hidden class=anchor aria-hidden=true href=#comando-completo-de-ejecución>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 1. Build todas las imágenes</span>
</span></span><span class=line><span class=cl>docker-compose build
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. Entrenar el modelo (pipeline container)</span>
</span></span><span class=line><span class=cl>docker run --env-file .env -v <span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span>/models:/app/models housing-pipeline:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. Iniciar API + Streamlit</span>
</span></span><span class=line><span class=cl>docker-compose up -d
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. Verificar health</span>
</span></span><span class=line><span class=cl>curl http://localhost:8080/health
</span></span><span class=line><span class=cl>curl http://localhost:8501/_stcore/health
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. Ver logs</span>
</span></span><span class=line><span class=cl>docker-compose logs -f
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 6. Detener todo</span>
</span></span><span class=line><span class=cl>docker-compose down
</span></span></code></pre></div><hr><h3 id=lo-que-esta-arquitectura-resuelve>Lo Que Esta Arquitectura Resuelve<a hidden class=anchor aria-hidden=true href=#lo-que-esta-arquitectura-resuelve>#</a></h3><p><strong>Sin containers:</strong></p><ul><li>&ldquo;Funciona en mi máquina&rdquo; syndrome</li><li>Dependencias conflictivas (Python 3.9 vs 3.12)</li><li>Setup manual en cada ambiente (dev, staging, prod)</li></ul><p><strong>Con esta arquitectura:</strong></p><ul><li><strong>Reproducibilidad:</strong> Mismo container corre en laptop, CI/CD, y producción</li><li><strong>Isolation:</strong> API no interfiere con Streamlit, pipeline no interfiere con API</li><li><strong>Deployment:</strong> <code>docker push</code> → <code>gcloud run deploy</code> en &lt;5 minutos</li><li><strong>Rollback:</strong> <code>docker pull previous-image</code> → restart</li><li><strong>Observability:</strong> Health checks automáticos, logs centralizados</li></ul><p><strong>El valor real:</strong> Un data scientist sin experiencia en DevOps puede deployar a producción sin saber cómo configurar nginx, systemd, o virtual environments. Docker abstrae toda esa complejidad.</p><hr><p><a name=api-architecture></a></p><h2 id=105-arquitectura-del-api-fastapi-en-producción>10.5. Arquitectura del API: FastAPI en Producción<a hidden class=anchor aria-hidden=true href=#105-arquitectura-del-api-fastapi-en-producción>#</a></h2><h3 id=por-qué-esta-sección-importa>Por Qué Esta Sección Importa<a hidden class=anchor aria-hidden=true href=#por-qué-esta-sección-importa>#</a></h3><p>Has visto pipelines de entrenamiento, sweep de hiperparámetros, y model registry. Pero <strong>el 90% del tiempo, tu modelo no está entrenando—está sirviendo predicciones en producción.</strong></p><p>Un API mal diseñado es el cuello de botella entre un modelo excelente y un producto útil. Esta sección desmenuza cómo este proyecto construye un API production-ready, no un prototipo de tutorial.</p><hr><h3 id=la-arquitectura-general>La Arquitectura General<a hidden class=anchor aria-hidden=true href=#la-arquitectura-general>#</a></h3><pre tabindex=0><code>api/
├── app/
│   ├── main.py                    # FastAPI app + lifespan management
│   ├── core/
│   │   ├── config.py              # Pydantic Settings (env vars)
│   │   ├── model_loader.py        # Multi-source model loading
│   │   ├── wandb_logger.py        # Prediction logging
│   │   └── preprocessor.py        # Feature engineering
│   ├── routers/
│   │   └── predict.py             # Prediction endpoints
│   └── models/
│       └── schemas.py             # Pydantic request/response models
├── requirements.txt
├── Dockerfile
└── tests/
</code></pre><p><strong>Decisión arquitectónica:</strong> Separation of concerns por capas:</p><ol><li><strong>Core</strong>: Lógica de negocio (cargar modelo, logging, config)</li><li><strong>Routers</strong>: Endpoints HTTP (rutas, validación de requests)</li><li><strong>Models</strong>: Schemas de datos (Pydantic)</li></ol><p><strong>Por qué no todo en <code>main.py</code>?</strong> Porque cuando el API crece (agregar autenticación, rate limiting, múltiples modelos), cada capa se extiende independientemente sin tocar el resto.</p><hr><h3 id=1-lifespan-management-el-patrón-que-evita-latencia-en-primera-request>1. Lifespan Management: El Patrón Que Evita Latencia en Primera Request<a hidden class=anchor aria-hidden=true href=#1-lifespan-management-el-patrón-que-evita-latencia-en-primera-request>#</a></h3><h4 id=el-problema-que-resuelve>El Problema Que Resuelve<a hidden class=anchor aria-hidden=true href=#el-problema-que-resuelve>#</a></h4><p><strong>Anti-pattern común:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># BAD: Cargar modelo en cada request</span>
</span></span><span class=line><span class=cl><span class=nd>@app.post</span><span class=p>(</span><span class=s2>&#34;/predict&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>features</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=nb>open</span><span class=p>(</span><span class=s2>&#34;model.pkl&#34;</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>))</span>  <span class=c1># 5 segundos cada request</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Problemas:</strong></p><ul><li>Primera request toma 5 segundos (cargar modelo)</li><li>Cada request subsecuente también (no hay caching)</li><li>Si 10 requests concurrentes → 10 cargas del modelo (50 segundos total)</li></ul><h4 id=la-solución-asynccontextmanager>La Solución: asynccontextmanager<a hidden class=anchor aria-hidden=true href=#la-solución-asynccontextmanager>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@asynccontextmanager</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>lifespan</span><span class=p>(</span><span class=n>app</span><span class=p>:</span> <span class=n>FastAPI</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Lifecycle manager for the FastAPI application.
</span></span></span><span class=line><span class=cl><span class=s2>    Loads the model on startup and cleans up on shutdown.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Starting up API...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># STARTUP: Cargar modelo UNA VEZ</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb_logger</span> <span class=o>=</span> <span class=n>WandBLogger</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>project</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>WANDB_PROJECT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>enabled</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_loader</span> <span class=o>=</span> <span class=n>ModelLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>local_model_path</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>LOCAL_MODEL_PATH</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_bucket</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>GCS_BUCKET</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_model_path</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>GCS_MODEL_PATH</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_name</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_NAME</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_stage</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_STAGE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_tracking_uri</span><span class=o>=</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_TRACKING_URI</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Loading model...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>model_loader</span><span class=o>.</span><span class=n>load_model</span><span class=p>()</span>  <span class=c1># Toma 5 segundos, pero SOLO una vez</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Model loaded: </span><span class=si>{</span><span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Guardar en app state (disponible para todos los endpoints)</span>
</span></span><span class=line><span class=cl>        <span class=n>app</span><span class=o>.</span><span class=n>state</span><span class=o>.</span><span class=n>model_loader</span> <span class=o>=</span> <span class=n>model_loader</span>
</span></span><span class=line><span class=cl>        <span class=n>app</span><span class=o>.</span><span class=n>state</span><span class=o>.</span><span class=n>wandb_logger</span> <span class=o>=</span> <span class=n>wandb_logger</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Failed to load model: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&#34;API will start but predictions will fail&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>yield</span>  <span class=c1># API corre aquí</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># SHUTDOWN: Cleanup</span>
</span></span><span class=line><span class=cl>    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Shutting down API...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>wandb_logger</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Usar lifespan en FastAPI</span>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>FastAPI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>title</span><span class=o>=</span><span class=s2>&#34;Housing Price Prediction API&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>version</span><span class=o>=</span><span class=s2>&#34;1.0.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lifespan</span><span class=o>=</span><span class=n>lifespan</span>  <span class=c1># CRÍTICO</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>Lo que hace:</strong></p><ol><li><p><strong>Startup (antes de <code>yield</code>):</strong></p><ul><li>Carga modelo en memoria (5 segundos, <strong>una sola vez</strong>)</li><li>Inicializa W&amp;B logger</li><li>Guarda ambos en <code>app.state</code> (singleton pattern)</li></ul></li><li><p><strong>Running (después de <code>yield</code>):</strong></p><ul><li>Todas las requests usan el modelo cacheado en <code>app.state.model_loader</code></li><li>Latencia por request: &lt;50ms (solo inference, no I/O)</li></ul></li><li><p><strong>Shutdown (después del context manager):</strong></p><ul><li>Cierra W&amp;B run (flush pending logs)</li><li>Libera recursos</li></ul></li></ol><p><strong>Resultado:</strong></p><ul><li>Primera request: &lt;50ms (modelo ya cargado)</li><li>Requests subsecuentes: &lt;50ms</li><li>10 requests concurrentes: &lt;100ms promedio (paralelizable)</li></ul><p><strong>Trade-off:</strong> Startup time de 5-10 segundos. Aceptable para producción—mejor que 5 segundos por request.</p><hr><h3 id=2-configuration-management-pydantic-settings-con-prioridades>2. Configuration Management: Pydantic Settings con Prioridades<a hidden class=anchor aria-hidden=true href=#2-configuration-management-pydantic-settings-con-prioridades>#</a></h3><h4 id=el-pattern-settings-as-code>El Pattern: Settings-as-Code<a hidden class=anchor aria-hidden=true href=#el-pattern-settings-as-code>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/config.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pydantic_settings</span> <span class=kn>import</span> <span class=n>BaseSettings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Settings</span><span class=p>(</span><span class=n>BaseSettings</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>PROJECT_NAME</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;Housing Price Prediction API&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>VERSION</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;1.0.0&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>API_V1_STR</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;/api/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Model - MLflow (priority 1)</span>
</span></span><span class=line><span class=cl>    <span class=n>MLFLOW_MODEL_NAME</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>MLFLOW_MODEL_STAGE</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;Production&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>MLFLOW_TRACKING_URI</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Model - GCS (priority 2)</span>
</span></span><span class=line><span class=cl>    <span class=n>GCS_BUCKET</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>GCS_MODEL_PATH</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Model - Local (priority 3, fallback)</span>
</span></span><span class=line><span class=cl>    <span class=n>LOCAL_MODEL_PATH</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Weights &amp; Biases</span>
</span></span><span class=line><span class=cl>    <span class=n>WANDB_API_KEY</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>WANDB_PROJECT</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;housing-mlops-api&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Config</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>env_file</span> <span class=o>=</span> <span class=s2>&#34;.env&#34;</span>  <span class=c1># Lee de .env automáticamente</span>
</span></span><span class=line><span class=cl>        <span class=n>case_sensitive</span> <span class=o>=</span> <span class=kc>True</span>  <span class=c1># MLFLOW_MODEL_NAME != mlflow_model_name</span>
</span></span></code></pre></div><p><strong>Por qué Pydantic Settings:</strong></p><ol><li><strong>Type Safety</strong>: <code>settings.VERSION</code> es <code>str</code>, no <code>Optional[Any]</code></li><li><strong>Validation</strong>: Si <code>MLFLOW_MODEL_STAGE</code> no es string, falla en startup (no en la primera request)</li><li><strong>Auto .env loading</strong>: No necesitas <code>python-dotenv</code> manualmente</li><li><strong>Default values</strong>: <code>LOCAL_MODEL_PATH</code> tiene default, <code>MLFLOW_MODEL_NAME</code> no</li></ol><p><strong>Uso en código:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>app.core.config</span> <span class=kn>import</span> <span class=n>Settings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>settings</span> <span class=o>=</span> <span class=n>Settings</span><span class=p>()</span>  <span class=c1># Lee env vars + .env</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_NAME</span><span class=p>:</span>  <span class=c1># Type-safe check</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>load_from_mlflow</span><span class=p>(</span><span class=n>settings</span><span class=o>.</span><span class=n>MLFLOW_MODEL_NAME</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=la-estrategia-de-prioridades-cascade-fallback>La Estrategia de Prioridades (Cascade Fallback)<a hidden class=anchor aria-hidden=true href=#la-estrategia-de-prioridades-cascade-fallback>#</a></h4><pre tabindex=0><code>Intenta cargar de:
1. MLflow Registry (si MLFLOW_MODEL_NAME está configurado)
   ↓ Si falla
2. GCS (si GCS_BUCKET está configurado)
   ↓ Si falla
3. Local filesystem (siempre disponible como último recurso)
   ↓ Si falla
4. API inicia pero `/predict` retorna 500
</code></pre><p><strong>Configuración por ambiente:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Producción (.env.production)</span>
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Production
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_TRACKING_URI</span><span class=o>=</span>https://mlflow.company.com
</span></span><span class=line><span class=cl><span class=c1># GCS y Local quedan vacíos → no se usan</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Staging (.env.staging)</span>
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_NAME</span><span class=o>=</span>housing_price_model
</span></span><span class=line><span class=cl><span class=nv>MLFLOW_MODEL_STAGE</span><span class=o>=</span>Staging
</span></span><span class=line><span class=cl><span class=c1># Mismo setup, diferente stage</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Desarrollo local (.env.local)</span>
</span></span><span class=line><span class=cl><span class=nv>LOCAL_MODEL_PATH</span><span class=o>=</span>models/trained/housing_price_model.pkl
</span></span><span class=line><span class=cl><span class=c1># Sin MLflow ni GCS → carga de local directo</span>
</span></span></code></pre></div><p><strong>Valor:</strong> Un solo codebase, múltiples ambientes. No hay <code>if ENVIRONMENT == "production"</code> en el código.</p><hr><h3 id=3-model-loader-multi-source-con-fallback-inteligente>3. Model Loader: Multi-Source con Fallback Inteligente<a hidden class=anchor aria-hidden=true href=#3-model-loader-multi-source-con-fallback-inteligente>#</a></h3><h4 id=la-arquitectura-del-loader>La Arquitectura del Loader<a hidden class=anchor aria-hidden=true href=#la-arquitectura-del-loader>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/model_loader.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ModelLoader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Handles loading ML models from various sources.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>local_model_path</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_bucket</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>gcs_model_path</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_name</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_model_stage</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow_tracking_uri</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>str</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=o>=</span> <span class=n>local_model_path</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span> <span class=o>=</span> <span class=n>gcs_bucket</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span> <span class=o>=</span> <span class=n>gcs_model_path</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span> <span class=o>=</span> <span class=n>mlflow_model_name</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_stage</span> <span class=o>=</span> <span class=n>mlflow_model_stage</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_tracking_uri</span> <span class=o>=</span> <span class=n>mlflow_tracking_uri</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Any</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>  <span class=c1># Cacheado en memoria</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_model_version</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;unknown&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span> <span class=o>=</span> <span class=n>HousingPreprocessor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>load_model</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Any</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Load model with cascade fallback strategy.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Priority 1: MLflow Registry</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attempting MLflow load: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_stage</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_mlflow</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_model_stage</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>mlflow_tracking_uri</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;MLflow load failed: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>, trying GCS...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Priority 2: GCS</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attempting GCS load: gs://</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_gcs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gcs_bucket</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>gcs_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>            <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GCS load failed: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>, trying local...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Priority 3: Local filesystem</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span> <span class=ow>and</span> <span class=n>Path</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span><span class=o>.</span><span class=n>exists</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attempting local load: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_from_local</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>local_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># All strategies failed</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Could not load model from any source. &#34;</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;Check MLflow/GCS/local configuration.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Make predictions with preprocessing.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_loaded</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;Model not loaded&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Apply same preprocessing que el training pipeline</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Predict</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>predictions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_loaded</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bool</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Check if model is loaded.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
</span></span></code></pre></div><h4 id=decisiones-técnicas-críticas-3>Decisiones Técnicas Críticas<a hidden class=anchor aria-hidden=true href=#decisiones-técnicas-críticas-3>#</a></h4><p><strong>1. Por Qué MLflow Es Priority 1</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># MLflow load</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>load_model</span><span class=p>(</span><span class=s2>&#34;models:/housing_price_model/Production&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Ventajas sobre GCS/Local:</strong></p><ul><li><strong>Model URI abstrae storage</strong>: El modelo puede estar en S3, GCS, HDFS—MLflow lo resuelve</li><li><strong>Stage resolution</strong>: <code>Production</code> automáticamente resuelve a la versión correcta (v1, v2, etc.)</li><li><strong>Metadata incluida</strong>: MLflow también carga <code>conda.yaml</code>, <code>requirements.txt</code>, metadata de features</li><li><strong>Rollback trivial</strong>: Cambias stage en MLflow UI, API recarga automáticamente en próximo restart</li></ul><p><strong>2. GCS Como Fallback (No Primary)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># GCS load</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>google.cloud</span> <span class=kn>import</span> <span class=n>storage</span>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bucket</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>bucket</span><span class=p>(</span><span class=s2>&#34;my-bucket&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>blob</span> <span class=o>=</span> <span class=n>bucket</span><span class=o>.</span><span class=n>blob</span><span class=p>(</span><span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_bytes</span> <span class=o>=</span> <span class=n>blob</span><span class=o>.</span><span class=n>download_as_bytes</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>model_bytes</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Por qué no primary:</strong></p><ul><li><strong>No hay versionamiento:</strong> <code>models/trained/housing_price_model.pkl</code> es siempre el &ldquo;latest&rdquo;—no puedes cargar v1 vs v2 sin cambiar el path</li><li><strong>No metadata:</strong> Solo obtienes el pickle, no sabes qué hiperparámetros/features espera</li><li><strong>No stages:</strong> No existe concepto de Staging vs Production</li></ul><p><strong>Cuándo usar GCS como primary:</strong></p><ul><li>MLflow no está disponible (outage)</li><li>Setup simple (solo un modelo, no necesitas registry)</li><li>Budget constraint (evitar hosting de MLflow)</li></ul><p><strong>3. Local Como Last Resort</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Local load</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;models/trained/housing_price_model.pkl&#34;</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>pickle</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Solo para:</strong></p><ul><li>Desarrollo local (no quieres depender de GCS/MLflow)</li><li>Debugging (modelo roto en GCS, testeas con una copia local)</li><li>CI/CD tests (GitHub Actions no tiene acceso a GCS)</li></ul><p><strong>Nunca para producción real</strong>—si GCS y MLflow están down, tienes problemas más grandes que el modelo.</p><p><strong>4. Preprocessing Pipeline Embebido</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span> <span class=o>=</span> <span class=n>HousingPreprocessor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>processed_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>predictions</span>
</span></span></code></pre></div><p><strong>Por qué crítico:</strong> El modelo espera features procesadas (one-hot encoding de <code>ocean_proximity</code>, feature engineering de clusters). Si el cliente envía raw features, el modelo falla.</p><p><strong>Opciones de implementación:</strong></p><p><strong>A) Preprocessing en el API (este proyecto):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Cliente envía raw features</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;ocean_proximity&#34;</span><span class=p>:</span> <span class=s2>&#34;NEAR BAY&#34;</span><span class=p>,</span> <span class=s2>&#34;longitude&#34;</span><span class=p>:</span> <span class=o>-</span><span class=mf>122.23</span><span class=p>,</span> <span class=o>...</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API aplica preprocessing</span>
</span></span><span class=line><span class=cl><span class=n>processed</span> <span class=o>=</span> <span class=n>preprocessor</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>raw_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Modelo recibe features procesadas</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>B) Preprocessing en el cliente (mal para APIs públicos):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Cliente debe saber exact preprocessing</span>
</span></span><span class=line><span class=cl><span class=n>processed</span> <span class=o>=</span> <span class=n>client_side_preprocessing</span><span class=p>(</span><span class=n>raw_features</span><span class=p>)</span>  <span class=c1># ¿Qué hace esto?</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># API solo hace inference</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>processed</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Trade-offs:</strong></p><table><thead><tr><th>Approach</th><th>Ventaja</th><th>Desventaja</th></tr></thead><tbody><tr><td><strong>Preprocessing en API</strong></td><td>Cliente no necesita saber preprocessing</td><td>API más complejo, latencia +5ms</td></tr><tr><td><strong>Preprocessing en cliente</strong></td><td>API simple, latencia baja</td><td>Cliente debe replicar preprocessing exacto</td></tr></tbody></table><p><strong>Para APIs públicos:</strong> Siempre preprocessing en el API. Los clientes no deben conocer detalles internos del modelo.</p><p><strong>Para APIs internos:</strong> Depende. Si el cliente es otro servicio que controlas, puedes hacer preprocessing ahí para reducir latencia.</p><hr><h3 id=4-requestresponse-validation-pydantic-schemas>4. Request/Response Validation: Pydantic Schemas<a hidden class=anchor aria-hidden=true href=#4-requestresponse-validation-pydantic-schemas>#</a></h3><h4 id=el-anti-pattern-validación-manual>El Anti-Pattern: Validación Manual<a hidden class=anchor aria-hidden=true href=#el-anti-pattern-validación-manual>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># BAD: Validación manual propensa a errores</span>
</span></span><span class=line><span class=cl><span class=nd>@app.post</span><span class=p>(</span><span class=s2>&#34;/predict&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>request</span><span class=p>:</span> <span class=nb>dict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=s2>&#34;longitude&#34;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>request</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;missing longitude&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>request</span><span class=p>[</span><span class=s2>&#34;longitude&#34;</span><span class=p>],</span> <span class=p>(</span><span class=nb>int</span><span class=p>,</span> <span class=nb>float</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;longitude must be number&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>request</span><span class=p>[</span><span class=s2>&#34;longitude&#34;</span><span class=p>]</span> <span class=o>&lt;</span> <span class=o>-</span><span class=mi>180</span> <span class=ow>or</span> <span class=n>request</span><span class=p>[</span><span class=s2>&#34;longitude&#34;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>180</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;error&#34;</span><span class=p>:</span> <span class=s2>&#34;longitude out of range&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... 50 líneas más de validación manual</span>
</span></span></code></pre></div><p><strong>Problemas:</strong></p><ul><li>Código repetitivo y frágil</li><li>Errores inconsistentes (<code>"missing longitude"</code> vs <code>"longitude is required"</code>)</li><li>No hay documentación automática (OpenAPI)</li><li>Difícil de testear</li></ul><h4 id=la-solución-pydantic-schemas>La Solución: Pydantic Schemas<a hidden class=anchor aria-hidden=true href=#la-solución-pydantic-schemas>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/models/schemas.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pydantic</span> <span class=kn>import</span> <span class=n>BaseModel</span><span class=p>,</span> <span class=n>Field</span><span class=p>,</span> <span class=n>field_validator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>HousingFeatures</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Input features for housing price prediction.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>longitude</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span><span class=p>,</span>  <span class=c1># Required</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Longitude coordinate&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ge</span><span class=o>=-</span><span class=mi>180</span><span class=p>,</span>  <span class=c1># greater or equal</span>
</span></span><span class=line><span class=cl>        <span class=n>le</span><span class=o>=</span><span class=mi>180</span>    <span class=c1># less or equal</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>latitude</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Latitude coordinate&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=-</span><span class=mi>90</span><span class=p>,</span> <span class=n>le</span><span class=o>=</span><span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>housing_median_age</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Median age of houses&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>total_rooms</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Total number of rooms&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>total_bedrooms</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Total number of bedrooms&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>population</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Block population&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>households</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Number of households&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>median_income</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Median income&#34;</span><span class=p>,</span> <span class=n>ge</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ocean_proximity</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Proximity to ocean&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@field_validator</span><span class=p>(</span><span class=s1>&#39;ocean_proximity&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nd>@classmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>validate_ocean_proximity</span><span class=p>(</span><span class=bp>cls</span><span class=p>,</span> <span class=n>v</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Validate ocean proximity values.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>valid_values</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;&lt;1H OCEAN&#39;</span><span class=p>,</span> <span class=s1>&#39;INLAND&#39;</span><span class=p>,</span> <span class=s1>&#39;ISLAND&#39;</span><span class=p>,</span> <span class=s1>&#39;NEAR BAY&#39;</span><span class=p>,</span> <span class=s1>&#39;NEAR OCEAN&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>v</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>valid_values</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;ocean_proximity must be one of: </span><span class=si>{</span><span class=s1>&#39;, &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>valid_values</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>v</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>  <span class=c1># Normaliza a uppercase</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;json_schema_extra&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;examples&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;longitude&#34;</span><span class=p>:</span> <span class=o>-</span><span class=mf>122.23</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;latitude&#34;</span><span class=p>:</span> <span class=mf>37.88</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;housing_median_age&#34;</span><span class=p>:</span> <span class=mf>41.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;total_rooms&#34;</span><span class=p>:</span> <span class=mf>880.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;total_bedrooms&#34;</span><span class=p>:</span> <span class=mf>129.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;population&#34;</span><span class=p>:</span> <span class=mf>322.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;households&#34;</span><span class=p>:</span> <span class=mf>126.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;median_income&#34;</span><span class=p>:</span> <span class=mf>8.3252</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;ocean_proximity&#34;</span><span class=p>:</span> <span class=s2>&#34;NEAR BAY&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>}]</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p><strong>Lo que esto da automáticamente:</strong></p><ol><li><p><strong>Validación de tipos:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=s2>&#34;not a number&#34;</span><span class=p>}</span>  <span class=c1>// Rechazado: ValidationError
</span></span></span></code></pre></div></li><li><p><strong>Validación de rangos:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mi>-200</span><span class=p>}</span>  <span class=c1>// Rechazado: must be &gt;= -180
</span></span></span></code></pre></div></li><li><p><strong>Validación custom:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;ocean_proximity&#34;</span><span class=p>:</span> <span class=s2>&#34;INVALID&#34;</span><span class=p>}</span>  <span class=c1>// Rechazado: must be one of [...]
</span></span></span></code></pre></div></li><li><p><strong>Documentación automática en <code>/docs</code>:</strong></p><ul><li>Swagger UI muestra todos los fields</li><li>Descriptions, constraints, ejemplos</li><li>Try-it-out funciona out-of-the-box</li></ul></li><li><p><strong>Serialización type-safe:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>features</span> <span class=o>=</span> <span class=n>HousingFeatures</span><span class=p>(</span><span class=o>**</span><span class=n>request_json</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>features</span><span class=o>.</span><span class=n>longitude</span>  <span class=c1># Type: float (no Optional[Any])</span>
</span></span></code></pre></div></li></ol><h4 id=batch-predictions-support>Batch Predictions Support<a hidden class=anchor aria-hidden=true href=#batch-predictions-support>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PredictionRequest</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Request model for single or batch predictions.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>instances</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>HousingFeatures</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;List of housing features for prediction&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>min_length</span><span class=o>=</span><span class=mi>1</span>  <span class=c1># Al menos una instancia</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span></code></pre></div><p><strong>Uso:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;instances&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mf>-122.23</span><span class=p>,</span> <span class=err>...</span><span class=p>},</span>  <span class=c1>// Predict house 1
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mf>-118.45</span><span class=p>,</span> <span class=err>...</span><span class=p>},</span>  <span class=c1>// Predict house 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>{</span><span class=nt>&#34;longitude&#34;</span><span class=p>:</span> <span class=mf>-121.89</span><span class=p>,</span> <span class=err>...</span><span class=p>}</span>   <span class=c1>// Predict house 3
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>Por qué soportar batch:</strong></p><ul><li><strong>Latencia reducida:</strong> 3 requests individuales = 150ms. 1 batch de 3 = 60ms.</li><li><strong>Costo reducido:</strong> Menos HTTP overhead (headers, handshake, etc.)</li><li><strong>Inference eficiente:</strong> El modelo puede vectorizar operaciones</li></ul><p><strong>Trade-off:</strong> Batch size muy grande (>1000) puede causar timeouts. Implementar límite:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>instances</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>HousingFeatures</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>min_length</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>100</span>  <span class=c1># Máximo 100 predicciones por request</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h4 id=response-schema>Response Schema<a hidden class=anchor aria-hidden=true href=#response-schema>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PredictionResult</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Individual prediction result.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>predicted_price</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Predicted median house value&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>confidence_interval</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>dict</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Confidence interval (if available)&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PredictionResponse</span><span class=p>(</span><span class=n>BaseModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Response model for predictions.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>PredictionResult</span><span class=p>]</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;List of predictions&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_version</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>Field</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Model version used&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;json_schema_extra&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;examples&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;predictions&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;predicted_price&#34;</span><span class=p>:</span> <span class=mf>452600.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;confidence_interval&#34;</span><span class=p>:</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>                <span class=p>}],</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;model_version&#34;</span><span class=p>:</span> <span class=s2>&#34;randomforest_v1&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>}]</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p><strong><code>model_version</code> en response:</strong> Crucial para debugging. Si un cliente reporta predicciones incorrectas, el <code>model_version</code> te dice qué modelo usó (v1, v2, Production, etc.).</p><hr><h3 id=5-router-pattern-endpoints-y-error-handling>5. Router Pattern: Endpoints y Error Handling<a hidden class=anchor aria-hidden=true href=#5-router-pattern-endpoints-y-error-handling>#</a></h3><h4 id=la-estructura-del-router>La Estructura del Router<a hidden class=anchor aria-hidden=true href=#la-estructura-del-router>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/routers/predict.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fastapi</span> <span class=kn>import</span> <span class=n>APIRouter</span><span class=p>,</span> <span class=n>HTTPException</span><span class=p>,</span> <span class=n>status</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>router</span> <span class=o>=</span> <span class=n>APIRouter</span><span class=p>(</span><span class=n>prefix</span><span class=o>=</span><span class=s2>&#34;/api/v1&#34;</span><span class=p>,</span> <span class=n>tags</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;predictions&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Global instances (set by main.py)</span>
</span></span><span class=line><span class=cl><span class=n>model_loader</span><span class=p>:</span> <span class=n>ModelLoader</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=n>wandb_logger</span><span class=p>:</span> <span class=n>WandBLogger</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>set_model_loader</span><span class=p>(</span><span class=n>loader</span><span class=p>:</span> <span class=n>ModelLoader</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Dependency injection pattern.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>global</span> <span class=n>model_loader</span>
</span></span><span class=line><span class=cl>    <span class=n>model_loader</span> <span class=o>=</span> <span class=n>loader</span>
</span></span></code></pre></div><p><strong>Por qué <code>prefix="/api/v1"</code>:</strong></p><pre tabindex=0><code>/api/v1/predict  ← Versión 1 del API
/api/v2/predict  ← Versión 2 (breaking changes)
</code></pre><p>Puedes correr ambas versiones simultáneamente durante migración:</p><ul><li>Clientes legacy usan <code>/api/v1/</code></li><li>Clientes nuevos usan <code>/api/v2/</code></li><li>Deprecas v1 después de 6 meses</li></ul><p><strong>Sin versionamiento:</strong> Breaking change → todos los clientes se rompen al mismo tiempo.</p><h4 id=el-endpoint-principal-post-apiv1predict>El Endpoint Principal: POST /api/v1/predict<a hidden class=anchor aria-hidden=true href=#el-endpoint-principal-post-apiv1predict>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@router.post</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;/predict&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>response_model</span><span class=o>=</span><span class=n>PredictionResponse</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_200_OK</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>responses</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=mi>400</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>ErrorResponse</span><span class=p>,</span> <span class=s2>&#34;description&#34;</span><span class=p>:</span> <span class=s2>&#34;Invalid input data&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=mi>500</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>ErrorResponse</span><span class=p>,</span> <span class=s2>&#34;description&#34;</span><span class=p>:</span> <span class=s2>&#34;Prediction failed&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>summary</span><span class=o>=</span><span class=s2>&#34;Predict housing prices&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;Make predictions for housing prices based on input features&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>async</span> <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>request</span><span class=p>:</span> <span class=n>PredictionRequest</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>PredictionResponse</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Predict housing prices for given features.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1. Check model loaded</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>model_loader</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=ow>not</span> <span class=n>model_loader</span><span class=o>.</span><span class=n>is_loaded</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=n>HTTPException</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_500_INTERNAL_SERVER_ERROR</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>detail</span><span class=o>=</span><span class=s2>&#34;Model not loaded&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 2. Convert Pydantic models to DataFrame</span>
</span></span><span class=line><span class=cl>        <span class=n>features_list</span> <span class=o>=</span> <span class=p>[</span><span class=n>instance</span><span class=o>.</span><span class=n>model_dump</span><span class=p>()</span> <span class=k>for</span> <span class=n>instance</span> <span class=ow>in</span> <span class=n>request</span><span class=o>.</span><span class=n>instances</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>features_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. Make predictions</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span> <span class=o>=</span> <span class=n>model_loader</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 4. Calculate metrics</span>
</span></span><span class=line><span class=cl>        <span class=n>response_time_ms</span> <span class=o>=</span> <span class=p>(</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start_time</span><span class=p>)</span> <span class=o>*</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 5. Format response</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>PredictionResult</span><span class=p>(</span><span class=n>predicted_price</span><span class=o>=</span><span class=nb>float</span><span class=p>(</span><span class=n>pred</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>pred</span> <span class=ow>in</span> <span class=n>predictions</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 6. Log to W&amp;B (async, no bloquea)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>wandb_logger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb_logger</span><span class=o>.</span><span class=n>log_prediction</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>features</span><span class=o>=</span><span class=n>features_list</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>predictions</span><span class=o>=</span><span class=p>[</span><span class=nb>float</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>predictions</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>model_version</span><span class=o>=</span><span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>response_time_ms</span><span class=o>=</span><span class=n>response_time_ms</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>PredictionResponse</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>predictions</span><span class=o>=</span><span class=n>results</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>model_version</span><span class=o>=</span><span class=n>model_loader</span><span class=o>.</span><span class=n>model_version</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>ValueError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Validation error (ej: feature fuera de rango esperado)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>wandb_logger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb_logger</span><span class=o>.</span><span class=n>log_error</span><span class=p>(</span><span class=s2>&#34;validation_error&#34;</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>),</span> <span class=n>features_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=n>HTTPException</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_400_BAD_REQUEST</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>detail</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;Invalid input data: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Unexpected error (ej: modelo corrupto, OOM)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>wandb_logger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>wandb_logger</span><span class=o>.</span><span class=n>log_error</span><span class=p>(</span><span class=s2>&#34;prediction_error&#34;</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=n>HTTPException</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>status_code</span><span class=o>=</span><span class=n>status</span><span class=o>.</span><span class=n>HTTP_500_INTERNAL_SERVER_ERROR</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>detail</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;Prediction failed: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span></code></pre></div><p><strong>Decisiones de error handling:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Inference</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>ValueError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Cliente envió datos inválidos → 400 Bad Request</span>
</span></span><span class=line><span class=cl>    <span class=c1># Loguear a W&amp;B para análisis</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>400</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>Exception</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Error inesperado (bug en el código/modelo) → 500 Internal Server Error</span>
</span></span><span class=line><span class=cl>    <span class=c1># Loguear a W&amp;B para alerting</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>500</span>
</span></span></code></pre></div><p><strong>Por qué distinguir 400 vs 500:</strong></p><ul><li><strong>400:</strong> Culpa del cliente. No retries automáticos.</li><li><strong>500:</strong> Culpa del servidor. Cliente puede retry.</li></ul><p><strong>Logging de errores a W&amp;B:</strong> Permite detectar patrones. Si ves 1000 <code>validation_error</code> para <code>ocean_proximity="INVALID"</code>, agregas un mensaje de error más claro.</p><hr><h3 id=6-wb-logging-observability-en-producción>6. W&amp;B Logging: Observability en Producción<a hidden class=anchor aria-hidden=true href=#6-wb-logging-observability-en-producción>#</a></h3><h4 id=por-qué-loguear-predicciones>Por Qué Loguear Predicciones<a hidden class=anchor aria-hidden=true href=#por-qué-loguear-predicciones>#</a></h4><p><strong>Pregunta:</strong> &ldquo;¿Para qué loguear cada predicción si ya tengo logs de uvicorn?&rdquo;</p><p><strong>Respuesta:</strong> Los logs de uvicorn te dicen:</p><ul><li>Qué endpoint se llamó</li><li>HTTP status code</li><li>Cuánto tardó</li></ul><p>Los logs de W&amp;B te dicen:</p><ul><li><strong>Qué features</strong> se usaron</li><li><strong>Qué predicción</strong> se hizo</li><li><strong>Distribución de predicciones</strong> (¿todas están en $200k-$500k? ¿hay outliers?)</li><li><strong>Latencia promedio</strong> por request</li><li><strong>Error rate</strong> (¿cuántos requests fallan?)</li></ul><p><strong>Caso de uso real:</strong> Stakeholder reporta &ldquo;las predicciones están muy altas últimamente&rdquo;. Abres W&amp;B dashboard:</p><pre tabindex=0><code>prediction/mean: $450k (antes: $380k)
features/median_income: 9.2 (antes: 7.5)
</code></pre><p><strong>Conclusión:</strong> No hay bug—simplemente los clientes están consultando casas en áreas más caras (<code>median_income</code> más alto). Sin W&amp;B, estarías debuggeando código por horas.</p><h4 id=la-implementación>La Implementación<a hidden class=anchor aria-hidden=true href=#la-implementación>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/core/wandb_logger.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>WandBLogger</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>project</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;housing-mlops-api&#34;</span><span class=p>,</span> <span class=n>enabled</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>enabled</span> <span class=o>=</span> <span class=n>enabled</span> <span class=ow>and</span> <span class=nb>bool</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;WANDB_API_KEY&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>enabled</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_run</span> <span class=o>=</span> <span class=n>wandb</span><span class=o>.</span><span class=n>init</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>project</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>project</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>job_type</span><span class=o>=</span><span class=s2>&#34;api-inference&#34;</span><span class=p>,</span>  <span class=c1># Distinguir de training runs</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;environment&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;ENVIRONMENT&#34;</span><span class=p>,</span> <span class=s2>&#34;production&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;model_version&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&#34;MODEL_VERSION&#34;</span><span class=p>,</span> <span class=s2>&#34;unknown&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=n>reinit</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># Permite múltiples init() en mismo proceso</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>log_prediction</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>features</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Dict</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>model_version</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>response_time_ms</span><span class=p>:</span> <span class=nb>float</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>enabled</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Métricas agregadas</span>
</span></span><span class=line><span class=cl>        <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/count&#34;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/mean&#34;</span><span class=p>:</span> <span class=nb>sum</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/min&#34;</span><span class=p>:</span> <span class=nb>min</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;prediction/max&#34;</span><span class=p>:</span> <span class=nb>max</span><span class=p>(</span><span class=n>predictions</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;performance/response_time_ms&#34;</span><span class=p>:</span> <span class=n>response_time_ms</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;model/version&#34;</span><span class=p>:</span> <span class=n>model_version</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;timestamp&#34;</span><span class=p>:</span> <span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>isoformat</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Feature distributions (sample first 100)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>features</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>100</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>feat</span><span class=p>,</span> <span class=n>pred</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>features</span><span class=p>,</span> <span class=n>predictions</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>                <span class=n>wandb</span><span class=o>.</span><span class=n>log</span><span class=p>({</span>
</span></span><span class=line><span class=cl>                    <span class=sa>f</span><span class=s2>&#34;features/instance_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>/median_income&#34;</span><span class=p>:</span> <span class=n>feat</span><span class=p>[</span><span class=s2>&#34;median_income&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=sa>f</span><span class=s2>&#34;predictions/instance_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>:</span> <span class=n>pred</span>
</span></span><span class=line><span class=cl>                <span class=p>})</span>
</span></span></code></pre></div><p><strong>Por qué <code>job_type="api-inference"</code>:</strong></p><p>En W&amp;B dashboard, puedes filtrar por job type:</p><ul><li><code>training</code>: Runs del pipeline de entrenamiento</li><li><code>sweep</code>: Runs del hyperparameter sweep</li><li><code>api-inference</code>: Predicciones en producción</li></ul><p><strong>Por qué <code>reinit=True</code>:</strong> Un proceso de uvicorn puede vivir días. <code>reinit=True</code> permite crear múltiples W&amp;B runs dentro del mismo proceso (uno por startup/restart).</p><p><strong>Por qué sample first 100:</strong> Loguear 10,000 features individuales por request sería demasiado overhead. Muestrear 100 da distribución representativa sin matar performance.</p><h4 id=wb-dashboard-en-producción>W&amp;B Dashboard en Producción<a hidden class=anchor aria-hidden=true href=#wb-dashboard-en-producción>#</a></h4><pre tabindex=0><code># Métricas a monitorear:

prediction/count: Requests per minute (RPM)
  - Esperado: 100-500 RPM
  - Alerta: &lt;10 RPM (¿está caído?) o &gt;2000 RPM (¿DDoS?)

prediction/mean: Precio promedio predicho
  - Esperado: $300k-$450k (según mercado)
  - Alerta: &gt;$1M (modelo roto) o &lt;$50k (data drift)

performance/response_time_ms: Latencia
  - Esperado: 30-60ms
  - Alerta: &gt;200ms (modelo lento o CPU throttling)

error/count: Errores por minuto
  - Esperado: 0-5 errores/min
  - Alerta: &gt;50 errores/min (investigate immediately)
</code></pre><hr><h3 id=7-cors-y-security>7. CORS y Security<a hidden class=anchor aria-hidden=true href=#7-cors-y-security>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># api/app/main.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>app</span><span class=o>.</span><span class=n>add_middleware</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>CORSMiddleware</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_origins</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;http://localhost:3000&#34;</span><span class=p>,</span>     <span class=c1># Frontend local (React/Streamlit)</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;http://localhost:8080&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;https://app.company.com&#34;</span><span class=p>,</span>   <span class=c1># Frontend en producción</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_credentials</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1># No cookies (API es stateless)</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_methods</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;GET&#34;</span><span class=p>,</span> <span class=s2>&#34;POST&#34;</span><span class=p>],</span>  <span class=c1># Solo métodos necesarios</span>
</span></span><span class=line><span class=cl>    <span class=n>allow_headers</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;Content-Type&#34;</span><span class=p>,</span> <span class=s2>&#34;Authorization&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>max_age</span><span class=o>=</span><span class=mi>3600</span><span class=p>,</span>  <span class=c1># Cache preflight requests por 1 hora</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>Por qué restricted origins:</strong></p><p><strong>Anti-pattern (permissive):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>allow_origins</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;*&#34;</span><span class=p>]</span>  <span class=c1># MALO: Cualquier sitio puede llamar tu API</span>
</span></span></code></pre></div><p><strong>Problema:</strong> Un sitio malicioso <code>evil.com</code> puede hacer requests a tu API desde el navegador del usuario, potencialmente:</p><ul><li>Consumir tu cuota de GCP (si no hay auth)</li><li>Hacer predicciones spam</li><li>DoS attack</li></ul><p><strong>Pattern correcto (restrictive):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>allow_origins</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;https://app.company.com&#34;</span><span class=p>]</span>  <span class=c1># Solo tu frontend</span>
</span></span></code></pre></div><p><strong>Para desarrollo local:</strong> Agregar <code>http://localhost:3000</code> temporalmente, remover en producción.</p><p><strong>Por qué <code>allow_credentials=False</code>:</strong> Este API es stateless—no usa cookies ni sesiones. <code>allow_credentials=True</code> sería innecesario y una superficie de ataque adicional.</p><hr><h3 id=8-el-flujo-completo-de-una-request>8. El Flujo Completo de Una Request<a hidden class=anchor aria-hidden=true href=#8-el-flujo-completo-de-una-request>#</a></h3><p><strong>Request:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl -X POST http://localhost:8080/api/v1/predict <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>    &#34;instances&#34;: [{
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;longitude&#34;: -122.23,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;latitude&#34;: 37.88,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;housing_median_age&#34;: 41,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;total_rooms&#34;: 880,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;total_bedrooms&#34;: 129,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;population&#34;: 322,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;households&#34;: 126,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;median_income&#34;: 8.3252,
</span></span></span><span class=line><span class=cl><span class=s1>      &#34;ocean_proximity&#34;: &#34;NEAR BAY&#34;
</span></span></span><span class=line><span class=cl><span class=s1>    }]
</span></span></span><span class=line><span class=cl><span class=s1>  }&#39;</span>
</span></span></code></pre></div><p><strong>El viaje interno (&lt; 50ms):</strong></p><pre tabindex=0><code>1. FastAPI recibe request (1ms)
   ├─ CORS middleware valida origin
   └─ Router match: POST /api/v1/predict

2. Pydantic validation (2ms)
   ├─ Parse JSON → PredictionRequest object
   ├─ Validate types (longitude: float ✓)
   ├─ Validate ranges (longitude: -122.23, dentro de [-180, 180] ✓)
   └─ Custom validator (ocean_proximity: &#34;NEAR BAY&#34; → válido ✓)

3. Endpoint handler: predict() (40ms)
   ├─ Check model_loader.is_loaded (0.1ms)
   ├─ Convert Pydantic → DataFrame (1ms)
   ├─ Preprocessing (5ms)
   │   ├─ One-hot encode ocean_proximity
   │   ├─ Compute cluster similarity features
   │   └─ Scale numerical features
   ├─ Model inference (30ms)
   │   └─ RandomForest.predict(processed_features)
   ├─ Format response (1ms)
   └─ Log to W&amp;B (async, &lt;1ms non-blocking)

4. FastAPI serializa response (2ms)
   └─ PredictionResponse → JSON

5. HTTP response enviado (1ms)

Total: ~50ms
</code></pre><p><strong>Response:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;predictions&#34;</span><span class=p>:</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;predicted_price&#34;</span><span class=p>:</span> <span class=mf>452600.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;confidence_interval&#34;</span><span class=p>:</span> <span class=kc>null</span>
</span></span><span class=line><span class=cl>  <span class=p>}],</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;model_version&#34;</span><span class=p>:</span> <span class=s2>&#34;models:/housing_price_model/Production&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><h3 id=9-lo-que-esta-arquitectura-logra>9. Lo Que Esta Arquitectura Logra<a hidden class=anchor aria-hidden=true href=#9-lo-que-esta-arquitectura-logra>#</a></h3><p><strong>Sin esta arquitectura (API naive):</strong></p><ul><li>Cargar modelo en cada request (5 segundos/request)</li><li>Validación manual propensa a errores</li><li>Sin observability (debugging es adivinar)</li><li>Sin versionamiento de API (breaking changes rompen clientes)</li><li>CORS abierto (vulnerability)</li></ul><p><strong>Con esta arquitectura:</strong></p><ul><li><strong>Latencia:</strong> &lt;50ms por predicción (modelo cacheado)</li><li><strong>Confiabilidad:</strong> Pydantic garantiza requests válidos antes de llegar al modelo</li><li><strong>Observability:</strong> W&amp;B dashboard muestra distribución de predicciones, latencia, errores</li><li><strong>Maintainability:</strong> Separation of concerns (core/routers/models)</li><li><strong>Security:</strong> CORS restrictivo, error handling robusto</li><li><strong>Versionamiento:</strong> <code>/api/v1/</code> permite evolucionar el API sin romper clientes</li></ul><p><strong>El valor real:</strong> Este API puede escalar de 10 requests/min a 10,000 requests/min sin cambios en el código—solo agregar más containers con load balancer. La arquitectura ya está lista.</p><hr><p><a name=model-strategies></a></p><h2 id=11-estrategias-de-selección-de-modelos-y-parámetros>11. Estrategias de Selección de Modelos y Parámetros<a hidden class=anchor aria-hidden=true href=#11-estrategias-de-selección-de-modelos-y-parámetros>#</a></h2><hr><h2 id=navegación>Navegación<a hidden class=anchor aria-hidden=true href=#navegación>#</a></h2><p><strong><a href=/mlops/anatomia-pipeline-mlops-parte-1/>← Parte 1: Pipeline y Orquestación</a></strong> | <strong><a href=/mlops/anatomia-pipeline-mlops-parte-3/>Parte 3: Producción y Best Practices →</a></strong></p><p>En la Parte 3 cubriremos:</p><ul><li>Estrategias de selección de modelos y parámetros</li><li>Testing: Fixtures, mocking y coverage real</li><li>Patrones de producción (Transform Pattern, Data Drift, Feature Stores)</li><li>Checklist de Production Readiness</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://carlosdanieljimenez.com/tags/mlops/>Mlops</a></li><li><a href=https://carlosdanieljimenez.com/tags/ci-cd/>Ci-Cd</a></li><li><a href=https://carlosdanieljimenez.com/tags/docker/>Docker</a></li><li><a href=https://carlosdanieljimenez.com/tags/fastapi/>Fastapi</a></li><li><a href=https://carlosdanieljimenez.com/tags/github-actions/>Github-Actions</a></li><li><a href=https://carlosdanieljimenez.com/tags/wandb/>Wandb</a></li><li><a href=https://carlosdanieljimenez.com/tags/mlflow/>Mlflow</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://carlosdanieljimenez.com/>The Probability Engine</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div class=newsletter-cta><div class=newsletter-content><h3>📬 Did this help?</h3><p>I write about MLOps, Edge AI, and making models work outside the lab.
One email per month, max. No spam, no course pitches, just technical content.</p><form action=https://buttondown.com/api/emails/embed-subscribe/carlosjimenez88m method=post class=newsletter-form target=popupwindow onsubmit='window.open("https://buttondown.com/carlosjimenez88m","popupwindow")'><div class=form-group><input type=email name=email id=bd-email placeholder=your@email.com required aria-label="Email address">
<input type=submit value=Subscribe></div></form><p class=newsletter-stats>Join engineers building ML systems and Edge Computing infrastructure.</p></div></div><style>.newsletter-cta{margin:3rem auto 2rem;padding:2rem;max-width:650px;background:var(--entry);border:1px solid var(--border);border-radius:8px;text-align:center}.newsletter-content h3{margin:0 0 1rem;font-size:1.5rem;color:var(--primary)}.newsletter-content p{margin:0 0 1.5rem;color:var(--secondary);line-height:1.6}.newsletter-form{margin:1.5rem 0}.form-group{display:flex;gap:.5rem;max-width:500px;margin:0 auto;flex-wrap:wrap;justify-content:center}.newsletter-form input[type=email]{flex:1;min-width:250px;padding:.75rem 1rem;font-size:1rem;border:1px solid var(--border);border-radius:6px;background:var(--theme);color:var(--content);transition:border-color .2s ease}.newsletter-form input[type=email]:focus{outline:none;border-color:var(--primary);box-shadow:0 0 0 3px rgba(var(--primary-rgb),.1)}.newsletter-form input[type=submit]{padding:.75rem 2rem;font-size:1rem;font-weight:600;color:#fff;background:var(--primary);border:none;border-radius:6px;cursor:pointer;transition:opacity .2s ease}.newsletter-form input[type=submit]:hover{opacity:.9}.newsletter-stats{font-size:.875rem;color:var(--secondary);margin-top:1rem;font-style:italic}@media screen and (max-width:600px){.newsletter-cta{padding:1.5rem 1rem;margin:2rem .5rem 1rem}.newsletter-content h3{font-size:1.25rem}.form-group{flex-direction:column;width:100%}.newsletter-form input[type=email],.newsletter-form input[type=submit]{width:100%;min-width:100%}}</style><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>